{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"User Guide","text":"<p>This guide provides conceptual explanations and detailed tutorials for using CTDS effectively.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is CTDS?</li> <li>Key Concepts</li> <li>Model Architecture</li> <li>Parameter Structure</li> <li>Training Workflow</li> <li>Inference and Prediction</li> <li>Constraints and Dale's Law</li> <li>Best Practices</li> </ol>"},{"location":"#what-is-ctds","title":"What is CTDS?","text":"<p>Cell-Type Dynamical Systems (CTDS) is a probabilistic model for analyzing neural population dynamics while respecting the biological structure of neural circuits. Unlike traditional latent variable models, CTDS incorporates:</p> <ul> <li>Cell-type structure: Different neuron types (excitatory vs inhibitory)</li> <li>Dale's law: Sign constraints reflecting biological connectivity patterns</li> <li>Population dynamics: Shared latent dynamics across cell types</li> </ul>"},{"location":"#when-to-use-ctds","title":"When to Use CTDS","text":"<p>CTDS is particularly useful when: - You have neural population recordings with known cell types - You want to enforce biological constraints on connectivity - You need interpretable latent dynamics - You're studying circuit-level neural computation</p>"},{"location":"#key-concepts","title":"Key Concepts","text":""},{"location":"#state-space-model","title":"State-Space Model","text":"<p>CTDS is built on a linear state-space model:</p> <pre><code>States:       x_{t+1} = A x_t + w_t,  w_t ~ N(0, Q)\nObservations: y_t = C x_t + v_t,      v_t ~ N(0, R)\n</code></pre> <p>Where: - <code>x_t</code> \u2208 \u211d^D: Latent neural state (shared dynamics) - <code>y_t</code> \u2208 \u211d^N: Observed neural activity (spikes, calcium, etc.) - <code>A</code> \u2208 \u211d^(D\u00d7D): Dynamics matrix (how states evolve) - <code>C</code> \u2208 \u211d^(N\u00d7D): Emission matrix (how states map to observations) - <code>Q</code>, <code>R</code>: Process and observation noise covariances</p>"},{"location":"#cell-type-structure","title":"Cell-Type Structure","text":"<p>The key innovation is incorporating cell-type information:</p> <ol> <li>Cell Types: Each neuron belongs to a type (e.g., excitatory, inhibitory)</li> <li>Type Masks: Binary matrices indicating which neurons belong to each type</li> <li>Sign Constraints: Cell types have characteristic signs (+ for excitatory, - for inhibitory)</li> </ol>"},{"location":"#dales-law","title":"Dale's Law","text":"<p>Dale's law states that neurons are either excitatory or inhibitory. In CTDS: - Excitatory neurons have positive outgoing weights - Inhibitory neurons have negative outgoing weights - This is enforced on both dynamics (A) and emissions (C) matrices</p>"},{"location":"#model-architecture","title":"Model Architecture","text":""},{"location":"#core-components","title":"Core Components","text":"<pre><code>import ctds\n\n# Basic model creation\nmodel = ctds.CTDS(\n    emission_dim=50,    # Number of neurons (N)\n    state_dim=10,       # Latent dimension (D)\n    cell_types=jnp.array([0, 1]),        # Cell type labels\n    cell_sign=jnp.array([1, -1]),        # Type signs (E+, I-)\n    cell_type_mask=mask,                 # N\u00d7K binary mask\n    dale_law=True                        # Enforce Dale's law\n)\n</code></pre>"},{"location":"#cell-type-specification","title":"Cell Type Specification","text":""},{"location":"#method-1-simple-binary-types","title":"Method 1: Simple Binary Types","text":"<pre><code># For binary E/I classification\ncell_types = jnp.array([0, 1])  # 0=excitatory, 1=inhibitory\ncell_sign = jnp.array([1, -1])  # Signs for each type\n</code></pre>"},{"location":"#method-2-custom-mask-matrix","title":"Method 2: Custom Mask Matrix","text":"<pre><code># For fine-grained control\n# cell_type_mask[neuron, type] = 1 if neuron belongs to type\ncell_type_mask = jnp.array([\n    [1, 0],  # Neuron 0 \u2192 Type 0 (excitatory)\n    [1, 0],  # Neuron 1 \u2192 Type 0 (excitatory)\n    [0, 1],  # Neuron 2 \u2192 Type 1 (inhibitory)\n    [0, 1],  # Neuron 3 \u2192 Type 1 (inhibitory)\n])\n</code></pre>"},{"location":"#parameter-structure","title":"Parameter Structure","text":"<p>CTDS parameters are organized hierarchically:</p>"},{"location":"#paramsctds-top-level","title":"<code>ParamsCTDS</code> (Top Level)","text":"<pre><code>params = ParamsCTDS(\n    dynamics=dynamics_params,      # A, Q matrices\n    emissions=emissions_params,    # C, R matrices  \n    initial=initial_params,        # \u03bc\u2080, \u03a3\u2080\n    constraints=constraint_params  # Cell type info\n)\n</code></pre>"},{"location":"#dynamics-parameters-paramsctdsdynamics","title":"Dynamics Parameters (<code>ParamsCTDSDynamics</code>)","text":"<pre><code>dynamics = ParamsCTDSDynamics(\n    A=A_matrix,              # State transition matrix (D\u00d7D)\n    Q=Q_matrix,              # Process noise covariance (D\u00d7D)\n    dynamics_mask=mask       # Dale's law mask for A\n)\n</code></pre>"},{"location":"#emission-parameters-paramsctdsemissions","title":"Emission Parameters (<code>ParamsCTDSEmissions</code>)","text":"<pre><code>emissions = ParamsCTDSEmissions(\n    C=C_matrix,              # Emission matrix (N\u00d7D)\n    R=R_matrix,              # Observation noise covariance (N\u00d7N)\n    emissions_mask=mask      # Dale's law mask for C\n)\n</code></pre>"},{"location":"#constraint-parameters-paramsctdsconstraints","title":"Constraint Parameters (<code>ParamsCTDSConstraints</code>)","text":"<pre><code>constraints = ParamsCTDSConstraints(\n    cell_types=types,        # Cell type assignments\n    cell_sign=signs,         # Type signs\n    cell_type_mask=mask      # Type membership matrix\n)\n</code></pre>"},{"location":"#training-workflow","title":"Training Workflow","text":""},{"location":"#1-data-preparation","title":"1. Data Preparation","text":"<pre><code># Load your neural data\nobservations = load_neural_data()  # Shape: (T, N)\n\n# Standardize (recommended)\nobservations = (observations - jnp.mean(observations, axis=0)) / jnp.std(observations, axis=0)\n\n# Define cell types (from experimental annotation)\ncell_types = get_cell_type_labels()  # Shape: (N,)\n</code></pre>"},{"location":"#2-model-initialization","title":"2. Model Initialization","text":"<pre><code># Create model\nmodel = ctds.CTDS(\n    emission_dim=observations.shape[1],\n    state_dim=latent_dim,\n    cell_types=cell_types,\n    cell_sign=jnp.array([1, -1]),  # E+, I-\n    dale_law=True\n)\n\n# Initialize parameters from data\nparams = model.initialize(observations)\n</code></pre>"},{"location":"#3-em-training","title":"3. EM Training","text":"<pre><code># Fit model using EM algorithm\nfitted_params, losses = model.fit_em(\n    params, \n    observations,\n    num_iters=100,\n    verbose=True\n)\n\n# Monitor convergence\nimport matplotlib.pyplot as plt\nplt.plot(losses)\nplt.xlabel('EM Iteration')\nplt.ylabel('Log-likelihood')\nplt.title('Training Progress')\n</code></pre>"},{"location":"#4-model-validation","title":"4. Model Validation","text":"<pre><code># Check constraint satisfaction\nconstraints = fitted_params.constraints\nA = fitted_params.dynamics.A\ndale_satisfied = ctds.utils.check_dale_law(A, constraints.cell_sign)\nprint(f\"Dale's law satisfied: {dale_satisfied}\")\n\n# Evaluate fit quality\nfinal_loglik = losses[-1]\nprint(f\"Final log-likelihood: {final_loglik}\")\n</code></pre>"},{"location":"#inference-and-prediction","title":"Inference and Prediction","text":""},{"location":"#state-estimation-smoothing","title":"State Estimation (Smoothing)","text":"<pre><code># Kalman smoothing for state inference\nsmoothed_means, smoothed_covs = model.smoother(fitted_params, observations)\n\n# Extract latent trajectories\nlatent_dynamics = smoothed_means  # Shape: (T, D)\n</code></pre>"},{"location":"#forecasting","title":"Forecasting","text":"<pre><code># Predict future states\nhistory = observations[-20:]  # Use last 20 steps as context\nforecast_steps = 10\n\nforecast_means, forecast_covs = model.forecast(\n    fitted_params,\n    history,\n    num_steps=forecast_steps\n)\n\n# Generate predicted observations\npredicted_obs = forecast_means @ fitted_params.emissions.C.T\n</code></pre>"},{"location":"#log-likelihood-computation","title":"Log-Likelihood Computation","text":"<pre><code># Evaluate model likelihood\nlog_prob = model.log_prob(fitted_params, observations)\nprint(f\"Data log-likelihood: {log_prob}\")\n\n# Per-timestep likelihoods\ntimestep_logliks = model.log_prob_timesteps(fitted_params, observations)\n</code></pre>"},{"location":"#constraints-and-dales-law","title":"Constraints and Dale's Law","text":""},{"location":"#understanding-dales-law","title":"Understanding Dale's Law","text":"<p>Dale's law constrains neural connectivity based on cell types:</p> <ol> <li>Excitatory neurons (E): All outgoing connections are positive</li> <li>Inhibitory neurons (I): All outgoing connections are negative</li> </ol>"},{"location":"#implementation-in-ctds","title":"Implementation in CTDS","text":"<pre><code># Dale's law affects two matrices:\n\n# 1. Dynamics matrix A: How latent states interact\n# A[i,j] sign determined by the cell type of state j\n\n# 2. Emissions matrix C: How states map to observations  \n# C[i,j] sign determined by the cell type of neuron i\n</code></pre>"},{"location":"#constraint-enforcement","title":"Constraint Enforcement","text":"<p>CTDS enforces constraints during the M-step:</p> <pre><code># Automatic enforcement during EM\nmodel = ctds.CTDS(dale_law=True)  # Enables constraint enforcement\n\n# Manual constraint checking\nA_constrained = ctds.utils.enforce_dale_law(A, cell_types, cell_sign)\n</code></pre>"},{"location":"#flexible-constraint-specification","title":"Flexible Constraint Specification","text":"<pre><code># Mixed cell types\ncell_types = jnp.array([0, 0, 1, 1, 2])  # 3 cell types\ncell_sign = jnp.array([1, -1, 0])        # E+, I-, unconstrained\n\n# Partial constraints\ncell_type_mask = jnp.array([\n    [1, 0, 0],  # Neuron 0 \u2192 Type 0 only\n    [0.7, 0.3, 0],  # Neuron 1 \u2192 Mixed type (soft assignment)\n    [0, 1, 0],  # Neuron 2 \u2192 Type 1 only\n])\n</code></pre>"},{"location":"#best-practices","title":"Best Practices","text":""},{"location":"#model-selection","title":"Model Selection","text":""},{"location":"#state-dimension","title":"State Dimension","text":"<pre><code># Try multiple state dimensions\nstate_dims = [5, 10, 15, 20]\nresults = {}\n\nfor D in state_dims:\n    model = ctds.CTDS(emission_dim=N, state_dim=D)\n    params = model.initialize(observations)\n    _, losses = model.fit_em(params, observations)\n    results[D] = losses[-1]  # Final log-likelihood\n\n# Select best dimension\nbest_D = max(results, key=results.get)\n</code></pre>"},{"location":"#cross-validation","title":"Cross-Validation","text":"<pre><code># Time-series split\ntrain_frac = 0.8\nsplit_idx = int(train_frac * len(observations))\n\ntrain_obs = observations[:split_idx]\ntest_obs = observations[split_idx:]\n\n# Fit on training data\nmodel = ctds.CTDS(emission_dim=N, state_dim=D)\nparams = model.initialize(train_obs)\nfitted_params, _ = model.fit_em(params, train_obs)\n\n# Evaluate on test data\ntest_loglik = model.log_prob(fitted_params, test_obs)\n</code></pre>"},{"location":"#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"#standardization","title":"Standardization","text":"<pre><code># Z-score normalization (recommended)\nobservations = (observations - observations.mean(0)) / observations.std(0)\n\n# Min-max scaling (alternative)\nobservations = (observations - observations.min(0)) / (observations.max(0) - observations.min(0))\n</code></pre>"},{"location":"#handling-missing-data","title":"Handling Missing Data","text":"<pre><code># Simple masking approach\nmask = jnp.isfinite(observations)\nobservations = jnp.where(mask, observations, 0.0)\n\n# More sophisticated: interpolation\nfrom scipy.interpolate import interp1d\n# ... interpolation code ...\n</code></pre>"},{"location":"#performance-optimization","title":"Performance Optimization","text":""},{"location":"#gpu-usage","title":"GPU Usage","text":"<pre><code># Move data to GPU\nobservations = jax.device_put(observations)\nparams = jax.tree_map(jax.device_put, params)\n\n# Check GPU availability\nimport jax\nprint(f\"Devices: {jax.devices()}\")\n</code></pre>"},{"location":"#memory-management","title":"Memory Management","text":"<pre><code># For large datasets, consider chunking\ndef fit_chunked(model, params, observations, chunk_size=1000):\n    chunks = [observations[i:i+chunk_size] \n              for i in range(0, len(observations), chunk_size)]\n\n    for chunk in chunks:\n        params, _ = model.fit_em(params, chunk, num_iters=10)\n\n    return params\n</code></pre>"},{"location":"#jit-compilation","title":"JIT Compilation","text":"<pre><code># Pre-compile inference functions\n@jax.jit\ndef fast_smoother(params, obs):\n    return model.smoother(params, obs)\n\n@jax.jit  \ndef fast_forecast(params, history, steps):\n    return model.forecast(params, history, steps)\n</code></pre>"},{"location":"#initialization-strategies","title":"Initialization Strategies","text":""},{"location":"#from-factor-analysis","title":"From Factor Analysis","text":"<pre><code># Initialize with factor analysis\nfrom sklearn.decomposition import FactorAnalysis\n\nfa = FactorAnalysis(n_components=state_dim)\nfa.fit(observations)\n\n# Use FA loadings as initial C matrix\ninitial_C = fa.components_.T\n</code></pre>"},{"location":"#random-initialization","title":"Random Initialization","text":"<pre><code># Custom random initialization\nkey = jax.random.PRNGKey(42)\nparams = model.initialize(observations, key=key)\n</code></pre>"},{"location":"#diagnostic-tools","title":"Diagnostic Tools","text":""},{"location":"#parameter-inspection","title":"Parameter Inspection","text":"<pre><code># Check eigenvalues for stability\nA = fitted_params.dynamics.A\neigenvals = jnp.linalg.eigvals(A)\nstable = jnp.all(jnp.abs(eigenvals) &lt; 1.0)\nprint(f\"Dynamics stable: {stable}\")\n\n# Analyze emission loadings\nC = fitted_params.emissions.C\nloading_norms = jnp.linalg.norm(C, axis=1)\nprint(f\"Average loading magnitude: {loading_norms.mean()}\")\n</code></pre>"},{"location":"#convergence-monitoring","title":"Convergence Monitoring","text":"<pre><code># Track multiple metrics during training\nmetrics = {\n    'log_likelihood': [],\n    'param_change': [],\n    'constraint_violation': []\n}\n\n# Custom EM loop with monitoring\nfor iter in range(num_iters):\n    new_params, loglik = model.em_step(params, observations)\n\n    metrics['log_likelihood'].append(loglik)\n    metrics['param_change'].append(param_distance(params, new_params))\n    metrics['constraint_violation'].append(check_constraints(new_params))\n\n    params = new_params\n</code></pre>"},{"location":"#troubleshooting","title":"Troubleshooting","text":""},{"location":"#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li>Non-convergence</li> <li>Check data scaling and initialization</li> <li>Try smaller learning rates</li> <li> <p>Increase regularization</p> </li> <li> <p>Constraint violations</p> </li> <li>Verify cell type specification</li> <li>Check sign constraints are correct</li> <li> <p>Use stronger regularization</p> </li> <li> <p>Numerical instability</p> </li> <li>Add diagonal regularization to covariance matrices</li> <li>Use double precision (<code>jax.config.update(\"jax_enable_x64\", True)</code>)</li> <li> <p>Scale down learning rates</p> </li> <li> <p>Memory errors</p> </li> <li>Reduce state dimension</li> <li>Process data in chunks</li> <li>Move to CPU if GPU memory limited</li> </ol>"},{"location":"#debugging-workflow","title":"Debugging Workflow","text":"<pre><code># 1. Validate inputs\nassert observations.shape[0] &gt; observations.shape[1], \"Need T &gt; N\"\nassert jnp.isfinite(observations).all(), \"Remove NaN/Inf values\"\n\n# 2. Check model configuration\nmodel.validate_config()\n\n# 3. Monitor parameter evolution\nparam_norms = jax.tree_map(lambda x: jnp.linalg.norm(x), params)\nprint(f\"Parameter norms: {param_norms}\")\n\n# 4. Validate final result\nfinal_params = fitted_params\nconstraints_satisfied = ctds.utils.validate_constraints(final_params)\nprint(f\"All constraints satisfied: {constraints_satisfied}\")\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Try the examples to see CTDS in action</li> <li>Explore the API reference for detailed documentation</li> <li>Run the quickstart script for hands-on experience</li> <li>Check out the Jupyter notebooks for in-depth tutorials</li> </ul>"},{"location":"api/","title":"CTDS API Documentation","text":""},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#parameters","title":"Parameters","text":"<p>The parameter system in CTDS uses a hierarchical structure to organize model components:</p> <pre><code>ParamsCTDS\n\u251c\u2500\u2500 initial: ParamsCTDSInitial      # Initial state distribution\n\u251c\u2500\u2500 dynamics: ParamsCTDSDynamics    # Transition dynamics A, Q\n\u251c\u2500\u2500 emissions: ParamsCTDSEmissions  # Observation model C, R\n\u251c\u2500\u2500 constraints: ParamsCTDSConstraints  # Cell-type constraints\n\u2514\u2500\u2500 observations: Array             # Observed data\n</code></pre>"},{"location":"api/#paramsctdsinitial","title":"ParamsCTDSInitial","text":"<ul> <li><code>mean</code>: Initial state mean \u03bc\u2080 \u2208 \u211d\u1d30</li> <li><code>cov</code>: Initial state covariance \u03a3\u2080 \u2208 \u211d\u1d30\u02e3\u1d30</li> </ul>"},{"location":"api/#paramsctdsdynamics","title":"ParamsCTDSDynamics","text":"<ul> <li><code>weights</code>: Dynamics matrix A \u2208 \u211d\u1d30\u02e3\u1d30</li> <li><code>cov</code>: Process noise covariance Q \u2208 \u211d\u1d30\u02e3\u1d30</li> <li><code>dynamics_mask</code>: Dale's law sign constraints</li> </ul>"},{"location":"api/#paramsctdsemissions","title":"ParamsCTDSEmissions","text":"<ul> <li><code>weights</code>: Emission matrix C \u2208 \u211d\u1d3a\u02e3\u1d30  </li> <li><code>cov</code>: Observation noise covariance R \u2208 \u211d\u1d3a\u02e3\u1d3a</li> <li><code>emission_dims</code>: Cell-type block sizes</li> <li><code>left_padding_dims</code>, <code>right_padding_dims</code>: Block structure</li> </ul>"},{"location":"api/#paramsctdsconstraints","title":"ParamsCTDSConstraints","text":"<ul> <li><code>cell_type_dimensions</code>: Number of cells per type</li> <li><code>cell_sign</code>: Dale's law signs (+1 excitatory, -1 inhibitory)</li> <li><code>num_cell_types</code>: Total number of cell types</li> </ul>"},{"location":"api/#models","title":"Models","text":""},{"location":"api/#ctds-class","title":"CTDS Class","text":"<p>The main model class inheriting from Dynamax SSM:</p> <pre><code>class CTDS(SSM):\n    def __init__(self, state_dim, emission_dim, num_cell_types=2, \n                 cell_type_percentages=None, **kwargs)\n</code></pre> <p>Key Methods:</p> <ul> <li><code>initialize(observations)</code>: Initialize parameters from data</li> <li><code>fit_em(params, observations, num_iters=100, **kwargs)</code>: EM training</li> <li><code>sample(params, key, T)</code>: Generate synthetic data</li> <li><code>forecast(params, emissions, steps, **kwargs)</code>: Missing data prediction</li> <li><code>smoother(params, emissions)</code>: Posterior inference</li> <li><code>filter(params, emissions)</code>: Causal inference</li> </ul>"},{"location":"api/#inference","title":"Inference","text":""},{"location":"api/#inferencebackend-protocol","title":"InferenceBackend Protocol","text":"<p>Abstract interface for inference methods:</p> <pre><code>class InferenceBackend(Protocol):\n    def e_step(params, emissions, inputs=None) -&gt; Tuple[SufficientStats, float]\n    def smoother(params, emissions, inputs=None) -&gt; Tuple[Array, Array]  \n    def filter(params, emissions, inputs=None) -&gt; Tuple[Array, Array]\n    def posterior_sample(key, params, emissions, inputs=None) -&gt; Array\n</code></pre>"},{"location":"api/#dynamaxlgssmbackend","title":"DynamaxLGSSMBackend","text":"<p>Exact inference using Kalman filtering/smoothing:</p> <ul> <li>Uses Dynamax's linear Gaussian SSM routines</li> <li>Converts CTDS parameters via <code>params.to_lgssm()</code></li> <li>Provides optimal posterior estimates under linearity</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<p>Key utility functions for constrained optimization:</p> <ul> <li><code>solve_dale_QP(Q, c, mask, key)</code>: Dale's law constrained QP</li> <li><code>solve_constrained_QP(...)</code>: General constrained optimization</li> <li><code>blockwise_NNLS(...)</code>: Non-negative least squares with block structure</li> <li><code>compute_sufficient_statistics(posterior)</code>: EM sufficient statistics</li> <li><code>estimate_J(Y, mask)</code>: Connectivity estimation from data</li> </ul>"},{"location":"api/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/#basic-workflow","title":"Basic Workflow","text":"<ol> <li>Data preparation: Format neural data as <code>(N, T)</code> array</li> <li>Model creation: Instantiate CTDS with appropriate dimensions</li> <li>Initialization: Use <code>model.initialize(Y)</code> for data-driven initialization  </li> <li>Training: Run <code>model.fit_em(params, Y)</code> with convergence monitoring</li> <li>Inference: Use <code>model.smoother()</code> or <code>model.filter()</code> for posterior estimates</li> <li>Forecasting: Use <code>model.forecast()</code> for missing data prediction</li> </ol>"},{"location":"api/#advanced-configuration","title":"Advanced Configuration","text":"<p>For custom cell-type structures:</p> <pre><code># Define custom constraints\nconstraints = ParamsCTDSConstraints(\n    cell_type_dimensions=[n_E, n_I],\n    cell_sign=signs,  # +1 for E, -1 for I  \n    num_cell_types=2\n)\n\n# Create model with constraints\nmodel = CTDS(state_dim=D, emission_dim=N, constraints=constraints)\n</code></pre>"},{"location":"api/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Enable JIT compilation: Most functions are <code>@jax.jit</code> compatible</li> <li>Use GPU acceleration: Full JAX GPU support</li> <li>Batch processing: Add batch dimension for multiple sequences</li> <li>Memory management: Monitor convergence to avoid over-fitting</li> </ul>"},{"location":"api/#mathematical-details","title":"Mathematical Details","text":""},{"location":"api/#model-equations","title":"Model Equations","text":"<p>State dynamics: \\(\\(x_{t+1} = A x_t + w_t,  w_t ~ N(0, Q)\\)\\)</p> <p>Observations: <pre><code>y_t = C x_t + v_t,     v_t ~ N(0, R)\n</code></pre></p> <p>Factorization: <pre><code>A = U V^T\n</code></pre></p>"},{"location":"api/#constraints","title":"Constraints","text":"<p>Dale's Law: - A[i,j] \u2265 0 if neuron j is excitatory - A[i,j] \u2264 0 if neuron j is inhibitory</p> <p>Cell-Type Structure: - C has block structure respecting cell types - Non-negative emission weights preserve Dale signs</p>"},{"location":"api/#em-algorithm","title":"EM Algorithm","text":"<p>E-step: Compute sufficient statistics via Kalman smoother - E[x_t | y_{1:T}]: posterior means - E[x_t x_t^T | y_{1:T}]: posterior second moments - E[x_t x_{t-1}^T | y_{1:T}]: cross-time moments</p> <p>M-step: Update parameters via constrained optimization - A: Constrained QP with Dale's law - C: Block-wise NNLS with cell-type structure - Q, R: Residual covariance updates - Initial state: First time point posterior</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial release of CTDS package</li> <li>JAX-native implementation of Cell-Type Dynamical Systems</li> <li>EM algorithm for parameter estimation with Dale's law constraints</li> <li>Kalman filtering/smoothing for exact inference</li> <li>Comprehensive documentation with MkDocs</li> <li>Example Jupyter notebooks</li> <li>Full test suite with pytest</li> <li>CI/CD pipeline with GitHub Actions</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li><code>CTDS</code> model class with EM training</li> <li><code>ParamsCTDS</code> parameter structures</li> <li>Dale's law constrained optimization</li> <li>Cell-type factorized connectivity</li> <li>Missing data forecasting</li> <li>Posterior inference and sampling</li> <li>GPU acceleration via JAX</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Mathematical framework documentation</li> <li>API reference with NumPy-style docstrings</li> <li>Installation and quick start guides</li> <li>Example notebooks and tutorials</li> <li>Development and contributing guidelines</li> </ul>"},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial package structure</li> <li>Core CTDS implementation</li> <li>Basic documentation</li> <li>Test framework setup</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to the CTDS project!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/kasherri/CTDS.git\ncd CTDS\n</code></pre></p> </li> <li> <p>Install in development mode:    <pre><code>pip install -e \".[dev,docs,examples]\"\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use: - Black for code formatting - Ruff for linting - mypy for type checking</p> <p>Run all checks: <pre><code>pre-commit run --all-files\n</code></pre></p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Run the test suite: <pre><code>pytest\n</code></pre></p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Build documentation locally: <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests if applicable</li> <li>Ensure all checks pass</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Please use GitHub Issues to report bugs or request features.</p>"},{"location":"examples/","title":"CTDS Examples","text":""},{"location":"examples/#example-1-basic-model-fitting","title":"Example 1: Basic Model Fitting","text":"<pre><code>import jax.numpy as jnp\nimport jax.random as jr\nfrom models import CTDS\n\n# Synthetic data: 50 neurons, 200 time steps\nkey = jr.PRNGKey(42)\nT, N, D = 200, 50, 10\n\n# Create synthetic neural activity (replace with real data)\nY = jr.normal(key, (N, T))\n\n# Create CTDS model\nmodel = CTDS(\n    state_dim=D,\n    emission_dim=N,\n    num_cell_types=2,\n    cell_type_percentages=[0.8, 0.2]  # 80% excitatory, 20% inhibitory\n)\n\n# Initialize parameters\nparams = model.initialize(Y)\nprint(f\"Initialized with {params.dynamics.weights.shape} dynamics matrix\")\n\n# Fit model with EM\nparams_fitted, losses = model.fit_em(\n    params, Y, \n    num_iters=100,\n    tolerance=1e-6,\n    verbose=True\n)\n\nprint(f\"Final log-likelihood: {losses[-1]:.2f}\")\n</code></pre>"},{"location":"examples/#example-2-custom-cell-types","title":"Example 2: Custom Cell Types","text":"<pre><code>from params import ParamsCTDSConstraints\n\n# Define custom cell-type structure\nn_E, n_I = 8, 2  # 8 excitatory, 2 inhibitory\ncell_signs = jnp.array([1]*n_E + [-1]*n_I)  # Dale's law signs\n\nconstraints = ParamsCTDSConstraints(\n    cell_type_dimensions=[n_E, n_I],\n    cell_sign=cell_signs,\n    num_cell_types=2\n)\n\n# Create model with custom constraints\nmodel = CTDS(\n    state_dim=n_E + n_I,\n    emission_dim=25,\n    constraints=constraints\n)\n\n# Rest of fitting process same as Example 1\nparams = model.initialize(Y)\nparams_fitted, losses = model.fit_em(params, Y, num_iters=100)\n</code></pre>"},{"location":"examples/#example-3-posterior-inference","title":"Example 3: Posterior Inference","text":"<pre><code># After fitting model (from Examples 1 or 2)\n\n# Compute posterior state estimates\nposterior_means, posterior_covs = model.smoother(params_fitted, Y)\nprint(f\"Posterior means shape: {posterior_means.shape}\")  # (T, D)\n\n# Compute causal (filtered) estimates  \nfiltered_means, filtered_covs = model.filter(params_fitted, Y)\nprint(f\"Filtered means shape: {filtered_means.shape}\")   # (T, D)\n\n# Sample from posterior\nkey = jr.PRNGKey(123)\nposterior_sample = model.posterior_sample(key, params_fitted, Y)\nprint(f\"Posterior sample shape: {posterior_sample.shape}\")  # (T, D)\n</code></pre>"},{"location":"examples/#example-4-forecasting-missing-data","title":"Example 4: Forecasting Missing Data","text":"<pre><code># Split data into observed and missing portions\nT_obs = T // 2  # Use first half for fitting\nY_obs = Y[:, :T_obs]\nY_true = Y[:, T_obs:]  # Ground truth for validation\n\n# Fit model on observed data\nparams = model.initialize(Y_obs)\nparams_fitted, _ = model.fit_em(params, Y_obs, num_iters=50)\n\n# Forecast missing data\nsteps_ahead = T - T_obs\nforecasted = model.forecast(\n    params_fitted, \n    Y_obs, \n    steps=steps_ahead,\n    method='mean'  # Use posterior mean\n)\n\nprint(f\"Forecasted shape: {forecasted.shape}\")  # (N, steps_ahead)\n\n# Compute forecast error\nmse = jnp.mean((forecasted - Y_true)**2)\nprint(f\"Forecast MSE: {mse:.4f}\")\n</code></pre>"},{"location":"examples/#example-5-generating-synthetic-data","title":"Example 5: Generating Synthetic Data","text":"<pre><code># Generate synthetic data from fitted model\nkey = jr.PRNGKey(456) \nT_synth = 300\n\n# Sample latent states\nsynthetic_states = model.sample(params_fitted, key, T_synth)\nprint(f\"Synthetic states shape: {synthetic_states.shape}\")  # (T_synth, D)\n\n# Generate corresponding observations\nkey, subkey = jr.split(key)\nsynthetic_obs = model.sample(params_fitted, subkey, T_synth, states=synthetic_states)\nprint(f\"Synthetic observations shape: {synthetic_obs.shape}\")  # (N, T_synth)\n</code></pre>"},{"location":"examples/#example-6-batch-processing-multiple-sequences","title":"Example 6: Batch Processing Multiple Sequences","text":"<pre><code># Multiple sequences (e.g., different trials)\nn_trials = 5\nY_batch = jr.normal(jr.PRNGKey(789), (n_trials, N, T))\n\n# Fit to batch data\nbatch_params = []\nfor i in range(n_trials):\n    params = model.initialize(Y_batch[i])\n    params_fitted, _ = model.fit_em(params, Y_batch[i], num_iters=50)\n    batch_params.append(params_fitted)\n\nprint(f\"Fitted {len(batch_params)} models\")\n\n# Analyze dynamics matrices across trials\nA_matrices = [p.dynamics.weights for p in batch_params]\nA_mean = jnp.mean(jnp.stack(A_matrices), axis=0)\nA_std = jnp.std(jnp.stack(A_matrices), axis=0)\n\nprint(f\"Mean dynamics norm: {jnp.linalg.norm(A_mean):.3f}\")\nprint(f\"Dynamics variability: {jnp.mean(A_std):.3f}\")\n</code></pre>"},{"location":"examples/#example-7-model-validation","title":"Example 7: Model Validation","text":"<pre><code># Cross-validation for model selection\nfrom sklearn.model_selection import KFold\n\ndef cross_validate_ctds(Y, state_dims, n_folds=5):\n    \"\"\"Cross-validate CTDS models with different state dimensions.\"\"\"\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    results = {}\n\n    for D in state_dims:\n        fold_scores = []\n\n        for train_idx, test_idx in kf.split(Y.T):  # Split time points\n            Y_train = Y[:, train_idx]\n            Y_test = Y[:, test_idx]\n\n            # Fit model\n            model = CTDS(state_dim=D, emission_dim=N)\n            params = model.initialize(Y_train)\n            params_fitted, _ = model.fit_em(params, Y_train, num_iters=50)\n\n            # Evaluate on test set\n            test_loglik = model.marginal_log_prob(params_fitted, Y_test.T)\n            fold_scores.append(test_loglik)\n\n        results[D] = jnp.mean(jnp.array(fold_scores))\n\n    return results\n\n# Run cross-validation\nstate_dims = [5, 10, 15, 20]\ncv_results = cross_validate_ctds(Y, state_dims)\n\nbest_dim = max(cv_results, key=cv_results.get)\nprint(f\"Best state dimension: {best_dim}\")\nprint(\"CV results:\", cv_results)\n</code></pre>"},{"location":"examples/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"examples/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li>Center neural activity: <code>Y = Y - jnp.mean(Y, axis=1, keepdims=True)</code></li> <li>Scale variance: <code>Y = Y / jnp.std(Y, axis=1, keepdims=True)</code></li> <li>Handle missing data via masking</li> </ul>"},{"location":"examples/#model-selection","title":"Model Selection","text":"<ul> <li>Use cross-validation for state dimension selection</li> <li>Monitor EM convergence via log-likelihood</li> <li>Check Dale's law constraint satisfaction</li> </ul>"},{"location":"examples/#performance","title":"Performance","text":"<ul> <li>Enable JIT compilation for production use</li> <li>Use GPU for large datasets (N &gt; 100, T &gt; 1000)</li> <li>Consider batch processing for multiple sequences</li> </ul>"},{"location":"examples/#diagnostics","title":"Diagnostics","text":"<ul> <li>Plot EM convergence curves</li> <li>Visualize learned connectivity matrices</li> <li>Check residual autocorrelations</li> <li>Validate forecasting performance</li> </ul>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages and navigation.\"\"\"\n</pre> \"\"\"Generate the code reference pages and navigation.\"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre># Only generate the main ctds module reference\nwith mkdocs_gen_files.open(\"reference/ctds.md\", \"w\") as fd:\n    fd.write(\"::: ctds\")\n</pre> # Only generate the main ctds module reference with mkdocs_gen_files.open(\"reference/ctds.md\", \"w\") as fd:     fd.write(\"::: ctds\") In\u00a0[\u00a0]: Copied! <pre># Create a simple navigation file\nwith mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.write(\"* [CTDS](ctds.md)\\n\")\n</pre> # Create a simple navigation file with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:     nav_file.write(\"* [CTDS](ctds.md)\\n\")"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>JAX</li> <li>Dynamax</li> <li>NumPy</li> <li>SciPy</li> </ul>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/kasherri/CTDS.git\ncd CTDS\n</code></pre></p> </li> <li> <p>Install in development mode:    <pre><code>pip install -e \".[dev,docs,examples]\"\n</code></pre></p> </li> </ol>"},{"location":"installation/#install-core-dependencies-only","title":"Install Core Dependencies Only","text":"<p>For basic functionality: <pre><code>pip install -e .\n</code></pre></p>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li><code>examples</code>: Dependencies for running examples and notebooks</li> <li><code>docs</code>: Dependencies for building documentation  </li> <li><code>dev</code>: Development dependencies including testing and linting tools</li> </ul>"},{"location":"installation/#verification","title":"Verification","text":"<p>To verify the installation: <pre><code>import ctds\nprint(ctds.__version__)\n</code></pre></p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with CTDS for modeling neural population dynamics.</p>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<pre><code>import jax.numpy as jnp\nimport jax.random as jr\nfrom ctds import CTDS\n\n# Set up model parameters\nemission_dim = 50  # Number of neurons\ncell_types = 2     # Number of cell types (e.g., excitatory/inhibitory)\nstate_dim = 10     # Latent state dimension\n\n# Create model\nkey = jr.PRNGKey(0)\nmodel = CTDS(\n    emission_dim=emission_dim,\n    cell_types=cell_types,\n    state_dim=state_dim\n)\n\n# Generate synthetic data\nnum_timesteps = 100\nsynthetic_data = model.sample(\n    model.initialize(key, num_timesteps),\n    key=key,\n    num_timesteps=num_timesteps\n)\n\n# Fit the model using EM\nparams = model.initialize(key, num_timesteps)\nfitted_params, _ = model.fit_em(params, synthetic_data, num_iters=50)\n\n# Perform inference\nposterior = model.smoother(fitted_params, synthetic_data)\n</code></pre>"},{"location":"quickstart/#key-concepts","title":"Key Concepts","text":""},{"location":"quickstart/#cell-types","title":"Cell Types","text":"<p>CTDS models different cell types (e.g., excitatory and inhibitory neurons) with distinct dynamics.</p>"},{"location":"quickstart/#dales-principle","title":"Dale's Principle","text":"<p>The model enforces Dale's principle, ensuring neurons maintain consistent excitatory or inhibitory roles.</p>"},{"location":"quickstart/#state-space-model","title":"State Space Model","text":"<p>CTDS uses a linear Gaussian state space model with cell-type-specific constraints.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the examples for detailed use cases</li> <li>Check the API reference for complete documentation</li> <li>See the notebooks for visualization and analysis examples</li> </ul>"},{"location":"contributing/development/","title":"Development Setup","text":""},{"location":"contributing/development/#setting-up-a-development-environment","title":"Setting up a Development Environment","text":""},{"location":"contributing/development/#clone-and-setup","title":"Clone and Setup","text":"<pre><code>git clone https://github.com/kasherri/CTDS.git\ncd CTDS\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e \".[dev,docs,examples]\"\n</code></pre>"},{"location":"contributing/development/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to ensure code quality:</p> <pre><code>pre-commit install\n</code></pre> <p>This will run linting and formatting checks before each commit.</p>"},{"location":"contributing/development/#code-style","title":"Code Style","text":"<p>We use several tools to maintain code quality:</p>"},{"location":"contributing/development/#black-code-formatting","title":"Black (Code Formatting)","text":"<pre><code># Format all code\nblack src/ tests/\n\n# Check formatting without changes\nblack --check src/ tests/\n</code></pre>"},{"location":"contributing/development/#ruff-linting","title":"Ruff (Linting)","text":"<pre><code># Lint all code\nruff check src/ tests/\n\n# Auto-fix issues where possible\nruff check --fix src/ tests/\n</code></pre>"},{"location":"contributing/development/#mypy-type-checking","title":"MyPy (Type Checking)","text":"<pre><code># Type check the source code\nmypy src/\n</code></pre>"},{"location":"contributing/development/#testing","title":"Testing","text":""},{"location":"contributing/development/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=src/ctds\n\n# Run specific test file\npytest tests/test_models.py\n\n# Run with verbose output\npytest -v\n</code></pre>"},{"location":"contributing/development/#test-structure","title":"Test Structure","text":"<p>Tests are organized by module: - <code>tests/test_models.py</code>: Model class tests - <code>tests/test_params.py</code>: Parameter structure tests - <code>tests/test_inference.py</code>: Inference backend tests - <code>tests/test_utils.py</code>: Utility function tests</p>"},{"location":"contributing/development/#documentation","title":"Documentation","text":""},{"location":"contributing/development/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code># Install documentation dependencies\npip install -e \".[docs]\"\n\n# Build and serve locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre> <p>The documentation will be available at http://localhost:8000</p>"},{"location":"contributing/development/#writing-documentation","title":"Writing Documentation","text":"<ul> <li>Use NumPy-style docstrings for all functions and classes</li> <li>Include mathematical equations using LaTeX syntax</li> <li>Add examples to docstrings where helpful</li> <li>Update the user guide for major features</li> </ul>"},{"location":"contributing/development/#contributing-workflow","title":"Contributing Workflow","text":""},{"location":"contributing/development/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/development/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following the style guidelines</li> <li>Add tests for new functionality  </li> <li>Update documentation as needed</li> <li>Ensure all tests pass</li> </ul>"},{"location":"contributing/development/#3-commit-changes","title":"3. Commit Changes","text":"<pre><code># Pre-commit hooks will run automatically\ngit add .\ngit commit -m \"Add feature: descriptive commit message\"\n</code></pre>"},{"location":"contributing/development/#4-push-and-create-pr","title":"4. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a Pull Request on GitHub.</p>"},{"location":"contributing/development/#release-process","title":"Release Process","text":""},{"location":"contributing/development/#version-bumping","title":"Version Bumping","text":"<ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update <code>src/ctds/__init__.py</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create git tag: <code>git tag v0.x.x</code></li> <li>Push: <code>git push origin main --tags</code></li> </ol>"},{"location":"contributing/development/#pypi-release","title":"PyPI Release","text":"<pre><code># Build distributions\npython -m build\n\n# Upload to PyPI (requires credentials)\ntwine upload dist/*\n</code></pre>"},{"location":"contributing/development/#project-structure","title":"Project Structure","text":"<pre><code>CTDS/\n\u251c\u2500\u2500 src/ctds/           # Main package\n\u2502   \u251c\u2500\u2500 __init__.py     # Package initialization\n\u2502   \u251c\u2500\u2500 models.py       # CTDS model implementation\n\u2502   \u251c\u2500\u2500 params.py       # Parameter structures\n\u2502   \u251c\u2500\u2500 inference.py    # Inference backends\n\u2502   \u2514\u2500\u2500 utils.py        # Utility functions\n\u251c\u2500\u2500 tests/              # Test suite\n\u251c\u2500\u2500 docs/               # Documentation source\n\u251c\u2500\u2500 examples/           # Example notebooks\n\u251c\u2500\u2500 .github/workflows/  # CI/CD configuration\n\u251c\u2500\u2500 pyproject.toml      # Package configuration\n\u2514\u2500\u2500 mkdocs.yml         # Documentation configuration\n</code></pre>"},{"location":"examples/ctds_initialization_diagnostics/","title":"CTDS Initialization Diagnostics","text":"In\u00a0[1]: Copied! <pre># Setup and Imports\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Tuple, List, Dict, Optional\nimport time\nfrom functools import partial\n\n# Configure JAX for float64 precision\njax.config.update(\"jax_enable_x64\", True)\n\n# Import CTDS modules\nfrom models import CTDS\nfrom params import (\n    ParamsCTDS, ParamsCTDSInitial, ParamsCTDSDynamics, \n    ParamsCTDSEmissions, ParamsCTDSConstraints\n)\nfrom utlis import estimate_J, blockwise_NMF, NMF\nfrom simulation_utilis import generate_synthetic_data, generate_CTDS_Params\nfrom inference import DynamaxLGSSMBackend\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\nkey = jr.PRNGKey(42)\n\n# Configure plotting\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\nprint(\"\u2705 Setup complete!\")\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"JAX devices: {jax.devices()}\")\nprint(f\"JAX backend: {jax.default_backend()}\")\n</pre> # Setup and Imports import jax import jax.numpy as jnp import jax.random as jr import numpy as np import seaborn as sns  import matplotlib.pyplot as plt import seaborn as sns from typing import Tuple, List, Dict, Optional import time from functools import partial  # Configure JAX for float64 precision jax.config.update(\"jax_enable_x64\", True)  # Import CTDS modules from models import CTDS from params import (     ParamsCTDS, ParamsCTDSInitial, ParamsCTDSDynamics,      ParamsCTDSEmissions, ParamsCTDSConstraints ) from utlis import estimate_J, blockwise_NMF, NMF from simulation_utilis import generate_synthetic_data, generate_CTDS_Params from inference import DynamaxLGSSMBackend  # Set random seeds for reproducibility np.random.seed(42) key = jr.PRNGKey(42)  # Configure plotting plt.style.use('default') sns.set_palette(\"husl\") plt.rcParams['figure.figsize'] = (12, 8) plt.rcParams['font.size'] = 12  print(\"\u2705 Setup complete!\") print(f\"JAX version: {jax.__version__}\") print(f\"JAX devices: {jax.devices()}\") print(f\"JAX backend: {jax.default_backend()}\") <pre>\u2705 Setup complete!\nJAX version: 0.4.38\nJAX devices: [CpuDevice(id=0)]\nJAX backend: cpu\n</pre> In\u00a0[2]: Copied! <pre># Define dimensions and generate synthetic data\nD = 6  # Total state dimension (3 excitatory + 3 inhibitory)\nN = 20  # Number of observed neurons\nT = 200  # Number of time steps\nK = 2  # Number of cell types\n\nprint(\"\ud83d\udd04 Generating synthetic neural data...\")\nprint(\"=\" * 50)\n\n# Generate synthetic data with ground truth parameters\nstates, observations, ctds_model, ctds_params = generate_synthetic_data(\n    num_samples=1,\n    num_timesteps=T,\n    state_dim=D,\n    emission_dim=N,\n    cell_types=K\n)\n\n\n# Extract ground truth parameters for comparison\nA_true = ctds_params.dynamics.weights\nC_true = ctds_params.emissions.weights\nQ_true = ctds_params.dynamics.cov\nR_true = ctds_params.emissions.cov\n\n# Extract cell type information\ncell_constraints = ctds_model.constraints\ncell_type_mask = cell_constraints.cell_type_mask\ncell_types = cell_constraints.cell_types\ncell_type_dimensions = cell_constraints.cell_type_dimensions\n\ndef generate_states_from_dynamics(\n    A: jnp.ndarray,\n    Q: jnp.ndarray,\n    num_timesteps: int,\n    key: jax.random.PRNGKey,\n    x0: Optional[jnp.ndarray] = None\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Generate states using dynamics: x_{t+1} = A @ x_t + w_t, where w_t ~ N(0, Q)\n    \n    Args:\n        A: (D, D) state transition matrix\n        Q: (D, D) or (D,) state noise covariance\n        x0: (D,) initial state\n        num_timesteps: Number of time steps\n        key: Random key\n        \n    Returns:\n        states: (D, T) array of states\n    \"\"\"\n    \n    D = A.shape[0]\n    if x0 is None:\n        x0 = jax.random.normal(key, (D,))\n\n    # Handle both diagonal and full covariance\n    if Q.ndim == 1:\n        # Diagonal covariance\n        noise_std = jnp.sqrt(Q)\n        keys = jr.split(key, num_timesteps)\n        state_noise = jnp.array([jr.normal(keys[t], (D,)) * noise_std for t in range(num_timesteps)]).T\n    else:\n        # Full covariance\n        keys = jr.split(key, num_timesteps)\n        state_noise = jnp.array([jr.multivariate_normal(keys[t], jnp.zeros(D), Q) for t in range(num_timesteps)]).T\n    \n    # Pre-allocate states\n    states = jnp.zeros((D, num_timesteps))\n    states = states.at[:, 0].set(x0)\n    \n    # Generate states iteratively\n    for t in range(num_timesteps - 1):\n        next_state = A @ states[:, t] + state_noise[:, t]\n        states = states.at[:, t + 1].set(next_state)\n    \n    return states\n    \n\ndef generate_observations_from_states(\n    states: jnp.ndarray,\n    C: jnp.ndarray,\n    R: jnp.ndarray,\n    key: jax.random.PRNGKey = jr.PRNGKey(42)\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Generate observations from states using the emission model: y_t = C @ x_t + v_t\n    where v_t ~ N(0, R) is emission noise.\n    \n    Args:\n        states: (D, T) array of latent states over time\n        C: (N, D) emission matrix mapping states to observations\n        R: (N, N) emission covariance matrix or (N,) diagonal covariance\n        key: Random key for noise generation\n        \n    Returns:\n        observations: (N, T) array of observations\n    \"\"\"\n    \n    D, T = states.shape\n    N = C.shape[0]\n    \n    # Validate dimensions\n    #assert C.shape == (N, D), f\"C shape {C.shape} incompatible with states shape {states.shape}\"\n    \n    # Handle both full covariance and diagonal covariance\n    if R.ndim == 1:\n        # Diagonal covariance - R is (N,) vector of variances\n        assert R.shape == (N,), f\"Diagonal R shape {R.shape} incompatible with N={N}\"\n        noise_std = jnp.sqrt(R)\n        \n        # Generate independent noise for each dimension\n        keys = jr.split(key, T)\n        noise = jnp.array([jr.normal(keys[t], (N,)) * noise_std for t in range(T)]).T\n        \n    elif R.ndim == 2:\n        # Full covariance matrix - R is (N, N)\n        assert R.shape == (N, N), f\"Full R shape {R.shape} incompatible with N={N}\"\n        \n        # Generate multivariate normal noise\n        keys = jr.split(key, T)\n        noise = jnp.array([jr.multivariate_normal(keys[t], jnp.zeros(N), R) for t in range(T)]).T\n        \n    else:\n        raise ValueError(f\"R must be 1D (diagonal) or 2D (full covariance), got shape {R.shape}\")\n    \n    # Generate observations: y_t = C @ x_t + v_t\n    linear_observations = C @ states  # (N, T)\n    observations = linear_observations + noise  # (N, T)\n    \n    return observations.T\n\ndef simulate_observations_from_J(\n    J: jnp.ndarray,\n    num_timesteps: int,\n    initial_obs: Optional[jnp.ndarray] = None,\n    noise_std: float = 0.1,\n    key: jax.random.PRNGKey = jr.PRNGKey(42)\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Simulate observations using autoregressive dynamics: y_t = J @ y_{t-1} + noise\n    \n    Args:\n        J: (N, N) connectivity/transition matrix\n        num_timesteps: Number of time steps to simulate\n        initial_obs: (N,) initial observation. If None, random initialization\n        noise_std: Standard deviation of observation noise\n        key: Random key for noise generation\n        \n    Returns:\n        observations: (N, T) array of simulated observations over time\n    \"\"\"\n    \n    N = J.shape[0]\n    \n    # Validate J matrix\n    assert J.shape == (N, N), f\"J must be square, got shape {J.shape}\"\n    \n    keys = jr.split(key, num_timesteps + 1)\n    \n    # Initialize observations\n    if initial_obs is None:\n        initial_obs = jr.normal(keys[0], (N,)) * 0.5\n    else:\n        assert initial_obs.shape == (N,), f\"Initial obs shape {initial_obs.shape} incompatible with J shape {J.shape}\"\n    \n    # Pre-allocate observations array\n    observations = jnp.zeros((N, num_timesteps))\n    observations = observations.at[:, 0].set(initial_obs)\n    \n    # Simulate dynamics: y_t = J @ y_{t-1} + noise\n    for t in range(1, num_timesteps):\n        # Linear dynamics\n        predicted_obs = J @ observations[:, t-1]\n        \n        # Add noise\n        noise = jr.normal(keys[t], (N,)) * noise_std\n        next_obs = predicted_obs + noise\n        \n        # Store observation\n        observations = observations.at[:, t].set(next_obs)\n    \n    return observations.T\n\n\n\n\nprint(f\"\ud83d\udcca Data Generated:\")\nprint(f\"  \u2022 Observations shape: {observations.shape}\")\nprint(f\"  \u2022 States shape: {states.shape}\")\nprint(f\"  \u2022 Cell types: {cell_types}\")\nprint(f\"  \u2022 Cell type dimensions: {cell_type_dimensions}\")\nprint(f\"  \u2022 Cell type mask: {cell_type_mask}\")\nprint(f\"  \u2022 Excitatory neurons: {jnp.sum(cell_type_mask == 0)}\")\nprint(f\"  \u2022 Inhibitory neurons: {jnp.sum(cell_type_mask == 1)}\")\n\n# Create ground truth J matrix from C and A\n# J represents the effective connectivity: Y_t+1 \u2248 J @ Y_t\n# For linear-Gaussian SSM: Y = CA(CA)^\u2020Y + noise\n# So J \u2248 CA(CA)^\u2020 where (CA)^\u2020 is the pseudoinverse\nCA = C_true @ A_true\nCA_pinv = jnp.linalg.pinv(CA)\nJ_true = CA @ CA_pinv\n\nobservations = generate_observations_from_states(states.T, C_true, R_true, key=jr.PRNGKey(42))\n\nprint(f\"\\n\ud83c\udfaf Ground Truth J Matrix:\")\nprint(f\"  \u2022 Shape: {J_true.shape}\")\nprint(f\"  \u2022 Norm: {jnp.linalg.norm(J_true, 'fro'):.3f}\")\nprint(f\"  \u2022 Condition number: {jnp.linalg.cond(J_true):.2e}\")\n\nplt.figure(figsize=(12, 4))\nsns.heatmap(np.array(A_true),center=0, cmap='bwr', cbar=True)\nplt.title('Dynamics)')\nplt.xlabel('Time')\nplt.ylabel('Neuron')\nplt.show()\n\nplt.figure(figsize=(12, 4))\nsns.heatmap(np.array(observations),center=0, cmap='bwr', cbar=True)\nplt.title('Synthetic Observations ')\nplt.ylabel('Time')\nplt.xlabel('Neuron')\nplt.show()\n\nplt.figure(figsize=(12, 4))\nsns.heatmap(np.array(J_true),center=0, cmap='bwr', cbar=True)\nplt.title('Ground Truth J Matrix (Neurons x Neurons)')\nplt.xlabel('Target Neuron')\nplt.ylabel('Source Neuron')\nplt.show()\n\nobs=simulate_observations_from_J(J_true, num_timesteps=T, noise_std=0.1)\nprint(f\"  \u2022 Observations shape: {obs.shape}\")\nplt.figure(figsize=(12, 4))\nsns.heatmap(np.array(obs),center=0, cmap='bwr', cbar=True)\nplt.title('Synthetic Observations ')\nplt.ylabel('Time')\nplt.xlabel('Neuron')\nplt.show()\n</pre> # Define dimensions and generate synthetic data D = 6  # Total state dimension (3 excitatory + 3 inhibitory) N = 20  # Number of observed neurons T = 200  # Number of time steps K = 2  # Number of cell types  print(\"\ud83d\udd04 Generating synthetic neural data...\") print(\"=\" * 50)  # Generate synthetic data with ground truth parameters states, observations, ctds_model, ctds_params = generate_synthetic_data(     num_samples=1,     num_timesteps=T,     state_dim=D,     emission_dim=N,     cell_types=K )   # Extract ground truth parameters for comparison A_true = ctds_params.dynamics.weights C_true = ctds_params.emissions.weights Q_true = ctds_params.dynamics.cov R_true = ctds_params.emissions.cov  # Extract cell type information cell_constraints = ctds_model.constraints cell_type_mask = cell_constraints.cell_type_mask cell_types = cell_constraints.cell_types cell_type_dimensions = cell_constraints.cell_type_dimensions  def generate_states_from_dynamics(     A: jnp.ndarray,     Q: jnp.ndarray,     num_timesteps: int,     key: jax.random.PRNGKey,     x0: Optional[jnp.ndarray] = None ) -&gt; jnp.ndarray:     \"\"\"     Generate states using dynamics: x_{t+1} = A @ x_t + w_t, where w_t ~ N(0, Q)          Args:         A: (D, D) state transition matrix         Q: (D, D) or (D,) state noise covariance         x0: (D,) initial state         num_timesteps: Number of time steps         key: Random key              Returns:         states: (D, T) array of states     \"\"\"          D = A.shape[0]     if x0 is None:         x0 = jax.random.normal(key, (D,))      # Handle both diagonal and full covariance     if Q.ndim == 1:         # Diagonal covariance         noise_std = jnp.sqrt(Q)         keys = jr.split(key, num_timesteps)         state_noise = jnp.array([jr.normal(keys[t], (D,)) * noise_std for t in range(num_timesteps)]).T     else:         # Full covariance         keys = jr.split(key, num_timesteps)         state_noise = jnp.array([jr.multivariate_normal(keys[t], jnp.zeros(D), Q) for t in range(num_timesteps)]).T          # Pre-allocate states     states = jnp.zeros((D, num_timesteps))     states = states.at[:, 0].set(x0)          # Generate states iteratively     for t in range(num_timesteps - 1):         next_state = A @ states[:, t] + state_noise[:, t]         states = states.at[:, t + 1].set(next_state)          return states       def generate_observations_from_states(     states: jnp.ndarray,     C: jnp.ndarray,     R: jnp.ndarray,     key: jax.random.PRNGKey = jr.PRNGKey(42) ) -&gt; jnp.ndarray:     \"\"\"     Generate observations from states using the emission model: y_t = C @ x_t + v_t     where v_t ~ N(0, R) is emission noise.          Args:         states: (D, T) array of latent states over time         C: (N, D) emission matrix mapping states to observations         R: (N, N) emission covariance matrix or (N,) diagonal covariance         key: Random key for noise generation              Returns:         observations: (N, T) array of observations     \"\"\"          D, T = states.shape     N = C.shape[0]          # Validate dimensions     #assert C.shape == (N, D), f\"C shape {C.shape} incompatible with states shape {states.shape}\"          # Handle both full covariance and diagonal covariance     if R.ndim == 1:         # Diagonal covariance - R is (N,) vector of variances         assert R.shape == (N,), f\"Diagonal R shape {R.shape} incompatible with N={N}\"         noise_std = jnp.sqrt(R)                  # Generate independent noise for each dimension         keys = jr.split(key, T)         noise = jnp.array([jr.normal(keys[t], (N,)) * noise_std for t in range(T)]).T              elif R.ndim == 2:         # Full covariance matrix - R is (N, N)         assert R.shape == (N, N), f\"Full R shape {R.shape} incompatible with N={N}\"                  # Generate multivariate normal noise         keys = jr.split(key, T)         noise = jnp.array([jr.multivariate_normal(keys[t], jnp.zeros(N), R) for t in range(T)]).T              else:         raise ValueError(f\"R must be 1D (diagonal) or 2D (full covariance), got shape {R.shape}\")          # Generate observations: y_t = C @ x_t + v_t     linear_observations = C @ states  # (N, T)     observations = linear_observations + noise  # (N, T)          return observations.T  def simulate_observations_from_J(     J: jnp.ndarray,     num_timesteps: int,     initial_obs: Optional[jnp.ndarray] = None,     noise_std: float = 0.1,     key: jax.random.PRNGKey = jr.PRNGKey(42) ) -&gt; jnp.ndarray:     \"\"\"     Simulate observations using autoregressive dynamics: y_t = J @ y_{t-1} + noise          Args:         J: (N, N) connectivity/transition matrix         num_timesteps: Number of time steps to simulate         initial_obs: (N,) initial observation. If None, random initialization         noise_std: Standard deviation of observation noise         key: Random key for noise generation              Returns:         observations: (N, T) array of simulated observations over time     \"\"\"          N = J.shape[0]          # Validate J matrix     assert J.shape == (N, N), f\"J must be square, got shape {J.shape}\"          keys = jr.split(key, num_timesteps + 1)          # Initialize observations     if initial_obs is None:         initial_obs = jr.normal(keys[0], (N,)) * 0.5     else:         assert initial_obs.shape == (N,), f\"Initial obs shape {initial_obs.shape} incompatible with J shape {J.shape}\"          # Pre-allocate observations array     observations = jnp.zeros((N, num_timesteps))     observations = observations.at[:, 0].set(initial_obs)          # Simulate dynamics: y_t = J @ y_{t-1} + noise     for t in range(1, num_timesteps):         # Linear dynamics         predicted_obs = J @ observations[:, t-1]                  # Add noise         noise = jr.normal(keys[t], (N,)) * noise_std         next_obs = predicted_obs + noise                  # Store observation         observations = observations.at[:, t].set(next_obs)          return observations.T     print(f\"\ud83d\udcca Data Generated:\") print(f\"  \u2022 Observations shape: {observations.shape}\") print(f\"  \u2022 States shape: {states.shape}\") print(f\"  \u2022 Cell types: {cell_types}\") print(f\"  \u2022 Cell type dimensions: {cell_type_dimensions}\") print(f\"  \u2022 Cell type mask: {cell_type_mask}\") print(f\"  \u2022 Excitatory neurons: {jnp.sum(cell_type_mask == 0)}\") print(f\"  \u2022 Inhibitory neurons: {jnp.sum(cell_type_mask == 1)}\")  # Create ground truth J matrix from C and A # J represents the effective connectivity: Y_t+1 \u2248 J @ Y_t # For linear-Gaussian SSM: Y = CA(CA)^\u2020Y + noise # So J \u2248 CA(CA)^\u2020 where (CA)^\u2020 is the pseudoinverse CA = C_true @ A_true CA_pinv = jnp.linalg.pinv(CA) J_true = CA @ CA_pinv  observations = generate_observations_from_states(states.T, C_true, R_true, key=jr.PRNGKey(42))  print(f\"\\n\ud83c\udfaf Ground Truth J Matrix:\") print(f\"  \u2022 Shape: {J_true.shape}\") print(f\"  \u2022 Norm: {jnp.linalg.norm(J_true, 'fro'):.3f}\") print(f\"  \u2022 Condition number: {jnp.linalg.cond(J_true):.2e}\")  plt.figure(figsize=(12, 4)) sns.heatmap(np.array(A_true),center=0, cmap='bwr', cbar=True) plt.title('Dynamics)') plt.xlabel('Time') plt.ylabel('Neuron') plt.show()  plt.figure(figsize=(12, 4)) sns.heatmap(np.array(observations),center=0, cmap='bwr', cbar=True) plt.title('Synthetic Observations ') plt.ylabel('Time') plt.xlabel('Neuron') plt.show()  plt.figure(figsize=(12, 4)) sns.heatmap(np.array(J_true),center=0, cmap='bwr', cbar=True) plt.title('Ground Truth J Matrix (Neurons x Neurons)') plt.xlabel('Target Neuron') plt.ylabel('Source Neuron') plt.show()  obs=simulate_observations_from_J(J_true, num_timesteps=T, noise_std=0.1) print(f\"  \u2022 Observations shape: {obs.shape}\") plt.figure(figsize=(12, 4)) sns.heatmap(np.array(obs),center=0, cmap='bwr', cbar=True) plt.title('Synthetic Observations ') plt.ylabel('Time') plt.xlabel('Neuron') plt.show() <pre>\ud83d\udd04 Generating synthetic neural data...\n==================================================\n\ud83d\udcca Data Generated:\n  \u2022 Observations shape: (200, 20)\n  \u2022 States shape: (200, 6)\n  \u2022 Cell types: [0 1]\n  \u2022 Cell type dimensions: [3 3]\n  \u2022 Cell type mask: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n  \u2022 Excitatory neurons: 10\n  \u2022 Inhibitory neurons: 10\n\n\ud83c\udfaf Ground Truth J Matrix:\n  \u2022 Shape: (20, 20)\n  \u2022 Norm: 2.449\n  \u2022 Condition number: 5.08e+16\n</pre> <pre>  \u2022 Observations shape: (200, 20)\n</pre> In\u00a0[3]: Copied! <pre>print(\"\ud83d\udd0d ESTIMATING J MATRIX FROM NEURAL ACTIVITY\")\nprint(\"=\" * 60)\n\n# Create Dale's law mask (True for excitatory, False for inhibitory)\ndale_mask = (cell_type_mask == 0)  # Excitatory = True, Inhibitory = False\nprint(dale_mask.shape)\nprint(observations.shape)\n\n# Estimate J matrix using constrained optimization\nstart_time = time.time()\nJ_estimated = estimate_J(observations.T, dale_mask)\nestimation_time = time.time() - start_time\n\nprint(f\"\u2705 J Matrix Estimation Completed:\")\nprint(f\"  \u2022 Estimation time: {estimation_time:.3f} seconds\")\nprint(f\"  \u2022 Estimated J shape: {J_estimated.shape}\")\nprint(f\"  \u2022 Estimated J norm: {jnp.linalg.norm(J_estimated, 'fro'):.3f}\")\nprint(f\"  \u2022 Condition number: {jnp.linalg.cond(J_estimated):.2e}\")\n\n# Compute quantitative metrics\ndef compute_j_metrics(J_est, J_true, observations):\n    \"\"\"Compute comprehensive metrics for J matrix estimation.\"\"\"\n    \n    # 1. Pearson correlation between flattened matrices\n    J_est_flat = J_est.flatten()\n    J_true_flat = J_true.flatten()\n    correlation = jnp.corrcoef(J_est_flat, J_true_flat)[0, 1]\n    \n    # 2. Frobenius norm relative error\n    frobenius_error = jnp.linalg.norm(J_est - J_true, 'fro') / jnp.linalg.norm(J_true, 'fro')\n    \n    # 3. One-step prediction test\n    Y_past = observations[:, :-1]  # Shape: (N, T-1)\n    Y_future_true = observations[:, 1:]  # Shape: (N, T-1)\n    Y_future_pred = J_est @ Y_past  # Predicted future\n    \n    # Mean squared prediction error\n    mse_prediction = jnp.mean((Y_future_pred - Y_future_true)**2)\n    \n    # Baseline MSE (predicting with mean)\n    Y_mean = jnp.mean(observations, axis=1, keepdims=True)\n    mse_baseline = jnp.mean((Y_mean - Y_future_true)**2)\n    \n    # Normalized MSE\n    normalized_mse = mse_prediction / mse_baseline\n    \n    # R\u00b2 for prediction\n    ss_res = jnp.sum((Y_future_true - Y_future_pred)**2)\n    ss_tot = jnp.sum((Y_future_true - jnp.mean(Y_future_true))**2)\n    r2_prediction = 1 - (ss_res / ss_tot)\n    \n    return {\n        'correlation': correlation,\n        'frobenius_error': frobenius_error,\n        'mse_prediction': mse_prediction,\n        'mse_baseline': mse_baseline,\n        'normalized_mse': normalized_mse,\n        'r2_prediction': r2_prediction\n    }\n\n# Compute metrics\nj_metrics = compute_j_metrics(J_estimated, J_true, observations.T)\n\nprint(f\"\\n\ud83d\udcca J Matrix Quality Metrics:\")\nprint(f\"  \u2022 Pearson correlation: {j_metrics['correlation']:.4f}\")\nprint(f\"  \u2022 Relative Frobenius error: {j_metrics['frobenius_error']:.4f}\")\nprint(f\"  \u2022 Prediction MSE: {j_metrics['mse_prediction']:.6f}\")\nprint(f\"  \u2022 Baseline MSE: {j_metrics['mse_baseline']:.6f}\")\nprint(f\"  \u2022 Normalized MSE: {j_metrics['normalized_mse']:.4f}\")\nprint(f\"  \u2022 Prediction R\u00b2: {j_metrics['r2_prediction']:.4f}\")\n\n\nJ_obs = simulate_observations_from_J(J_estimated, num_timesteps=T, noise_std=0.1)\nprint(f\"  \u2022 Observations shape: {J_obs.shape}\")\nplt.figure(figsize=(12, 4))\nsns.heatmap(np.array(J_obs),center=0, cmap='bwr', cbar=True)\nplt.title('Synthetic Observations ')\nplt.ylabel('Time')\nplt.xlabel('Neuron')\nplt.show()\n</pre> print(\"\ud83d\udd0d ESTIMATING J MATRIX FROM NEURAL ACTIVITY\") print(\"=\" * 60)  # Create Dale's law mask (True for excitatory, False for inhibitory) dale_mask = (cell_type_mask == 0)  # Excitatory = True, Inhibitory = False print(dale_mask.shape) print(observations.shape)  # Estimate J matrix using constrained optimization start_time = time.time() J_estimated = estimate_J(observations.T, dale_mask) estimation_time = time.time() - start_time  print(f\"\u2705 J Matrix Estimation Completed:\") print(f\"  \u2022 Estimation time: {estimation_time:.3f} seconds\") print(f\"  \u2022 Estimated J shape: {J_estimated.shape}\") print(f\"  \u2022 Estimated J norm: {jnp.linalg.norm(J_estimated, 'fro'):.3f}\") print(f\"  \u2022 Condition number: {jnp.linalg.cond(J_estimated):.2e}\")  # Compute quantitative metrics def compute_j_metrics(J_est, J_true, observations):     \"\"\"Compute comprehensive metrics for J matrix estimation.\"\"\"          # 1. Pearson correlation between flattened matrices     J_est_flat = J_est.flatten()     J_true_flat = J_true.flatten()     correlation = jnp.corrcoef(J_est_flat, J_true_flat)[0, 1]          # 2. Frobenius norm relative error     frobenius_error = jnp.linalg.norm(J_est - J_true, 'fro') / jnp.linalg.norm(J_true, 'fro')          # 3. One-step prediction test     Y_past = observations[:, :-1]  # Shape: (N, T-1)     Y_future_true = observations[:, 1:]  # Shape: (N, T-1)     Y_future_pred = J_est @ Y_past  # Predicted future          # Mean squared prediction error     mse_prediction = jnp.mean((Y_future_pred - Y_future_true)**2)          # Baseline MSE (predicting with mean)     Y_mean = jnp.mean(observations, axis=1, keepdims=True)     mse_baseline = jnp.mean((Y_mean - Y_future_true)**2)          # Normalized MSE     normalized_mse = mse_prediction / mse_baseline          # R\u00b2 for prediction     ss_res = jnp.sum((Y_future_true - Y_future_pred)**2)     ss_tot = jnp.sum((Y_future_true - jnp.mean(Y_future_true))**2)     r2_prediction = 1 - (ss_res / ss_tot)          return {         'correlation': correlation,         'frobenius_error': frobenius_error,         'mse_prediction': mse_prediction,         'mse_baseline': mse_baseline,         'normalized_mse': normalized_mse,         'r2_prediction': r2_prediction     }  # Compute metrics j_metrics = compute_j_metrics(J_estimated, J_true, observations.T)  print(f\"\\n\ud83d\udcca J Matrix Quality Metrics:\") print(f\"  \u2022 Pearson correlation: {j_metrics['correlation']:.4f}\") print(f\"  \u2022 Relative Frobenius error: {j_metrics['frobenius_error']:.4f}\") print(f\"  \u2022 Prediction MSE: {j_metrics['mse_prediction']:.6f}\") print(f\"  \u2022 Baseline MSE: {j_metrics['mse_baseline']:.6f}\") print(f\"  \u2022 Normalized MSE: {j_metrics['normalized_mse']:.4f}\") print(f\"  \u2022 Prediction R\u00b2: {j_metrics['r2_prediction']:.4f}\")   J_obs = simulate_observations_from_J(J_estimated, num_timesteps=T, noise_std=0.1) print(f\"  \u2022 Observations shape: {J_obs.shape}\") plt.figure(figsize=(12, 4)) sns.heatmap(np.array(J_obs),center=0, cmap='bwr', cbar=True) plt.title('Synthetic Observations ') plt.ylabel('Time') plt.xlabel('Neuron') plt.show() <pre>\ud83d\udd0d ESTIMATING J MATRIX FROM NEURAL ACTIVITY\n============================================================\n(20,)\n(200, 20)\n\u2705 J Matrix Estimation Completed:\n  \u2022 Estimation time: 2.669 seconds\n  \u2022 Estimated J shape: (20, 20)\n  \u2022 Estimated J norm: 2.141\n  \u2022 Condition number: 2.33e+02\n\n\ud83d\udcca J Matrix Quality Metrics:\n  \u2022 Pearson correlation: 0.1911\n  \u2022 Relative Frobenius error: 1.1704\n  \u2022 Prediction MSE: 0.153589\n  \u2022 Baseline MSE: 0.269466\n  \u2022 Normalized MSE: 0.5700\n  \u2022 Prediction R\u00b2: 0.4312\n  \u2022 Observations shape: (200, 20)\n</pre> In\u00a0[4]: Copied! <pre># Verify Dale's law constraints\nprint(\"\\n\ud83e\uddec DALE'S LAW VERIFICATION\")\nprint(\"=\" * 40)\n\ndef verify_dales_law(J, dale_mask):\n    \"\"\"Verify Dale's law constraints on estimated J matrix.\"\"\"\n    \n    # Excitatory columns should have all non-negative entries\n    excitatory_indices = jnp.where(dale_mask)[0]\n    inhibitory_indices = jnp.where(~dale_mask)[0]\n    \n    # Check excitatory columns (all entries should be &gt;= 0)\n    excitatory_violations = []\n    for col_idx in excitatory_indices:\n        column = J[:, col_idx]\n        negative_entries = jnp.sum(column &lt; -1e-10)  # Allow small numerical errors\n        if negative_entries &gt; 0:\n            excitatory_violations.append((col_idx, negative_entries, jnp.min(column)))\n    \n    # Check inhibitory columns (all entries should be &lt;= 0)\n    inhibitory_violations = []\n    for col_idx in inhibitory_indices:\n        column = J[:, col_idx]\n        positive_entries = jnp.sum(column &gt; 1e-10)  # Allow small numerical errors\n        if positive_entries &gt; 0:\n            inhibitory_violations.append((col_idx, positive_entries, jnp.max(column)))\n    \n    return {\n        'excitatory_violations': excitatory_violations,\n        'inhibitory_violations': inhibitory_violations,\n        'total_excitatory_cols': len(excitatory_indices),\n        'total_inhibitory_cols': len(inhibitory_indices)\n    }\n\n# Verify Dale's law for estimated J\ndale_results = verify_dales_law(J_estimated, dale_mask)\n\nprint(f\"Excitatory columns ({dale_results['total_excitatory_cols']} total):\")\nif len(dale_results['excitatory_violations']) == 0:\n    print(f\"  \u2705 All excitatory columns satisfy Dale's law (all entries \u2265 0)\")\nelse:\n    print(f\"  \u274c {len(dale_results['excitatory_violations'])} violations found:\")\n    for col_idx, neg_count, min_val in dale_results['excitatory_violations']:\n        print(f\"    Column {col_idx}: {neg_count} negative entries, min = {min_val:.6f}\")\n\nprint(f\"\\nInhibitory columns ({dale_results['total_inhibitory_cols']} total):\")\nif len(dale_results['inhibitory_violations']) == 0:\n    print(f\"  \u2705 All inhibitory columns satisfy Dale's law (all entries \u2264 0)\")\nelse:\n    print(f\"  \u274c {len(dale_results['inhibitory_violations'])} violations found:\")\n    for col_idx, pos_count, max_val in dale_results['inhibitory_violations']:\n        print(f\"    Column {col_idx}: {pos_count} positive entries, max = {max_val:.6f}\")\n\n# Compute Dale's law compliance percentage\ntotal_violations = len(dale_results['excitatory_violations']) + len(dale_results['inhibitory_violations'])\ntotal_columns = dale_results['total_excitatory_cols'] + dale_results['total_inhibitory_cols']\ncompliance_rate = (total_columns - total_violations) / total_columns * 100\n\nprint(f\"\\n\ud83d\udcc8 Dale's Law Compliance: {compliance_rate:.1f}% ({total_columns - total_violations}/{total_columns} columns)\")\n</pre> # Verify Dale's law constraints print(\"\\n\ud83e\uddec DALE'S LAW VERIFICATION\") print(\"=\" * 40)  def verify_dales_law(J, dale_mask):     \"\"\"Verify Dale's law constraints on estimated J matrix.\"\"\"          # Excitatory columns should have all non-negative entries     excitatory_indices = jnp.where(dale_mask)[0]     inhibitory_indices = jnp.where(~dale_mask)[0]          # Check excitatory columns (all entries should be &gt;= 0)     excitatory_violations = []     for col_idx in excitatory_indices:         column = J[:, col_idx]         negative_entries = jnp.sum(column &lt; -1e-10)  # Allow small numerical errors         if negative_entries &gt; 0:             excitatory_violations.append((col_idx, negative_entries, jnp.min(column)))          # Check inhibitory columns (all entries should be &lt;= 0)     inhibitory_violations = []     for col_idx in inhibitory_indices:         column = J[:, col_idx]         positive_entries = jnp.sum(column &gt; 1e-10)  # Allow small numerical errors         if positive_entries &gt; 0:             inhibitory_violations.append((col_idx, positive_entries, jnp.max(column)))          return {         'excitatory_violations': excitatory_violations,         'inhibitory_violations': inhibitory_violations,         'total_excitatory_cols': len(excitatory_indices),         'total_inhibitory_cols': len(inhibitory_indices)     }  # Verify Dale's law for estimated J dale_results = verify_dales_law(J_estimated, dale_mask)  print(f\"Excitatory columns ({dale_results['total_excitatory_cols']} total):\") if len(dale_results['excitatory_violations']) == 0:     print(f\"  \u2705 All excitatory columns satisfy Dale's law (all entries \u2265 0)\") else:     print(f\"  \u274c {len(dale_results['excitatory_violations'])} violations found:\")     for col_idx, neg_count, min_val in dale_results['excitatory_violations']:         print(f\"    Column {col_idx}: {neg_count} negative entries, min = {min_val:.6f}\")  print(f\"\\nInhibitory columns ({dale_results['total_inhibitory_cols']} total):\") if len(dale_results['inhibitory_violations']) == 0:     print(f\"  \u2705 All inhibitory columns satisfy Dale's law (all entries \u2264 0)\") else:     print(f\"  \u274c {len(dale_results['inhibitory_violations'])} violations found:\")     for col_idx, pos_count, max_val in dale_results['inhibitory_violations']:         print(f\"    Column {col_idx}: {pos_count} positive entries, max = {max_val:.6f}\")  # Compute Dale's law compliance percentage total_violations = len(dale_results['excitatory_violations']) + len(dale_results['inhibitory_violations']) total_columns = dale_results['total_excitatory_cols'] + dale_results['total_inhibitory_cols'] compliance_rate = (total_columns - total_violations) / total_columns * 100  print(f\"\\n\ud83d\udcc8 Dale's Law Compliance: {compliance_rate:.1f}% ({total_columns - total_violations}/{total_columns} columns)\") <pre>\n\ud83e\uddec DALE'S LAW VERIFICATION\n========================================\nExcitatory columns (10 total):\n  \u274c 1 violations found:\n    Column 9: 1 negative entries, min = -0.000000\n\nInhibitory columns (10 total):\n  \u274c 2 violations found:\n    Column 11: 1 positive entries, max = 0.000000\n    Column 18: 1 positive entries, max = 0.000000\n\n\ud83d\udcc8 Dale's Law Compliance: 85.0% (17/20 columns)\n</pre> In\u00a0[5]: Copied! <pre># Visualize J matrices comparison\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Determine color scale for consistent comparison\nvmin = min(jnp.min(J_true), jnp.min(J_estimated))\nvmax = max(jnp.max(J_true), jnp.max(J_estimated))\n\n# Plot ground truth J\nim1 = axes[0, 0].imshow(J_true, cmap='bwr',  vmin=vmin, vmax=vmax, aspect='auto')\naxes[0, 0].set_title('Ground Truth J Matrix')\naxes[0, 0].set_xlabel('Source Neuron')\naxes[0, 0].set_ylabel('Target Neuron')\nplt.colorbar(im1, ax=axes[0, 0])\n\n# Plot estimated J\nim2 = axes[0, 1].imshow(J_estimated, cmap='bwr', vmin=vmin, vmax=vmax, aspect='auto')\naxes[0, 1].set_title('Estimated J Matrix')\naxes[0, 1].set_xlabel('Source Neuron')\naxes[0, 1].set_ylabel('Target Neuron')\nplt.colorbar(im2, ax=axes[0, 1])\n\n# Plot difference (residual)\ndiff = J_estimated - J_true\nim3 = axes[0, 2].imshow(diff, cmap='bwr', vmin=-jnp.max(jnp.abs(diff)), \n                       vmax=jnp.max(jnp.abs(diff)), aspect='auto')\naxes[0, 2].set_title('Difference (Estimated - True)')\naxes[0, 2].set_xlabel('Source Neuron')\naxes[0, 2].set_ylabel('Target Neuron')\nplt.colorbar(im3, ax=axes[0, 2])\n\n# Scatter plot: estimated vs true\naxes[1, 0].scatter(J_true.flatten(), J_estimated.flatten(), alpha=0.6, s=20)\naxes[1, 0].plot([vmin, vmax], [vmin, vmax], 'r--', alpha=0.8, label='Perfect agreement')\naxes[1, 0].set_xlabel('True J values')\naxes[1, 0].set_ylabel('Estimated J values')\naxes[1, 0].set_title(f'J Values Comparison\\n(r = {j_metrics[\"correlation\"]:.3f})')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Histogram of differences\naxes[1, 1].hist(diff.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\naxes[1, 1].axvline(0, color='red', linestyle='--', alpha=0.8)\naxes[1, 1].set_xlabel('Estimation Error')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Distribution of Estimation Errors')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Dale's law verification visualization\n# Create a mask showing cell types\ndale_vis = jnp.zeros_like(J_estimated)\nfor i, is_exc in enumerate(dale_mask):\n    if is_exc:  # Excitatory\n        dale_vis = dale_vis.at[:, i].set(1)  # Mark excitatory columns\n    else:  # Inhibitory  \n        dale_vis = dale_vis.at[:, i].set(-1)  # Mark inhibitory columns\n\n# Color code: positive for excitatory columns, negative for inhibitory\nim4 = axes[1, 2].imshow(dale_vis * jnp.sign(J_estimated), cmap='bwr', \n                       vmin=-1, vmax=1, aspect='auto')\naxes[1, 2].set_title('Dale\\'s Law Compliance\\n(Red=Violations, Blue=Compliant)')\naxes[1, 2].set_xlabel('Source Neuron (Col)')\naxes[1, 2].set_ylabel('Target Neuron (Row)')\n\n# Add vertical lines to separate cell types\nexc_count = jnp.sum(dale_mask)\naxes[1, 2].axvline(exc_count - 0.5, color='black', linewidth=2, alpha=0.8)\n\nplt.colorbar(im4, ax=axes[1, 2])\n\nplt.suptitle('J Matrix Estimation Diagnostics', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2705 J Matrix estimation diagnostics visualization completed!\")\n</pre> # Visualize J matrices comparison fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # Determine color scale for consistent comparison vmin = min(jnp.min(J_true), jnp.min(J_estimated)) vmax = max(jnp.max(J_true), jnp.max(J_estimated))  # Plot ground truth J im1 = axes[0, 0].imshow(J_true, cmap='bwr',  vmin=vmin, vmax=vmax, aspect='auto') axes[0, 0].set_title('Ground Truth J Matrix') axes[0, 0].set_xlabel('Source Neuron') axes[0, 0].set_ylabel('Target Neuron') plt.colorbar(im1, ax=axes[0, 0])  # Plot estimated J im2 = axes[0, 1].imshow(J_estimated, cmap='bwr', vmin=vmin, vmax=vmax, aspect='auto') axes[0, 1].set_title('Estimated J Matrix') axes[0, 1].set_xlabel('Source Neuron') axes[0, 1].set_ylabel('Target Neuron') plt.colorbar(im2, ax=axes[0, 1])  # Plot difference (residual) diff = J_estimated - J_true im3 = axes[0, 2].imshow(diff, cmap='bwr', vmin=-jnp.max(jnp.abs(diff)),                         vmax=jnp.max(jnp.abs(diff)), aspect='auto') axes[0, 2].set_title('Difference (Estimated - True)') axes[0, 2].set_xlabel('Source Neuron') axes[0, 2].set_ylabel('Target Neuron') plt.colorbar(im3, ax=axes[0, 2])  # Scatter plot: estimated vs true axes[1, 0].scatter(J_true.flatten(), J_estimated.flatten(), alpha=0.6, s=20) axes[1, 0].plot([vmin, vmax], [vmin, vmax], 'r--', alpha=0.8, label='Perfect agreement') axes[1, 0].set_xlabel('True J values') axes[1, 0].set_ylabel('Estimated J values') axes[1, 0].set_title(f'J Values Comparison\\n(r = {j_metrics[\"correlation\"]:.3f})') axes[1, 0].legend() axes[1, 0].grid(True, alpha=0.3)  # Histogram of differences axes[1, 1].hist(diff.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black') axes[1, 1].axvline(0, color='red', linestyle='--', alpha=0.8) axes[1, 1].set_xlabel('Estimation Error') axes[1, 1].set_ylabel('Frequency') axes[1, 1].set_title('Distribution of Estimation Errors') axes[1, 1].grid(True, alpha=0.3)  # Dale's law verification visualization # Create a mask showing cell types dale_vis = jnp.zeros_like(J_estimated) for i, is_exc in enumerate(dale_mask):     if is_exc:  # Excitatory         dale_vis = dale_vis.at[:, i].set(1)  # Mark excitatory columns     else:  # Inhibitory           dale_vis = dale_vis.at[:, i].set(-1)  # Mark inhibitory columns  # Color code: positive for excitatory columns, negative for inhibitory im4 = axes[1, 2].imshow(dale_vis * jnp.sign(J_estimated), cmap='bwr',                         vmin=-1, vmax=1, aspect='auto') axes[1, 2].set_title('Dale\\'s Law Compliance\\n(Red=Violations, Blue=Compliant)') axes[1, 2].set_xlabel('Source Neuron (Col)') axes[1, 2].set_ylabel('Target Neuron (Row)')  # Add vertical lines to separate cell types exc_count = jnp.sum(dale_mask) axes[1, 2].axvline(exc_count - 0.5, color='black', linewidth=2, alpha=0.8)  plt.colorbar(im4, ax=axes[1, 2])  plt.suptitle('J Matrix Estimation Diagnostics', fontsize=16, fontweight='bold') plt.tight_layout() plt.show()  print(\"\u2705 J Matrix estimation diagnostics visualization completed!\") <pre>\u2705 J Matrix estimation diagnostics visualization completed!\n</pre> In\u00a0[6]: Copied! <pre>print(\"\ud83e\udde9 BLOCKWISE NMF FACTORIZATION ANALYSIS\")\nprint(\"=\" * 55)\n\n# Perform blockwise NMF on estimated J matrix\nstart_time = time.time()\nblock_factors = blockwise_NMF(J_estimated, cell_constraints)\nnmf_time = time.time() - start_time\n\nprint(f\"\u2705 Blockwise NMF Completed:\")\nprint(f\"  \u2022 NMF time: {nmf_time:.3f} seconds\")\nprint(f\"  \u2022 Number of cell types: {len(block_factors)}\")\n\n# Analyze each cell type block\ndef analyze_nmf_block(U, V, J_block, cell_type_idx, cell_type_label):\n    \"\"\"Analyze NMF factorization quality for a single cell type block.\"\"\"\n    \n    # 1. Compute reconstruction\n    J_reconstructed = U @ V.T\n    \n    # 2. Reconstruction error\n    reconstruction_error = jnp.linalg.norm(J_block - J_reconstructed, 'fro')\n    relative_error = reconstruction_error / jnp.linalg.norm(J_block, 'fro')\n    \n    # 3. Verify non-negativity\n    U_negative_count = jnp.sum(U &lt; -1e-10)\n    V_negative_count = jnp.sum(V &lt; -1e-10)\n    \n    # 4. Check factor variance (avoid degenerate cases)\n    U_row_variances = jnp.var(U, axis=1)\n    V_row_variances = jnp.var(V, axis=1)\n    \n    # Degenerate factors: all zeros or all ones\n    U_zero_rows = jnp.sum(jnp.all(jnp.abs(U) &lt; 1e-10, axis=1))\n    V_zero_rows = jnp.sum(jnp.all(jnp.abs(V) &lt; 1e-10, axis=1))\n    \n    # Near-constant rows (low variance)\n    U_low_var_rows = jnp.sum(U_row_variances &lt; 1e-6)\n    V_low_var_rows = jnp.sum(V_row_variances &lt; 1e-6)\n    \n    # 5. Frobenius norms\n    U_norm = jnp.linalg.norm(U, 'fro')\n    V_norm = jnp.linalg.norm(V, 'fro')\n    \n    # 6. Explained variance\n    total_variance = jnp.var(J_block)\n    residual_variance = jnp.var(J_block - J_reconstructed)\n    explained_variance = 1 - (residual_variance / total_variance)\n    \n    return {\n        'cell_type_idx': cell_type_idx,\n        'cell_type_label': cell_type_label,\n        'J_block_shape': J_block.shape,\n        'U_shape': U.shape,\n        'V_shape': V.shape,\n        'reconstruction_error': reconstruction_error,\n        'relative_error': relative_error,\n        'U_negative_count': U_negative_count,\n        'V_negative_count': V_negative_count,\n        'U_zero_rows': U_zero_rows,\n        'V_zero_rows': V_zero_rows,\n        'U_low_var_rows': U_low_var_rows,\n        'V_low_var_rows': V_low_var_rows,\n        'U_norm': U_norm,\n        'V_norm': V_norm,\n        'explained_variance': explained_variance,\n        'J_reconstructed': J_reconstructed\n    }\n\n# Analyze all cell type blocks\nJ_abs = jnp.abs(J_estimated)\nblock_analyses = []\n\nfor i, (U, V) in enumerate(block_factors):\n    cell_type = cell_types[i]\n    \n    # Get indices for this cell type\n    type_indices = jnp.where(cell_type_mask == cell_type)[0]\n    \n    # Extract the corresponding block from |J|\n    J_block = J_abs[type_indices, :]\n    \n    # Analyze this block\n    analysis = analyze_nmf_block(U, V, J_block, i, cell_type)\n    block_analyses.append(analysis)\n    \n    # Print results\n    print(f\"\\n\ud83d\udcca Cell Type {cell_type} (Block {i+1}):\")\n    print(f\"  \u2022 Block shape: {analysis['J_block_shape']}\")\n    print(f\"  \u2022 U shape: {analysis['U_shape']}, V shape: {analysis['V_shape']}\")\n    print(f\"  \u2022 Relative reconstruction error: {analysis['relative_error']:.6f}\")\n    print(f\"  \u2022 Explained variance: {analysis['explained_variance']:.4f}\")\n    print(f\"  \u2022 Non-negativity violations: U={analysis['U_negative_count']}, V={analysis['V_negative_count']}\")\n    print(f\"  \u2022 Degenerate factors: U zeros={analysis['U_zero_rows']}, V zeros={analysis['V_zero_rows']}\")\n    print(f\"  \u2022 Low variance factors: U={analysis['U_low_var_rows']}, V={analysis['V_low_var_rows']}\")\n    print(f\"  \u2022 Factor norms: ||U||_F={analysis['U_norm']:.3f}, ||V||_F={analysis['V_norm']:.3f}\")\n</pre> print(\"\ud83e\udde9 BLOCKWISE NMF FACTORIZATION ANALYSIS\") print(\"=\" * 55)  # Perform blockwise NMF on estimated J matrix start_time = time.time() block_factors = blockwise_NMF(J_estimated, cell_constraints) nmf_time = time.time() - start_time  print(f\"\u2705 Blockwise NMF Completed:\") print(f\"  \u2022 NMF time: {nmf_time:.3f} seconds\") print(f\"  \u2022 Number of cell types: {len(block_factors)}\")  # Analyze each cell type block def analyze_nmf_block(U, V, J_block, cell_type_idx, cell_type_label):     \"\"\"Analyze NMF factorization quality for a single cell type block.\"\"\"          # 1. Compute reconstruction     J_reconstructed = U @ V.T          # 2. Reconstruction error     reconstruction_error = jnp.linalg.norm(J_block - J_reconstructed, 'fro')     relative_error = reconstruction_error / jnp.linalg.norm(J_block, 'fro')          # 3. Verify non-negativity     U_negative_count = jnp.sum(U &lt; -1e-10)     V_negative_count = jnp.sum(V &lt; -1e-10)          # 4. Check factor variance (avoid degenerate cases)     U_row_variances = jnp.var(U, axis=1)     V_row_variances = jnp.var(V, axis=1)          # Degenerate factors: all zeros or all ones     U_zero_rows = jnp.sum(jnp.all(jnp.abs(U) &lt; 1e-10, axis=1))     V_zero_rows = jnp.sum(jnp.all(jnp.abs(V) &lt; 1e-10, axis=1))          # Near-constant rows (low variance)     U_low_var_rows = jnp.sum(U_row_variances &lt; 1e-6)     V_low_var_rows = jnp.sum(V_row_variances &lt; 1e-6)          # 5. Frobenius norms     U_norm = jnp.linalg.norm(U, 'fro')     V_norm = jnp.linalg.norm(V, 'fro')          # 6. Explained variance     total_variance = jnp.var(J_block)     residual_variance = jnp.var(J_block - J_reconstructed)     explained_variance = 1 - (residual_variance / total_variance)          return {         'cell_type_idx': cell_type_idx,         'cell_type_label': cell_type_label,         'J_block_shape': J_block.shape,         'U_shape': U.shape,         'V_shape': V.shape,         'reconstruction_error': reconstruction_error,         'relative_error': relative_error,         'U_negative_count': U_negative_count,         'V_negative_count': V_negative_count,         'U_zero_rows': U_zero_rows,         'V_zero_rows': V_zero_rows,         'U_low_var_rows': U_low_var_rows,         'V_low_var_rows': V_low_var_rows,         'U_norm': U_norm,         'V_norm': V_norm,         'explained_variance': explained_variance,         'J_reconstructed': J_reconstructed     }  # Analyze all cell type blocks J_abs = jnp.abs(J_estimated) block_analyses = []  for i, (U, V) in enumerate(block_factors):     cell_type = cell_types[i]          # Get indices for this cell type     type_indices = jnp.where(cell_type_mask == cell_type)[0]          # Extract the corresponding block from |J|     J_block = J_abs[type_indices, :]          # Analyze this block     analysis = analyze_nmf_block(U, V, J_block, i, cell_type)     block_analyses.append(analysis)          # Print results     print(f\"\\n\ud83d\udcca Cell Type {cell_type} (Block {i+1}):\")     print(f\"  \u2022 Block shape: {analysis['J_block_shape']}\")     print(f\"  \u2022 U shape: {analysis['U_shape']}, V shape: {analysis['V_shape']}\")     print(f\"  \u2022 Relative reconstruction error: {analysis['relative_error']:.6f}\")     print(f\"  \u2022 Explained variance: {analysis['explained_variance']:.4f}\")     print(f\"  \u2022 Non-negativity violations: U={analysis['U_negative_count']}, V={analysis['V_negative_count']}\")     print(f\"  \u2022 Degenerate factors: U zeros={analysis['U_zero_rows']}, V zeros={analysis['V_zero_rows']}\")     print(f\"  \u2022 Low variance factors: U={analysis['U_low_var_rows']}, V={analysis['V_low_var_rows']}\")     print(f\"  \u2022 Factor norms: ||U||_F={analysis['U_norm']:.3f}, ||V||_F={analysis['V_norm']:.3f}\") <pre>\ud83e\udde9 BLOCKWISE NMF FACTORIZATION ANALYSIS\n=======================================================\n\u2705 Blockwise NMF Completed:\n  \u2022 NMF time: 3.910 seconds\n  \u2022 Number of cell types: 2\n\n\ud83d\udcca Cell Type 0 (Block 1):\n  \u2022 Block shape: (10, 20)\n  \u2022 U shape: (10, 3), V shape: (20, 3)\n  \u2022 Relative reconstruction error: 0.307553\n  \u2022 Explained variance: 0.8236\n  \u2022 Non-negativity violations: U=1, V=6\n  \u2022 Degenerate factors: U zeros=0, V zeros=0\n  \u2022 Low variance factors: U=0, V=1\n  \u2022 Factor norms: ||U||_F=0.405, ||V||_F=7.602\n\n\ud83d\udcca Cell Type 1 (Block 2):\n  \u2022 Block shape: (10, 20)\n  \u2022 U shape: (10, 3), V shape: (20, 3)\n  \u2022 Relative reconstruction error: 0.454440\n  \u2022 Explained variance: 0.6990\n  \u2022 Non-negativity violations: U=3, V=9\n  \u2022 Degenerate factors: U zeros=0, V zeros=0\n  \u2022 Low variance factors: U=0, V=0\n  \u2022 Factor norms: ||U||_F=0.209, ||V||_F=7.552\n</pre> In\u00a0[9]: Copied! <pre># Visualize NMF factorization results\nn_cell_types = len(block_factors)\nfig, axes = plt.subplots(n_cell_types, 4, figsize=(20, 6 * n_cell_types))\n\nif n_cell_types == 1:\n    axes = axes.reshape(1, -1)  # Ensure 2D array for indexing\n\nfor i, analysis in enumerate(block_analyses):\n    U, V = block_factors[i]\n    J_block = J_abs[jnp.where(cell_type_mask == cell_types[i])[0], :]\n    J_reconstructed = analysis['J_reconstructed']\n    \n    # Determine color scale\n    vmax = max(jnp.max(J_block), jnp.max(J_reconstructed))\n    \n    # Plot 1: Original block\n    im1 = axes[i, 0].imshow(J_block, cmap='bwr', vmin=0, vmax=vmax, aspect='auto')\n    axes[i, 0].set_title(f'Cell Type {cell_types[i]}\\nOriginal |J| Block')\n    axes[i, 0].set_xlabel('All Neurons')\n    axes[i, 0].set_ylabel(f'Type {cell_types[i]} Neurons')\n    plt.colorbar(im1, ax=axes[i, 0])\n    \n    # Plot 2: Reconstructed block\n    im2 = axes[i, 1].imshow(J_reconstructed, cmap='bwr', vmin=0, vmax=vmax, aspect='auto')\n    axes[i, 1].set_title(f'Reconstructed Block\\n(U @ V.T)')\n    axes[i, 1].set_xlabel('All Neurons')\n    axes[i, 1].set_ylabel(f'Type {cell_types[i]} Neurons')\n    plt.colorbar(im2, ax=axes[i, 1])\n    \n    # Plot 3: Factor U\n    im3 = axes[i, 2].imshow(U, cmap='bwr', vmin=0, aspect='auto')\n    axes[i, 2].set_title(f'Factor U\\n({U.shape[0]} \u00d7 {U.shape[1]})')\n    axes[i, 2].set_xlabel('Latent Dimensions')\n    axes[i, 2].set_ylabel(f'Type {cell_types[i]} Neurons')\n    plt.colorbar(im3, ax=axes[i, 2])\n    \n    # Plot 4: Factor V (transposed for visualization)\n    im4 = axes[i, 3].imshow(V.T, cmap='bwr', vmin=0, aspect='auto')\n    axes[i, 3].set_title(f'Factor V.T\\n({V.shape[1]} \u00d7 {V.shape[0]})')\n    axes[i, 3].set_xlabel('All Neurons')\n    axes[i, 3].set_ylabel('Latent Dimensions')\n    plt.colorbar(im4, ax=axes[i, 3])\n\nplt.suptitle('Blockwise NMF Factorization Results', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2705 Blockwise NMF visualization completed!\")\n</pre> # Visualize NMF factorization results n_cell_types = len(block_factors) fig, axes = plt.subplots(n_cell_types, 4, figsize=(20, 6 * n_cell_types))  if n_cell_types == 1:     axes = axes.reshape(1, -1)  # Ensure 2D array for indexing  for i, analysis in enumerate(block_analyses):     U, V = block_factors[i]     J_block = J_abs[jnp.where(cell_type_mask == cell_types[i])[0], :]     J_reconstructed = analysis['J_reconstructed']          # Determine color scale     vmax = max(jnp.max(J_block), jnp.max(J_reconstructed))          # Plot 1: Original block     im1 = axes[i, 0].imshow(J_block, cmap='bwr', vmin=0, vmax=vmax, aspect='auto')     axes[i, 0].set_title(f'Cell Type {cell_types[i]}\\nOriginal |J| Block')     axes[i, 0].set_xlabel('All Neurons')     axes[i, 0].set_ylabel(f'Type {cell_types[i]} Neurons')     plt.colorbar(im1, ax=axes[i, 0])          # Plot 2: Reconstructed block     im2 = axes[i, 1].imshow(J_reconstructed, cmap='bwr', vmin=0, vmax=vmax, aspect='auto')     axes[i, 1].set_title(f'Reconstructed Block\\n(U @ V.T)')     axes[i, 1].set_xlabel('All Neurons')     axes[i, 1].set_ylabel(f'Type {cell_types[i]} Neurons')     plt.colorbar(im2, ax=axes[i, 1])          # Plot 3: Factor U     im3 = axes[i, 2].imshow(U, cmap='bwr', vmin=0, aspect='auto')     axes[i, 2].set_title(f'Factor U\\n({U.shape[0]} \u00d7 {U.shape[1]})')     axes[i, 2].set_xlabel('Latent Dimensions')     axes[i, 2].set_ylabel(f'Type {cell_types[i]} Neurons')     plt.colorbar(im3, ax=axes[i, 2])          # Plot 4: Factor V (transposed for visualization)     im4 = axes[i, 3].imshow(V.T, cmap='bwr', vmin=0, aspect='auto')     axes[i, 3].set_title(f'Factor V.T\\n({V.shape[1]} \u00d7 {V.shape[0]})')     axes[i, 3].set_xlabel('All Neurons')     axes[i, 3].set_ylabel('Latent Dimensions')     plt.colorbar(im4, ax=axes[i, 3])  plt.suptitle('Blockwise NMF Factorization Results', fontsize=16, fontweight='bold') plt.tight_layout() plt.show()  print(\"\u2705 Blockwise NMF visualization completed!\") <pre>\u2705 Blockwise NMF visualization completed!\n</pre> In\u00a0[8]: Copied! <pre># Summary metrics and comparison across cell types\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Extract metrics for comparison\ncell_type_labels = [f\"Type {analysis['cell_type_label']}\" for analysis in block_analyses]\nrelative_errors = [analysis['relative_error'] for analysis in block_analyses]\nexplained_variances = [analysis['explained_variance'] for analysis in block_analyses]\nU_norms = [analysis['U_norm'] for analysis in block_analyses]\nV_norms = [analysis['V_norm'] for analysis in block_analyses]\ntotal_violations = [analysis['U_negative_count'] + analysis['V_negative_count'] for analysis in block_analyses]\ndegenerate_factors = [analysis['U_zero_rows'] + analysis['V_zero_rows'] for analysis in block_analyses]\n\n# Plot 1: Reconstruction errors\nbars1 = axes[0, 0].bar(cell_type_labels, relative_errors, alpha=0.7, color='skyblue')\naxes[0, 0].set_ylabel('Relative Reconstruction Error')\naxes[0, 0].set_title('NMF Reconstruction Quality')\naxes[0, 0].grid(True, alpha=0.3)\n# Add value labels\nfor bar, value in zip(bars1, relative_errors):\n    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n                   f'{value:.4f}', ha='center', va='bottom')\n\n# Plot 2: Explained variance\nbars2 = axes[0, 1].bar(cell_type_labels, explained_variances, alpha=0.7, color='lightgreen')\naxes[0, 1].set_ylabel('Explained Variance')\naxes[0, 1].set_title('Variance Explained by NMF')\naxes[0, 1].set_ylim(0, 1)\naxes[0, 1].grid(True, alpha=0.3)\nfor bar, value in zip(bars2, explained_variances):\n    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n                   f'{value:.3f}', ha='center', va='bottom')\n\n# Plot 3: Factor norms\nx = jnp.arange(len(cell_type_labels))\nwidth = 0.35\naxes[0, 2].bar(x - width/2, U_norms, width, label='||U||_F', alpha=0.7, color='orange')\naxes[0, 2].bar(x + width/2, V_norms, width, label='||V||_F', alpha=0.7, color='purple')\naxes[0, 2].set_ylabel('Frobenius Norm')\naxes[0, 2].set_title('Factor Matrix Norms')\naxes[0, 2].set_xticks(x)\naxes[0, 2].set_xticklabels(cell_type_labels)\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# Plot 4: Constraint violations\nbars4 = axes[1, 0].bar(cell_type_labels, total_violations, alpha=0.7, color='red')\naxes[1, 0].set_ylabel('Non-negativity Violations')\naxes[1, 0].set_title('Constraint Violations (U, V &lt; 0)')\naxes[1, 0].grid(True, alpha=0.3)\nfor bar, value in zip(bars4, total_violations):\n    if value &gt; 0:\n        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n                       f'{int(value)}', ha='center', va='bottom')\n\n# Plot 5: Degenerate factors\nbars5 = axes[1, 1].bar(cell_type_labels, degenerate_factors, alpha=0.7, color='gold')\naxes[1, 1].set_ylabel('Zero Rows Count')\naxes[1, 1].set_title('Degenerate Factors (All-Zero Rows)')\naxes[1, 1].grid(True, alpha=0.3)\nfor bar, value in zip(bars5, degenerate_factors):\n    if value &gt; 0:\n        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n                       f'{int(value)}', ha='center', va='bottom')\n\n# Plot 6: Block dimensions summary\nblock_sizes = [analysis['J_block_shape'][0] for analysis in block_analyses]\nlatent_dims = [analysis['U_shape'][1] for analysis in block_analyses]\n\naxes[1, 2].bar(x - width/2, block_sizes, width, label='Block Size', alpha=0.7, color='teal')\naxes[1, 2].bar(x + width/2, latent_dims, width, label='Latent Dims', alpha=0.7, color='coral')\naxes[1, 2].set_ylabel('Count')\naxes[1, 2].set_title('Block Dimensions')\naxes[1, 2].set_xticks(x)\naxes[1, 2].set_xticklabels(cell_type_labels)\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.suptitle('Blockwise NMF Summary Metrics', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Print overall summary\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\ud83d\udccb CTDS INITIALIZATION DIAGNOSTICS SUMMARY\")\nprint(\"=\" * 60)\n\nprint(f\"\\n\ud83c\udfaf J Matrix Estimation:\")\nprint(f\"  \u2022 Correlation with ground truth: {j_metrics['correlation']:.4f}\")\nprint(f\"  \u2022 Relative Frobenius error: {j_metrics['frobenius_error']:.4f}\")\nprint(f\"  \u2022 Dale's law compliance: {compliance_rate:.1f}%\")\nprint(f\"  \u2022 Prediction R\u00b2: {j_metrics['r2_prediction']:.4f}\")\n\nprint(f\"\\n\ud83e\udde9 Blockwise NMF Factorization:\")\navg_rel_error = jnp.mean(jnp.array(relative_errors))\navg_explained_var = jnp.mean(jnp.array(explained_variances))\ntotal_violations_all = sum(total_violations)\ntotal_degenerate = sum(degenerate_factors)\n\nprint(f\"  \u2022 Average reconstruction error: {avg_rel_error:.6f}\")\nprint(f\"  \u2022 Average explained variance: {avg_explained_var:.4f}\")\nprint(f\"  \u2022 Total non-negativity violations: {total_violations_all}\")\nprint(f\"  \u2022 Total degenerate factors: {total_degenerate}\")\n\nprint(f\"\\n\u23f1\ufe0f Computational Performance:\")\nprint(f\"  \u2022 J estimation time: {estimation_time:.3f}s\")\nprint(f\"  \u2022 NMF factorization time: {nmf_time:.3f}s\")\nprint(f\"  \u2022 Total initialization time: {estimation_time + nmf_time:.3f}s\")\n\nprint(\"\\n\u2705 Diagnostics completed successfully!\")\n</pre> # Summary metrics and comparison across cell types fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # Extract metrics for comparison cell_type_labels = [f\"Type {analysis['cell_type_label']}\" for analysis in block_analyses] relative_errors = [analysis['relative_error'] for analysis in block_analyses] explained_variances = [analysis['explained_variance'] for analysis in block_analyses] U_norms = [analysis['U_norm'] for analysis in block_analyses] V_norms = [analysis['V_norm'] for analysis in block_analyses] total_violations = [analysis['U_negative_count'] + analysis['V_negative_count'] for analysis in block_analyses] degenerate_factors = [analysis['U_zero_rows'] + analysis['V_zero_rows'] for analysis in block_analyses]  # Plot 1: Reconstruction errors bars1 = axes[0, 0].bar(cell_type_labels, relative_errors, alpha=0.7, color='skyblue') axes[0, 0].set_ylabel('Relative Reconstruction Error') axes[0, 0].set_title('NMF Reconstruction Quality') axes[0, 0].grid(True, alpha=0.3) # Add value labels for bar, value in zip(bars1, relative_errors):     axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,                    f'{value:.4f}', ha='center', va='bottom')  # Plot 2: Explained variance bars2 = axes[0, 1].bar(cell_type_labels, explained_variances, alpha=0.7, color='lightgreen') axes[0, 1].set_ylabel('Explained Variance') axes[0, 1].set_title('Variance Explained by NMF') axes[0, 1].set_ylim(0, 1) axes[0, 1].grid(True, alpha=0.3) for bar, value in zip(bars2, explained_variances):     axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,                    f'{value:.3f}', ha='center', va='bottom')  # Plot 3: Factor norms x = jnp.arange(len(cell_type_labels)) width = 0.35 axes[0, 2].bar(x - width/2, U_norms, width, label='||U||_F', alpha=0.7, color='orange') axes[0, 2].bar(x + width/2, V_norms, width, label='||V||_F', alpha=0.7, color='purple') axes[0, 2].set_ylabel('Frobenius Norm') axes[0, 2].set_title('Factor Matrix Norms') axes[0, 2].set_xticks(x) axes[0, 2].set_xticklabels(cell_type_labels) axes[0, 2].legend() axes[0, 2].grid(True, alpha=0.3)  # Plot 4: Constraint violations bars4 = axes[1, 0].bar(cell_type_labels, total_violations, alpha=0.7, color='red') axes[1, 0].set_ylabel('Non-negativity Violations') axes[1, 0].set_title('Constraint Violations (U, V &lt; 0)') axes[1, 0].grid(True, alpha=0.3) for bar, value in zip(bars4, total_violations):     if value &gt; 0:         axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,                        f'{int(value)}', ha='center', va='bottom')  # Plot 5: Degenerate factors bars5 = axes[1, 1].bar(cell_type_labels, degenerate_factors, alpha=0.7, color='gold') axes[1, 1].set_ylabel('Zero Rows Count') axes[1, 1].set_title('Degenerate Factors (All-Zero Rows)') axes[1, 1].grid(True, alpha=0.3) for bar, value in zip(bars5, degenerate_factors):     if value &gt; 0:         axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,                        f'{int(value)}', ha='center', va='bottom')  # Plot 6: Block dimensions summary block_sizes = [analysis['J_block_shape'][0] for analysis in block_analyses] latent_dims = [analysis['U_shape'][1] for analysis in block_analyses]  axes[1, 2].bar(x - width/2, block_sizes, width, label='Block Size', alpha=0.7, color='teal') axes[1, 2].bar(x + width/2, latent_dims, width, label='Latent Dims', alpha=0.7, color='coral') axes[1, 2].set_ylabel('Count') axes[1, 2].set_title('Block Dimensions') axes[1, 2].set_xticks(x) axes[1, 2].set_xticklabels(cell_type_labels) axes[1, 2].legend() axes[1, 2].grid(True, alpha=0.3)  plt.suptitle('Blockwise NMF Summary Metrics', fontsize=16, fontweight='bold') plt.tight_layout() plt.show()  # Print overall summary print(\"\\n\" + \"=\" * 60) print(\"\ud83d\udccb CTDS INITIALIZATION DIAGNOSTICS SUMMARY\") print(\"=\" * 60)  print(f\"\\n\ud83c\udfaf J Matrix Estimation:\") print(f\"  \u2022 Correlation with ground truth: {j_metrics['correlation']:.4f}\") print(f\"  \u2022 Relative Frobenius error: {j_metrics['frobenius_error']:.4f}\") print(f\"  \u2022 Dale's law compliance: {compliance_rate:.1f}%\") print(f\"  \u2022 Prediction R\u00b2: {j_metrics['r2_prediction']:.4f}\")  print(f\"\\n\ud83e\udde9 Blockwise NMF Factorization:\") avg_rel_error = jnp.mean(jnp.array(relative_errors)) avg_explained_var = jnp.mean(jnp.array(explained_variances)) total_violations_all = sum(total_violations) total_degenerate = sum(degenerate_factors)  print(f\"  \u2022 Average reconstruction error: {avg_rel_error:.6f}\") print(f\"  \u2022 Average explained variance: {avg_explained_var:.4f}\") print(f\"  \u2022 Total non-negativity violations: {total_violations_all}\") print(f\"  \u2022 Total degenerate factors: {total_degenerate}\")  print(f\"\\n\u23f1\ufe0f Computational Performance:\") print(f\"  \u2022 J estimation time: {estimation_time:.3f}s\") print(f\"  \u2022 NMF factorization time: {nmf_time:.3f}s\") print(f\"  \u2022 Total initialization time: {estimation_time + nmf_time:.3f}s\")  print(\"\\n\u2705 Diagnostics completed successfully!\") <pre>\n============================================================\n\ud83d\udccb CTDS INITIALIZATION DIAGNOSTICS SUMMARY\n============================================================\n\n\ud83c\udfaf J Matrix Estimation:\n  \u2022 Correlation with ground truth: 0.1911\n  \u2022 Relative Frobenius error: 1.1704\n  \u2022 Dale's law compliance: 85.0%\n  \u2022 Prediction R\u00b2: 0.4312\n\n\ud83e\udde9 Blockwise NMF Factorization:\n  \u2022 Average reconstruction error: 0.380997\n  \u2022 Average explained variance: 0.7613\n  \u2022 Total non-negativity violations: 19\n  \u2022 Total degenerate factors: 0\n\n\u23f1\ufe0f Computational Performance:\n  \u2022 J estimation time: 2.669s\n  \u2022 NMF factorization time: 3.910s\n  \u2022 Total initialization time: 6.579s\n\n\u2705 Diagnostics completed successfully!\n</pre>"},{"location":"examples/ctds_initialization_diagnostics/#ctds-initialization-diagnostics","title":"CTDS Initialization Diagnostics\u00b6","text":"<p>This notebook provides comprehensive diagnostics for CTDS (Cell-Type Constrained Dynamical Systems) initialization procedures, focusing on:</p> <ol> <li>J Matrix Estimation Diagnostics - Validation of Dale's law constrained connectivity matrix estimation</li> <li>Blockwise NMF Factorization Diagnostics - Analysis of non-negative matrix factorization for cell-type specific dynamics</li> </ol>"},{"location":"examples/ctds_initialization_diagnostics/#overview","title":"Overview\u00b6","text":"<p>The CTDS initialization process involves:</p> <ul> <li>Estimating the Dale matrix J from neural activity using constrained optimization</li> <li>Performing blockwise NMF to extract cell-type specific latent factors</li> <li>Converting these factors into state-space model parameters</li> </ul> <p>This notebook validates each step with quantitative metrics and visualizations.</p>"},{"location":"examples/ctds_initialization_diagnostics/#generate-synthetic-data-with-ground-truth","title":"Generate Synthetic Data with Ground Truth\u00b6","text":"<p>First, we'll generate synthetic neural data with known ground truth connectivity and cell types to validate our initialization procedures.</p>"},{"location":"examples/ctds_initialization_diagnostics/#1-j-matrix-estimation-diagnostics","title":"1. J Matrix Estimation Diagnostics\u00b6","text":"<p>We estimate the Dale's law constrained connectivity matrix J from neural activity and validate it against ground truth.</p>"},{"location":"examples/ctds_initialization_diagnostics/#2-blockwise-nmf-factorization-diagnostics","title":"2. Blockwise NMF Factorization Diagnostics\u00b6","text":"<p>We perform blockwise Non-negative Matrix Factorization (NMF) on the absolute Dale matrix |J| and validate the factorization quality for each cell type.</p>"},{"location":"examples/ctds_initialization_diagnostics/","title":"Examples","text":"<p>For detailed examples and tutorials, see the Jupyter notebooks:</p>"},{"location":"examples/ctds_initialization_diagnostics/#ctds-initialization-diagnostics","title":"CTDS Initialization Diagnostics","text":"<p>View Notebook</p> <p>This notebook demonstrates how to properly initialize CTDS models and diagnose potential issues.</p>"},{"location":"examples/ctds_initialization_diagnostics/#ctds-model-validation","title":"CTDS Model Validation","text":"<p>View Notebook</p> <p>This notebook shows how to validate fitted CTDS models and assess model performance.</p>"},{"location":"examples/ctds_model_validation/","title":"CTDS Single-Region","text":"In\u00a0[1]: Copied! <pre># Setup and Imports\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import seaborn as sns\nfrom typing import Tuple, Optional\nimport time\nfrom functools import partial\nimport seaborn as sns\n# Configure JAX for float64 precision\njax.config.update(\"jax_enable_x64\", True)\n\n# Import CTDS modules\nfrom models import CTDS\nfrom params import (\n    ParamsCTDS, ParamsCTDSInitial, ParamsCTDSDynamics, \n    ParamsCTDSEmissions, ParamsCTDSConstraints, SufficientStats\n)\nfrom inference import DynamaxLGSSMBackend\nfrom simulation_utilis import generate_synthetic_data, generate_CTDS_Params, align_single_region_ctds\n# Set random seeds for reproducibility\nnp.random.seed(42)\nkey = jr.PRNGKey(42)\n\n# Configure plotting\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\nprint(\"\u2705 Setup complete!\")\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"JAX devices: {jax.devices()}\")\nprint(f\"JAX backend: {jax.default_backend()}\")\n</pre> # Setup and Imports import jax import jax.numpy as jnp import jax.random as jr import numpy as np import matplotlib.pyplot as plt #import seaborn as sns from typing import Tuple, Optional import time from functools import partial import seaborn as sns # Configure JAX for float64 precision jax.config.update(\"jax_enable_x64\", True)  # Import CTDS modules from models import CTDS from params import (     ParamsCTDS, ParamsCTDSInitial, ParamsCTDSDynamics,      ParamsCTDSEmissions, ParamsCTDSConstraints, SufficientStats ) from inference import DynamaxLGSSMBackend from simulation_utilis import generate_synthetic_data, generate_CTDS_Params, align_single_region_ctds # Set random seeds for reproducibility np.random.seed(42) key = jr.PRNGKey(42)  # Configure plotting plt.style.use('default') sns.set_palette(\"husl\") plt.rcParams['figure.figsize'] = (12, 8) plt.rcParams['font.size'] = 12  print(\"\u2705 Setup complete!\") print(f\"JAX version: {jax.__version__}\") print(f\"JAX devices: {jax.devices()}\") print(f\"JAX backend: {jax.default_backend()}\") <pre>\u2705 Setup complete!\nJAX version: 0.4.38\nJAX devices: [CpuDevice(id=0)]\nJAX backend: cpu\n</pre> In\u00a0[2]: Copied! <pre># Define dimensions and structure\nD = 4  # Total state dimension\nN = 20  # Number of observed neurons\nT = 200  # Number of time steps\nK = 2  # Number of cell types\nkey = jr.PRNGKey(0)  # Random key for reproducibility\n# Step 1: Generate Synthetic Data for Demonstration\nprint(\" STEP 1: GENERATING SYNTHETIC NEURAL DATA\")\nprint(\"=\" * 60)\nstates, observations, ctds, ctds_params = generate_synthetic_data(\n    num_samples=1,\n    num_timesteps=T,\n    state_dim=D,\n    emission_dim=N,\n    cell_types=K\n)\n#Defining True Params\nA_true = ctds_params.dynamics.weights\nC_true = ctds_params.emissions.weights\nQ_true = ctds_params.dynamics.cov\nR_true = ctds_params.emissions.cov\n\n\n#checking condition numbers\nprint(f\"Condition number of A_true: {jnp.linalg.cond(A_true)}\")\nprint(f\"Condition number of C_true: {jnp.linalg.cond(C_true)}\")\nprint(f\"Condition number of Q_true: {jnp.linalg.cond(Q_true)}\")\nprint(f\"Condition number of R_true: {jnp.linalg.cond(R_true)}\")\nprint(f\"Condition number of observations: {jnp.linalg.cond(observations)}\")\n\nprint(f\"Model structure:\")\nprint(f\"  State dimension (D): {D}\")\nprint(f\"  Observation dimension (N): {N}\")\nprint(f\"  Time steps (T): {T}\")\nprint(f\"  Cell types: {len(ctds.constraints.cell_types)}\")\nprint(f\"  Cell type mask: {ctds.constraints.cell_type_mask}\")\nprint(f\"  Cell type dimensions: {ctds.constraints.cell_type_dimensions}\")\nprint(f\"  Dynamics mask: {ctds_params.dynamics.dynamics_mask}\")\nprint(f\"\\n\ud83d\udcca Dataset Generated:\")\nprint(f\"  \u2022 A true shape: {A_true.shape}\")\nprint(f\"  \u2022 C true shape: {C_true.shape}\")\nprint(f\"  \u2022 Q true shape: {Q_true.shape}\")\nprint(f\"  \u2022 R true shape: {R_true.shape}\")\n#print(f\"  \u2022 A true: {A_true.__array__()}\")\n#print(f\"  \u2022 C true: {C_true.__array__()}\")\n#print(f\"  \u2022 Q true: {Q_true.__array__()}\")\n#print(f\"  \u2022 R true: {R_true.__array__()}\")\n\n# Step 2: Visualize the Synthetic Data\n%matplotlib inline\n\n# Visualize observations (neurons x time)\n\"\"\"\nplt.figure(figsize=(12, 4))\nsns.heatmap(np.array(observations), cmap='bwr', cbar=True)\nplt.title('Synthetic Observations (Neurons x Time)')\nplt.xlabel('Time')\nplt.ylabel('Neuron')\nplt.show()\n\"\"\"\n# Visualize all parameter matrices in subplots\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Visualize A_true (dynamics weights)\nsns.heatmap(np.array(A_true), cmap='bwr', center=0, cbar=True, ax=axes[0, 0])\naxes[0, 0].set_title('A_true: Dynamics Matrix')\naxes[0, 0].set_xlabel('Latent Dim')\naxes[0, 0].set_ylabel('Latent Dim')\n\n# Visualize C_true (emission weights)\nsns.heatmap(np.array(C_true), cmap='bwr', center=0, cbar=True, ax=axes[0, 1])\naxes[0, 1].set_title('C_true: Emission Matrix')\naxes[0, 1].set_xlabel('Latent Dim')\naxes[0, 1].set_ylabel('Neuron')\n\n# Visualize Q_true (dynamics covariance)\nsns.heatmap(np.array(Q_true), cmap='bwr', center=0, cbar=True, ax=axes[1, 0])\naxes[1, 0].set_title('Q_true: Dynamics Covariance')\naxes[1, 0].set_xlabel('Latent Dim')\naxes[1, 0].set_ylabel('Latent Dim')\n\n# Visualize R_true (emission covariance)\nsns.heatmap(np.array(R_true), cmap='bwr', center=0, cbar=True, ax=axes[1, 1])\naxes[1, 1].set_title('R_true: Emission Covariance')\naxes[1, 1].set_xlabel('Neuron')\naxes[1, 1].set_ylabel('Neuron')\n\nplt.tight_layout()\nplt.show()\n</pre> # Define dimensions and structure D = 4  # Total state dimension N = 20  # Number of observed neurons T = 200  # Number of time steps K = 2  # Number of cell types key = jr.PRNGKey(0)  # Random key for reproducibility # Step 1: Generate Synthetic Data for Demonstration print(\" STEP 1: GENERATING SYNTHETIC NEURAL DATA\") print(\"=\" * 60) states, observations, ctds, ctds_params = generate_synthetic_data(     num_samples=1,     num_timesteps=T,     state_dim=D,     emission_dim=N,     cell_types=K ) #Defining True Params A_true = ctds_params.dynamics.weights C_true = ctds_params.emissions.weights Q_true = ctds_params.dynamics.cov R_true = ctds_params.emissions.cov   #checking condition numbers print(f\"Condition number of A_true: {jnp.linalg.cond(A_true)}\") print(f\"Condition number of C_true: {jnp.linalg.cond(C_true)}\") print(f\"Condition number of Q_true: {jnp.linalg.cond(Q_true)}\") print(f\"Condition number of R_true: {jnp.linalg.cond(R_true)}\") print(f\"Condition number of observations: {jnp.linalg.cond(observations)}\")  print(f\"Model structure:\") print(f\"  State dimension (D): {D}\") print(f\"  Observation dimension (N): {N}\") print(f\"  Time steps (T): {T}\") print(f\"  Cell types: {len(ctds.constraints.cell_types)}\") print(f\"  Cell type mask: {ctds.constraints.cell_type_mask}\") print(f\"  Cell type dimensions: {ctds.constraints.cell_type_dimensions}\") print(f\"  Dynamics mask: {ctds_params.dynamics.dynamics_mask}\") print(f\"\\n\ud83d\udcca Dataset Generated:\") print(f\"  \u2022 A true shape: {A_true.shape}\") print(f\"  \u2022 C true shape: {C_true.shape}\") print(f\"  \u2022 Q true shape: {Q_true.shape}\") print(f\"  \u2022 R true shape: {R_true.shape}\") #print(f\"  \u2022 A true: {A_true.__array__()}\") #print(f\"  \u2022 C true: {C_true.__array__()}\") #print(f\"  \u2022 Q true: {Q_true.__array__()}\") #print(f\"  \u2022 R true: {R_true.__array__()}\")  # Step 2: Visualize the Synthetic Data %matplotlib inline  # Visualize observations (neurons x time) \"\"\" plt.figure(figsize=(12, 4)) sns.heatmap(np.array(observations), cmap='bwr', cbar=True) plt.title('Synthetic Observations (Neurons x Time)') plt.xlabel('Time') plt.ylabel('Neuron') plt.show() \"\"\" # Visualize all parameter matrices in subplots fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Visualize A_true (dynamics weights) sns.heatmap(np.array(A_true), cmap='bwr', center=0, cbar=True, ax=axes[0, 0]) axes[0, 0].set_title('A_true: Dynamics Matrix') axes[0, 0].set_xlabel('Latent Dim') axes[0, 0].set_ylabel('Latent Dim')  # Visualize C_true (emission weights) sns.heatmap(np.array(C_true), cmap='bwr', center=0, cbar=True, ax=axes[0, 1]) axes[0, 1].set_title('C_true: Emission Matrix') axes[0, 1].set_xlabel('Latent Dim') axes[0, 1].set_ylabel('Neuron')  # Visualize Q_true (dynamics covariance) sns.heatmap(np.array(Q_true), cmap='bwr', center=0, cbar=True, ax=axes[1, 0]) axes[1, 0].set_title('Q_true: Dynamics Covariance') axes[1, 0].set_xlabel('Latent Dim') axes[1, 0].set_ylabel('Latent Dim')  # Visualize R_true (emission covariance) sns.heatmap(np.array(R_true), cmap='bwr', center=0, cbar=True, ax=axes[1, 1]) axes[1, 1].set_title('R_true: Emission Covariance') axes[1, 1].set_xlabel('Neuron') axes[1, 1].set_ylabel('Neuron')  plt.tight_layout() plt.show() <pre> STEP 1: GENERATING SYNTHETIC NEURAL DATA\n============================================================\nCondition number of A_true: 4.906572570537099\nCondition number of C_true: 3.3799634841061237\nCondition number of Q_true: 5.238426242675188\nCondition number of R_true: 6.706393092485412\nCondition number of observations: 3.534626711349381\nModel structure:\n  State dimension (D): 4\n  Observation dimension (N): 20\n  Time steps (T): 200\n  Cell types: 2\n  Cell type mask: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n  Cell type dimensions: [2 2]\n  Dynamics mask: [ 1  1 -1 -1]\n\n\ud83d\udcca Dataset Generated:\n  \u2022 A true shape: (4, 4)\n  \u2022 C true shape: (20, 4)\n  \u2022 Q true shape: (4, 4)\n  \u2022 R true shape: (20, 20)\n</pre> In\u00a0[3]: Copied! <pre># Generate datasets from CTDS model using ground truth parameters\nprint(\" STEP 1b: GENERATING DATASETS FROM CTDS MODEL\")\nprint(\"=\" * 60)\nsamples=50\n\ndatas=[]\nstates_list = []\nkeys= jr.split(key, samples)\nfor i in range(samples): \n    sampled_states, sampled_observations = ctds.sample(ctds_params, key=keys[i], num_timesteps=T)\n    states_list.append(sampled_states)\n    datas.append(sampled_observations)\n#covert to jax array with shape (samples, T, N)\nbatched_states = jnp.array(states_list)\nbatched_observations = jnp.array(datas)\n\n# Plot all true states in two subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\ncolors = plt.cm.tab10(np.linspace(0, 1, D))\n\n# Left subplot: Individual samples\nfor dim in range(D):\n    for sample in range(min(10, batched_states.shape[0])):\n        axes[0].plot(batched_states[sample, :, dim], \n                    color=colors[dim], alpha=0.3, linewidth=0.5)\n\naxes[0].set_title('Individual Sample Trajectories')\naxes[0].set_xlabel('Time')\naxes[0].set_ylabel('State Value')\naxes[0].grid(True, alpha=0.3)\naxes[0].spines['right'].set_visible(False)\naxes[0].spines['top'].set_visible(False)\n\n# Right subplot: Mean trajectories\nfor dim in range(D):\n    mean_trajectory = jnp.mean(batched_states[:, :, dim], axis=0)\n    axes[1].plot(mean_trajectory, color=colors[dim], linewidth=2, \n                label=f'Dim {dim+1} (mean)')\n\naxes[1].set_title('Mean State Trajectories')\naxes[1].set_xlabel('Time')\naxes[1].set_ylabel('State Value')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].spines['right'].set_visible(False)\naxes[1].spines['top'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n</pre> # Generate datasets from CTDS model using ground truth parameters print(\" STEP 1b: GENERATING DATASETS FROM CTDS MODEL\") print(\"=\" * 60) samples=50  datas=[] states_list = [] keys= jr.split(key, samples) for i in range(samples):      sampled_states, sampled_observations = ctds.sample(ctds_params, key=keys[i], num_timesteps=T)     states_list.append(sampled_states)     datas.append(sampled_observations) #covert to jax array with shape (samples, T, N) batched_states = jnp.array(states_list) batched_observations = jnp.array(datas)  # Plot all true states in two subplots fig, axes = plt.subplots(1, 2, figsize=(20, 8))  colors = plt.cm.tab10(np.linspace(0, 1, D))  # Left subplot: Individual samples for dim in range(D):     for sample in range(min(10, batched_states.shape[0])):         axes[0].plot(batched_states[sample, :, dim],                      color=colors[dim], alpha=0.3, linewidth=0.5)  axes[0].set_title('Individual Sample Trajectories') axes[0].set_xlabel('Time') axes[0].set_ylabel('State Value') axes[0].grid(True, alpha=0.3) axes[0].spines['right'].set_visible(False) axes[0].spines['top'].set_visible(False)  # Right subplot: Mean trajectories for dim in range(D):     mean_trajectory = jnp.mean(batched_states[:, :, dim], axis=0)     axes[1].plot(mean_trajectory, color=colors[dim], linewidth=2,                  label=f'Dim {dim+1} (mean)')  axes[1].set_title('Mean State Trajectories') axes[1].set_xlabel('Time') axes[1].set_ylabel('State Value') axes[1].legend() axes[1].grid(True, alpha=0.3) axes[1].spines['right'].set_visible(False) axes[1].spines['top'].set_visible(False)  plt.tight_layout() plt.show() <pre> STEP 1b: GENERATING DATASETS FROM CTDS MODEL\n============================================================\n</pre> In\u00a0[4]: Copied! <pre># divide into train and test datasets\nnum_train_trials = int(0.8*sample)\ntrain_datas = batched_observations[:num_train_trials]\ntest_datas = batched_observations[num_train_trials:]\ntrain_obs=jnp.mean(train_datas, axis=0)\ntest_obs=jnp.mean(test_datas, axis=0)\n\n# compute LLs for the test and train datasets\ntrue_model_train_ll = ctds.log_prob(ctds_params,jnp.mean(batched_states[:num_train_trials], axis=0), train_obs)\ntrue_model_test_ll = ctds.log_prob(ctds_params,jnp.mean(batched_states[num_train_trials:], axis=0), test_obs)\nprint(\"Train ll:\", true_model_train_ll)\nprint(\"Test ll:\",true_model_test_ll)\n</pre> # divide into train and test datasets num_train_trials = int(0.8*sample) train_datas = batched_observations[:num_train_trials] test_datas = batched_observations[num_train_trials:] train_obs=jnp.mean(train_datas, axis=0) test_obs=jnp.mean(test_datas, axis=0)  # compute LLs for the test and train datasets true_model_train_ll = ctds.log_prob(ctds_params,jnp.mean(batched_states[:num_train_trials], axis=0), train_obs) true_model_test_ll = ctds.log_prob(ctds_params,jnp.mean(batched_states[num_train_trials:], axis=0), test_obs) print(\"Train ll:\", true_model_train_ll) print(\"Test ll:\",true_model_test_ll)  <pre>Train ll: 2109.8048070115683\nTest ll: 2394.4054925762703\n</pre> <p>Fit CTDS with generated data</p> In\u00a0[5]: Copied! <pre># Step 2: Model Fitting \nprint(\"\\nSTEP 2: MODEL FITTING WITH EM ALGORITHM\")\nprint(\"=\" * 60)\n\n\nprint(\" Initializing model parameters...\")\nstart_time = time.time()\n\n# Initialize parameters from observations\ndemo_params_init = ctds.initialize(train_obs.T)\ninit_time = time.time() - start_time\n\nprint(f\"\u2705 Initialization completed in {init_time:.2f} seconds\")\n\n# Fit model using EM algorithm\nprint(\" Running EM algorithm...\")\nnum_em_iters = 50\n\n\n# Track timing\nem_start_time = time.time()\n\n# Run EM fitting with progress tracking \n# Refit on training data\nparams_fitted, test_lls = ctds.fit_em(\n    demo_params_init, \n    train_datas, \n    num_iters=num_em_iters, \n    verbose=True\n)\n\nem_total_time = time.time() - em_start_time\nem_per_iter_time = em_total_time / num_em_iters\nfrom inference import DynamaxLGSSMBackend\nprint(f\"\\n\u2705 EM Algorithm Results:\")\nprint(f\"  \u2022 Total fitting time: {em_total_time:.2f} seconds\")\nprint(f\"  \u2022 Time per iteration: {em_per_iter_time:.3f} seconds\")\nprint(f\"  \u2022 Initial log-likelihood: {test_lls[0]:.2f}\")\nprint(f\"  \u2022 Final log-likelihood: {test_lls[-1]:.2f}\")\nprint(f\"  \u2022 Log-likelihood improvement: {test_lls[-1] - test_lls[1]:.2f}\")\n\n\n# Compute latent states using smoother\nprint(\"\\n\ud83d\udd0d Computing fitted latent trajectories...\")\nsmoothed_means, smoothed_covariances = DynamaxLGSSMBackend.smoother(params_fitted, observations)\nstates_fitted = smoothed_means #shape (T, D)\n\nprint(f\"  \u2022 Fitted states shape: {states_fitted.shape}\")\nprint(f\"  \u2022 Smoothing completed successfully!\")\n\n\n#plot logs\nplt.figure(figsize=(5, 4))\nplt.plot(test_lls[1:], color = 'k')\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['top'].set_visible(False)\nplt.ylabel('LL', fontsize=15)\nplt.xlabel('iteration', fontsize=15)\n</pre> # Step 2: Model Fitting  print(\"\\nSTEP 2: MODEL FITTING WITH EM ALGORITHM\") print(\"=\" * 60)   print(\" Initializing model parameters...\") start_time = time.time()  # Initialize parameters from observations demo_params_init = ctds.initialize(train_obs.T) init_time = time.time() - start_time  print(f\"\u2705 Initialization completed in {init_time:.2f} seconds\")  # Fit model using EM algorithm print(\" Running EM algorithm...\") num_em_iters = 50   # Track timing em_start_time = time.time()  # Run EM fitting with progress tracking  # Refit on training data params_fitted, test_lls = ctds.fit_em(     demo_params_init,      train_datas,      num_iters=num_em_iters,      verbose=True )  em_total_time = time.time() - em_start_time em_per_iter_time = em_total_time / num_em_iters from inference import DynamaxLGSSMBackend print(f\"\\n\u2705 EM Algorithm Results:\") print(f\"  \u2022 Total fitting time: {em_total_time:.2f} seconds\") print(f\"  \u2022 Time per iteration: {em_per_iter_time:.3f} seconds\") print(f\"  \u2022 Initial log-likelihood: {test_lls[0]:.2f}\") print(f\"  \u2022 Final log-likelihood: {test_lls[-1]:.2f}\") print(f\"  \u2022 Log-likelihood improvement: {test_lls[-1] - test_lls[1]:.2f}\")   # Compute latent states using smoother print(\"\\n\ud83d\udd0d Computing fitted latent trajectories...\") smoothed_means, smoothed_covariances = DynamaxLGSSMBackend.smoother(params_fitted, observations) states_fitted = smoothed_means #shape (T, D)  print(f\"  \u2022 Fitted states shape: {states_fitted.shape}\") print(f\"  \u2022 Smoothing completed successfully!\")   #plot logs plt.figure(figsize=(5, 4)) plt.plot(test_lls[1:], color = 'k') plt.gca().spines['right'].set_visible(False) plt.gca().spines['top'].set_visible(False) plt.ylabel('LL', fontsize=15) plt.xlabel('iteration', fontsize=15) <pre>\nSTEP 2: MODEL FITTING WITH EM ALGORITHM\n============================================================\n Initializing model parameters...\n\u2705 Initialization completed in 9.52 seconds\n Running EM algorithm...\n</pre>        100.00% [49/49 00:29&lt;00:00]      <pre>Iteration 1: log-likelihood = -48382.15387601523\nIteration 2: log-likelihood = -32481.22804024085\nIteration 3: log-likelihood = -24829.64387719541\nIteration 4: log-likelihood = -21045.65421820377\nIteration 5: log-likelihood = -19878.26897688544\nIteration 6: log-likelihood = -19906.776540149032\nIteration 7: log-likelihood = -19912.55091219045\nIteration 8: log-likelihood = -19941.842665131044\nIteration 9: log-likelihood = -19957.96784355319\nIteration 10: log-likelihood = -19969.70452897862\nIteration 11: log-likelihood = -19979.547623792663\nIteration 12: log-likelihood = -19986.08093597045\nIteration 13: log-likelihood = -19990.558812800686\nIteration 14: log-likelihood = -19995.58961297587\nIteration 15: log-likelihood = -20001.53597694125\nIteration 16: log-likelihood = -20007.729124596764\nIteration 17: log-likelihood = -20013.618370610595\nIteration 18: log-likelihood = -20018.91985131987\nIteration 19: log-likelihood = -20023.573634476284\nIteration 20: log-likelihood = -20027.645091228653\nIteration 21: log-likelihood = -20031.255242390678\nIteration 22: log-likelihood = -20035.45950168154\nIteration 23: log-likelihood = -20040.55998042568\nIteration 24: log-likelihood = -20046.015053659885\nIteration 25: log-likelihood = -20051.449046893154\nIteration 26: log-likelihood = -20056.666652336997\nIteration 27: log-likelihood = -20061.59715018145\nIteration 28: log-likelihood = -20066.462695931325\nIteration 29: log-likelihood = -20071.657303287197\nIteration 30: log-likelihood = -20077.13503554222\nIteration 31: log-likelihood = -20082.617649881315\nIteration 32: log-likelihood = -20087.809780424643\nIteration 33: log-likelihood = -20092.396299827888\nIteration 34: log-likelihood = -20096.457707413872\nIteration 35: log-likelihood = -20100.120833977908\nIteration 36: log-likelihood = -20103.471201408407\nIteration 37: log-likelihood = -20106.565080876288\nIteration 38: log-likelihood = -20109.428866010567\nIteration 39: log-likelihood = -20112.080700416423\nIteration 40: log-likelihood = -20114.535970562003\nIteration 41: log-likelihood = -20116.808444930775\nIteration 42: log-likelihood = -20118.911045225344\nIteration 43: log-likelihood = -20120.855816731622\nIteration 44: log-likelihood = -20122.65409018827\nIteration 45: log-likelihood = -20124.316478369816\nIteration 46: log-likelihood = -20125.852929362845\nIteration 47: log-likelihood = -20127.272742299967\nIteration 48: log-likelihood = -20128.58459956924\nIteration 49: log-likelihood = -20129.796591405782\n\n\u2705 EM Algorithm Results:\n  \u2022 Total fitting time: 36.28 seconds\n  \u2022 Time per iteration: 0.726 seconds\n  \u2022 Initial log-likelihood: -48382.15\n  \u2022 Final log-likelihood: -20129.80\n  \u2022 Log-likelihood improvement: 12351.43\n\n\ud83d\udd0d Computing fitted latent trajectories...\n  \u2022 Fitted states shape: (200, 4)\n  \u2022 Smoothing completed successfully!\n</pre> Out[5]: <pre>Text(0.5, 0, 'iteration')</pre> In\u00a0[6]: Copied! <pre>#First recover true parameters from fitted model\nA_rec, C_rec, Q_rec, info = align_single_region_ctds(\n    C_true, params_fitted.emissions.weights, params_fitted.dynamics.weights, params_fitted.dynamics.cov, block_sizes=ctds.constraints.cell_type_dimensions, ridge=1e-6\n)\nprint(\"mean |matched corr|:\", info[\"mean_abs_matched_corr\"])\n</pre> #First recover true parameters from fitted model A_rec, C_rec, Q_rec, info = align_single_region_ctds(     C_true, params_fitted.emissions.weights, params_fitted.dynamics.weights, params_fitted.dynamics.cov, block_sizes=ctds.constraints.cell_type_dimensions, ridge=1e-6 ) print(\"mean |matched corr|:\", info[\"mean_abs_matched_corr\"]) <pre>mean |matched corr|: 0.5635193056958845\n</pre> In\u00a0[\u00a0]: Copied! <pre># Compute accuracy metrics\nprint(\"Computing accuracy metrics...\")\n\n\nA_error = jnp.linalg.norm(A_rec - A_true, 'fro')\nC_error = jnp.linalg.norm(C_rec - C_true, 'fro')\nQ_error = jnp.linalg.norm(Q_rec - Q_true, 'fro')\nR_error = jnp.linalg.norm(params_fitted.emissions.cov - R_true, 'fro')\n\"\"\"\nA_error = jnp.linalg.norm(params_fitted.dynamics.weights - A_true, 'fro')\nC_error = jnp.linalg.norm(params_fitted.emissions.weights - C_true, 'fro')\nQ_error = jnp.linalg.norm(params_fitted.dynamics.cov - Q_true, 'fro')\nR_error = jnp.linalg.norm(params_fitted.emissions.cov - R_true, 'fro')\n\"\"\"\n# Relative errors (normalized by true parameter magnitude)\nA_rel_error = A_error / jnp.linalg.norm(A_true, 'fro')\nC_rel_error = C_error / jnp.linalg.norm(C_true, 'fro')\nQ_rel_error = Q_error / jnp.linalg.norm(Q_true, 'fro')\nR_rel_error = R_error / jnp.linalg.norm(R_true, 'fro')\n\nprint(\"\u2705 Accuracy Metrics:\")\nprint(f\"\\nAbsolute Frobenius Errors:\")\nprint(f\"  Dynamics (A):     {A_error:.4f}\")\nprint(f\"  Emissions (C):    {C_error:.4f}\")\nprint(f\"  Process noise (Q): {Q_error:.4f}\")\nprint(f\"  Obs noise (R):    {R_error:.4f}\")\n\nprint(f\"\\nRelative Errors (%):\")\nprint(f\"  Dynamics (A):     {A_rel_error*100:.2f}%\")\nprint(f\"  Emissions (C):    {C_rel_error*100:.2f}%\")\nprint(f\"  Process noise (Q): {Q_rel_error*100:.2f}%\")\nprint(f\"  Obs noise (R):    {R_rel_error*100:.2f}%\")\n\n\"\"\"\n\n# Visualize all parameter matrices in subplots\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Visualize A_true (dynamics weights)\nsns.heatmap(np.array(params_fitted.dynamics.weights), cmap='bwr', center=0, cbar=True, ax=axes[0, 0])\naxes[0, 0].set_title('A_true: Dynamics Matrix')\naxes[0, 0].set_xlabel('Latent Dim')\naxes[0, 0].set_ylabel('Latent Dim')\n\n# Visualize C_true (emission weights)\nsns.heatmap(np.array(params_fitted.emissions.weights), cmap='bwr', center=0, cbar=True, ax=axes[0, 1])\naxes[0, 1].set_title('C_true: Emission Matrix')\naxes[0, 1].set_xlabel('Latent Dim')\naxes[0, 1].set_ylabel('Neuron')\n\n# Visualize Q_true (dynamics covariance)\nsns.heatmap(np.array(params_fitted.dynamics.cov), cmap='bwr', center=0, cbar=True, ax=axes[1, 0])\naxes[1, 0].set_title('Q_true: Dynamics Covariance')\naxes[1, 0].set_xlabel('Latent Dim')\naxes[1, 0].set_ylabel('Latent Dim')\n\n# Visualize R_true (emission covariance)\nsns.heatmap(np.array(params_fitted.emissions.cov), cmap='bwr', center=0, cbar=True, ax=axes[1, 1])\naxes[1, 1].set_title('R_true: Emission Covariance')\naxes[1, 1].set_xlabel('Neuron')\naxes[1, 1].set_ylabel('Neuron')\n\nplt.tight_layout()\nplt.show()\n\"\"\"\n# Visualize all parameter matrices in subplots\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Visualize A_true (dynamics weights)\nsns.heatmap(np.array(A_rec), cmap='bwr', center=0, cbar=True, ax=axes[0, 0])\naxes[0, 0].set_title('A_fitted: Dynamics Matrix')\naxes[0, 0].set_xlabel('Latent Dim')\naxes[0, 0].set_ylabel('Latent Dim')\n\n# Visualize C_true (emission weights)\nsns.heatmap(np.array(params_fitted.emissions.weights), cmap='bwr', center=0, cbar=True, ax=axes[0, 1])\naxes[0, 1].set_title('C_fitted: Emission Matrix')\naxes[0, 1].set_xlabel('Latent Dim')\naxes[0, 1].set_ylabel('Neuron')\n\n# Visualize Q_true (dynamics covariance)\nsns.heatmap(np.array(params_fitted.dynamics.cov), cmap='bwr', center=0, cbar=True, ax=axes[1, 0])\naxes[1, 0].set_title('Q_fitted: Dynamics Covariance')\naxes[1, 0].set_xlabel('Latent Dim')\naxes[1, 0].set_ylabel('Latent Dim')\n\n# Visualize R_true (emission covariance)\nsns.heatmap(np.array(params_fitted.emissions.cov), cmap='bwr', center=0, cbar=True, ax=axes[1, 1])\naxes[1, 1].set_title('R_fitted: Emission Covariance')\naxes[1, 1].set_xlabel('Neuron')\naxes[1, 1].set_ylabel('Neuron')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"dynamics\",params_fitted.dynamics.weights)\nprint(\"emissions\",params_fitted.emissions.weights)\nprint(\"dynamics_cov\",params_fitted.dynamics.cov)\nprint(\"emissions_cov\",params_fitted.emissions.cov)\n</pre> # Compute accuracy metrics print(\"Computing accuracy metrics...\")   A_error = jnp.linalg.norm(A_rec - A_true, 'fro') C_error = jnp.linalg.norm(C_rec - C_true, 'fro') Q_error = jnp.linalg.norm(Q_rec - Q_true, 'fro') R_error = jnp.linalg.norm(params_fitted.emissions.cov - R_true, 'fro') \"\"\" A_error = jnp.linalg.norm(params_fitted.dynamics.weights - A_true, 'fro') C_error = jnp.linalg.norm(params_fitted.emissions.weights - C_true, 'fro') Q_error = jnp.linalg.norm(params_fitted.dynamics.cov - Q_true, 'fro') R_error = jnp.linalg.norm(params_fitted.emissions.cov - R_true, 'fro') \"\"\" # Relative errors (normalized by true parameter magnitude) A_rel_error = A_error / jnp.linalg.norm(A_true, 'fro') C_rel_error = C_error / jnp.linalg.norm(C_true, 'fro') Q_rel_error = Q_error / jnp.linalg.norm(Q_true, 'fro') R_rel_error = R_error / jnp.linalg.norm(R_true, 'fro')  print(\"\u2705 Accuracy Metrics:\") print(f\"\\nAbsolute Frobenius Errors:\") print(f\"  Dynamics (A):     {A_error:.4f}\") print(f\"  Emissions (C):    {C_error:.4f}\") print(f\"  Process noise (Q): {Q_error:.4f}\") print(f\"  Obs noise (R):    {R_error:.4f}\")  print(f\"\\nRelative Errors (%):\") print(f\"  Dynamics (A):     {A_rel_error*100:.2f}%\") print(f\"  Emissions (C):    {C_rel_error*100:.2f}%\") print(f\"  Process noise (Q): {Q_rel_error*100:.2f}%\") print(f\"  Obs noise (R):    {R_rel_error*100:.2f}%\")  \"\"\"  # Visualize all parameter matrices in subplots fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Visualize A_true (dynamics weights) sns.heatmap(np.array(params_fitted.dynamics.weights), cmap='bwr', center=0, cbar=True, ax=axes[0, 0]) axes[0, 0].set_title('A_true: Dynamics Matrix') axes[0, 0].set_xlabel('Latent Dim') axes[0, 0].set_ylabel('Latent Dim')  # Visualize C_true (emission weights) sns.heatmap(np.array(params_fitted.emissions.weights), cmap='bwr', center=0, cbar=True, ax=axes[0, 1]) axes[0, 1].set_title('C_true: Emission Matrix') axes[0, 1].set_xlabel('Latent Dim') axes[0, 1].set_ylabel('Neuron')  # Visualize Q_true (dynamics covariance) sns.heatmap(np.array(params_fitted.dynamics.cov), cmap='bwr', center=0, cbar=True, ax=axes[1, 0]) axes[1, 0].set_title('Q_true: Dynamics Covariance') axes[1, 0].set_xlabel('Latent Dim') axes[1, 0].set_ylabel('Latent Dim')  # Visualize R_true (emission covariance) sns.heatmap(np.array(params_fitted.emissions.cov), cmap='bwr', center=0, cbar=True, ax=axes[1, 1]) axes[1, 1].set_title('R_true: Emission Covariance') axes[1, 1].set_xlabel('Neuron') axes[1, 1].set_ylabel('Neuron')  plt.tight_layout() plt.show() \"\"\" # Visualize all parameter matrices in subplots fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # Visualize A_true (dynamics weights) sns.heatmap(np.array(A_rec), cmap='bwr', center=0, cbar=True, ax=axes[0, 0]) axes[0, 0].set_title('A_fitted: Dynamics Matrix') axes[0, 0].set_xlabel('Latent Dim') axes[0, 0].set_ylabel('Latent Dim')  # Visualize C_true (emission weights) sns.heatmap(np.array(params_fitted.emissions.weights), cmap='bwr', center=0, cbar=True, ax=axes[0, 1]) axes[0, 1].set_title('C_fitted: Emission Matrix') axes[0, 1].set_xlabel('Latent Dim') axes[0, 1].set_ylabel('Neuron')  # Visualize Q_true (dynamics covariance) sns.heatmap(np.array(params_fitted.dynamics.cov), cmap='bwr', center=0, cbar=True, ax=axes[1, 0]) axes[1, 0].set_title('Q_fitted: Dynamics Covariance') axes[1, 0].set_xlabel('Latent Dim') axes[1, 0].set_ylabel('Latent Dim')  # Visualize R_true (emission covariance) sns.heatmap(np.array(params_fitted.emissions.cov), cmap='bwr', center=0, cbar=True, ax=axes[1, 1]) axes[1, 1].set_title('R_fitted: Emission Covariance') axes[1, 1].set_xlabel('Neuron') axes[1, 1].set_ylabel('Neuron')  plt.tight_layout() plt.show()  print(\"dynamics\",params_fitted.dynamics.weights) print(\"emissions\",params_fitted.emissions.weights) print(\"dynamics_cov\",params_fitted.dynamics.cov) print(\"emissions_cov\",params_fitted.emissions.cov) <pre>Computing accuracy metrics...\n\u2705 Accuracy Metrics:\n\nAbsolute Frobenius Errors:\n  Dynamics (A):     1.4378\n  Emissions (C):    2.6304\n  Process noise (Q): 0.5692\n  Obs noise (R):    0.5305\n\nRelative Errors (%):\n  Dynamics (A):     82.35%\n  Emissions (C):    69.11%\n  Process noise (Q): 29062.11%\n  Obs noise (R):    63.59%\n</pre> <pre>dynamics [[ 5.69595512e-02  7.82336036e-02  8.15242639e-12  2.14778170e-11]\n [ 6.62385366e-02  4.15127607e-02 -4.44440748e-02 -2.83561434e-02]\n [ 1.35372009e-01  7.25373497e-04  5.80183869e-01 -6.71811031e-12]\n [ 1.47885622e-01 -5.36144672e-11 -1.66531858e-12  4.80092414e-01]]\nemissions [[0.28445844 0.         0.         0.        ]\n [0.18580272 0.38108364 0.         0.        ]\n [0.46227644 0.38004201 0.         0.        ]\n [0.29253019 0.         0.         0.        ]\n [0.56806555 0.42156836 0.         0.        ]\n [0.11896837 0.37752304 0.         0.        ]\n [0.18819704 0.         0.         0.        ]\n [0.         0.49061619 0.         0.        ]\n [0.33974652 0.26694849 0.         0.        ]\n [0.31239329 0.2795247  0.         0.        ]\n [0.         0.         0.6021017  0.        ]\n [0.         0.         0.45829222 0.29391263]\n [0.         0.         0.16253516 0.40283731]\n [0.         0.         0.36290853 0.        ]\n [0.         0.         0.         0.        ]\n [0.         0.         0.27859187 0.3301346 ]\n [0.         0.         0.24346564 0.40093518]\n [0.         0.         0.36391284 0.        ]\n [0.         0.         0.         0.55609328]\n [0.         0.         0.         0.41516246]]\ndynamics_cov [[ 0.53605911 -0.34397876  0.0377887   0.05674481]\n [-0.34397876  0.51245594  0.01641404  0.00406522]\n [ 0.0377887   0.01641404  0.32143811 -0.05120062]\n [ 0.05674481  0.00406522 -0.05120062  0.27823703]]\nemissions_cov [[0.04992571 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.05076274 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.07927458 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.05564493 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.11992926 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.05713257\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.02481193 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.13125724 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.04537274 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.03954678 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.20308137 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.16913676\n  0.         0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.09442711 0.         0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.07173178 0.         0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.06483202 0.         0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.10170473 0.         0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.11991852 0.\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.07252134\n  0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.1247841  0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.06881739]]\n</pre> In\u00a0[8]: Copied! <pre># Compute latent trajectory recovery\nprint(\"Computing latent trajectory metrics...\")\nstates_true=jnp.mean(batched_states[:num_train_trials], axis=0)\n# Run smoother to get fitted latent states\nposterior = ctds.smoother(params_fitted, train_obs)\nstates_fitted = posterior[0]  \n\n# Latent trajectory MSE\nlatent_mse = jnp.mean((states_fitted - states_true)**2)\n\n# R\u00b2 for latent trajectories (per dimension)\ndef compute_r_squared(y_true, y_pred):\n    ss_res = jnp.sum((y_true - y_pred)**2)\n    ss_tot = jnp.sum((y_true - jnp.mean(y_true))**2)\n    return 1 - (ss_res / ss_tot)\n\nlatent_r2_per_dim = jnp.array([\n    compute_r_squared(states_true[:, i], states_fitted[:, i]) \n    for i in range(D)\n])\nlatent_r2_avg = jnp.mean(latent_r2_per_dim)\n\n# Prediction R\u00b2 for observations\nobservations_pred = states_fitted @ params_fitted.emissions.weights.T\nobs_r2_per_neuron = jnp.array([\n    compute_r_squared(observations[:, i], observations_pred[:, i])\n    for i in range(N)\n])\nobs_r2_avg = jnp.mean(obs_r2_per_neuron)\n\nprint(\"\u2705 Trajectory Recovery Metrics:\")\nprint(f\"  Latent MSE:           {latent_mse:.6f}\")\nprint(f\"  Latent R\u00b2 (avg):      {latent_r2_avg:.4f}\")\nprint(f\"  Latent R\u00b2 (per dim):  {latent_r2_per_dim}\")\nprint(f\"  Observation R\u00b2 (avg): {obs_r2_avg:.4f}\")\nprint(f\"  Obs R\u00b2 range:         [{jnp.min(obs_r2_per_neuron):.3f}, {jnp.max(obs_r2_per_neuron):.3f}]\")\n</pre> # Compute latent trajectory recovery print(\"Computing latent trajectory metrics...\") states_true=jnp.mean(batched_states[:num_train_trials], axis=0) # Run smoother to get fitted latent states posterior = ctds.smoother(params_fitted, train_obs) states_fitted = posterior[0]    # Latent trajectory MSE latent_mse = jnp.mean((states_fitted - states_true)**2)  # R\u00b2 for latent trajectories (per dimension) def compute_r_squared(y_true, y_pred):     ss_res = jnp.sum((y_true - y_pred)**2)     ss_tot = jnp.sum((y_true - jnp.mean(y_true))**2)     return 1 - (ss_res / ss_tot)  latent_r2_per_dim = jnp.array([     compute_r_squared(states_true[:, i], states_fitted[:, i])      for i in range(D) ]) latent_r2_avg = jnp.mean(latent_r2_per_dim)  # Prediction R\u00b2 for observations observations_pred = states_fitted @ params_fitted.emissions.weights.T obs_r2_per_neuron = jnp.array([     compute_r_squared(observations[:, i], observations_pred[:, i])     for i in range(N) ]) obs_r2_avg = jnp.mean(obs_r2_per_neuron)  print(\"\u2705 Trajectory Recovery Metrics:\") print(f\"  Latent MSE:           {latent_mse:.6f}\") print(f\"  Latent R\u00b2 (avg):      {latent_r2_avg:.4f}\") print(f\"  Latent R\u00b2 (per dim):  {latent_r2_per_dim}\") print(f\"  Observation R\u00b2 (avg): {obs_r2_avg:.4f}\") print(f\"  Obs R\u00b2 range:         [{jnp.min(obs_r2_per_neuron):.3f}, {jnp.max(obs_r2_per_neuron):.3f}]\") <pre>Computing latent trajectory metrics...\n\u2705 Trajectory Recovery Metrics:\n  Latent MSE:           0.025722\n  Latent R\u00b2 (avg):      -60.3896\n  Latent R\u00b2 (per dim):  [-80.47080925 -99.986896    -2.4445005  -58.6563408 ]\n  Observation R\u00b2 (avg): -0.3128\n  Obs R\u00b2 range:         [-1.420, 0.004]\n</pre> In\u00a0[9]: Copied! <pre># Generate observation predictions from recovered parameters and compare to true observations\nprint(\"\\n\ud83d\udd0d OBSERVATION PREDICTIONS FROM RECOVERED PARAMETERS\")\nprint(\"=\" * 60)\n\n# Generate predictions using recovered parameters on training data\nprint(\"Generating predictions from recovered parameters...\")\n\n# Use the fitted latent states but with recovered emission matrix\nobs_pred_recovered = states_fitted @ C_rec.T  # Predict using recovered C\nobs_pred_fitted = states_fitted @ params_fitted.emissions.weights.T  # Predict using fitted C\n#obs_pred_true = states_true @ C_true.T  # Predict using true C and true states\nobs_pred_true =observations\n# Compute correlation coefficients between true and predicted observations\ndef compute_correlations(y_true, y_pred):\n    \"\"\"Compute correlation coefficient for each neuron\"\"\"\n    correlations = []\n    for neuron in range(y_true.shape[1]):\n        corr = jnp.corrcoef(y_true[:, neuron], y_pred[:, neuron])[0, 1]\n        correlations.append(corr)\n    return jnp.array(correlations)\n\n# Compute correlations for both recovered and fitted parameters\ncorr_recovered = compute_correlations(train_obs, obs_pred_recovered)\ncorr_fitted = compute_correlations(train_obs, obs_pred_fitted)\ncorr_true = compute_correlations(train_obs, obs_pred_true)\n\nprint(f\"\u2705 Prediction Results:\")\nprint(f\"  Recovered params - Mean correlation: {jnp.nanmean(corr_recovered):.4f}\")\nprint(f\"  Recovered params - Correlation range: [{jnp.nanmin(corr_recovered):.4f}, {jnp.nanmax(corr_recovered):.4f}]\")\nprint(f\"  Fitted params - Mean correlation: {jnp.nanmean(corr_fitted):.4f}\")\nprint(f\"  Fitted params - Correlation range: [{jnp.nanmin(corr_fitted):.4f}, {jnp.nanmax(corr_fitted):.4f}]\")\nprint(f\"  True params - Mean correlation: {jnp.nanmean(corr_true):.4f}\")\nprint(f\"  True params - Correlation range: [{jnp.nanmin(corr_true):.4f}, {jnp.nanmax(corr_true):.4f}]\")\n\n# Create comprehensive visualization\nfig = plt.figure(figsize=(20, 16))\n\n# Select neurons to plot (evenly spaced across all neurons)\nn_neurons_to_plot = 6\nneuron_indices = jnp.linspace(0, N-1, n_neurons_to_plot).astype(int)\ntime_window = slice(0, min(150, T))  # Show first 150 time points\nt_range = jnp.arange(time_window.start, time_window.start + len(train_obs[time_window, 0]))\n\n# Plot 1: Individual neuron traces (recovered parameters)\nfor i, neuron_idx in enumerate(neuron_indices):\n    ax = plt.subplot(3, 3, i+1)\n    \n    # True vs predicted for this neuron\n    true_trace = train_obs[time_window, neuron_idx]\n    pred_trace_recovered = obs_pred_recovered[time_window, neuron_idx]\n    pred_trace_fitted = obs_pred_fitted[time_window, neuron_idx]\n    \n    ax.plot(t_range, true_trace, 'k-', linewidth=2, alpha=0.8, label='True')\n    ax.plot(t_range, pred_trace_recovered, 'r--', linewidth=2, alpha=0.8, label='Predicted (Recovered)')\n    ax.plot(t_range, pred_trace_fitted, 'b:', linewidth=2, alpha=0.6, label='Predicted (Fitted)')\n    \n    # Add correlation coefficients\n    corr_rec = corr_recovered[neuron_idx]\n    corr_fit = corr_fitted[neuron_idx]\n    ax.text(0.02, 0.98, f'r_rec = {corr_rec:.3f}\\nr_fit = {corr_fit:.3f}', \n           transform=ax.transAxes, verticalalignment='top', \n           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    ax.set_xlabel('Time')\n    ax.set_ylabel('Activity')\n    ax.set_title(f'Neuron {neuron_idx}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n# Plot 2: Histogram of correlation coefficients\nax_hist = plt.subplot(3, 3, 7)\nax_hist.hist(corr_recovered, bins=15, alpha=0.7, color='red', edgecolor='black', \n             label=f'Recovered (\u03bc={jnp.nanmean(corr_recovered):.3f})')\nax_hist.hist(corr_fitted, bins=15, alpha=0.5, color='blue', edgecolor='black',\n             label=f'Fitted (\u03bc={jnp.nanmean(corr_fitted):.3f})')\nax_hist.hist(corr_true, bins=15, alpha=0.3, color='green', edgecolor='black',\n             label=f'True (\u03bc={jnp.nanmean(corr_true):.3f})')\nax_hist.axvline(jnp.nanmean(corr_recovered), color='red', linestyle='--', linewidth=2)\nax_hist.axvline(jnp.nanmean(corr_fitted), color='blue', linestyle='--', linewidth=2)\nax_hist.axvline(jnp.nanmean(corr_true), color='green', linestyle='--', linewidth=2)\nax_hist.set_xlabel('Correlation Coefficient')\nax_hist.set_ylabel('Number of Neurons')\nax_hist.set_title('Distribution of Neuron-wise Correlations')\nax_hist.legend()\nax_hist.grid(True, alpha=0.3)\n\n# Plot 3: Scatter plot of correlations (recovered vs fitted)\nax_scatter = plt.subplot(3, 3, 8)\nax_scatter.scatter(corr_fitted, corr_recovered, alpha=0.7, s=50, c='purple')\nax_scatter.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Agreement')\nax_scatter.set_xlabel('Fitted Parameters Correlation')\nax_scatter.set_ylabel('Recovered Parameters Correlation')\nax_scatter.set_title('Recovered vs Fitted Correlations')\nax_scatter.legend()\nax_scatter.grid(True, alpha=0.3)\n\n# Plot 4: Summary statistics comparison\nax_summary = plt.subplot(3, 3, 9)\nmetrics = ['Mean Corr', 'Median Corr', 'Std Corr']\nrecovered_metrics = [jnp.nanmean(corr_recovered), jnp.nanmedian(corr_recovered), jnp.nanstd(corr_recovered)]\nfitted_metrics = [jnp.nanmean(corr_fitted), jnp.nanmedian(corr_fitted), jnp.nanstd(corr_fitted)]\ntrue_metrics = [jnp.nanmean(corr_true), jnp.nanmedian(corr_true), jnp.nanstd(corr_true)]\n\nx_pos = jnp.arange(len(metrics))\nwidth = 0.25\n\nbars1 = ax_summary.bar(x_pos - width, recovered_metrics, width, label='Recovered', alpha=0.8, color='red')\nbars2 = ax_summary.bar(x_pos, fitted_metrics, width, label='Fitted', alpha=0.8, color='blue')\nbars3 = ax_summary.bar(x_pos + width, true_metrics, width, label='True', alpha=0.8, color='green')\n\nax_summary.set_xlabel('Metric')\nax_summary.set_ylabel('Value')\nax_summary.set_title('Summary Statistics Comparison')\nax_summary.set_xticks(x_pos)\nax_summary.set_xticklabels(metrics)\nax_summary.legend()\nax_summary.grid(True, alpha=0.3)\n\n# Add value labels on bars\nfor bars in [bars1, bars2, bars3]:\n    for bar in bars:\n        height = bar.get_height()\n        ax_summary.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n\nplt.suptitle('Observation Predictions: True vs Recovered vs Fitted Parameters', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Additional detailed analysis\nprint(f\"\\n\ud83d\udcca Detailed Analysis:\")\nprint(f\"  Neurons with correlation &gt; 0.8:\")\nprint(f\"    Recovered: {jnp.sum(corr_recovered &gt; 0.8)}/{N} ({100*jnp.mean(corr_recovered &gt; 0.8):.1f}%)\")\nprint(f\"    Fitted:    {jnp.sum(corr_fitted &gt; 0.8)}/{N} ({100*jnp.mean(corr_fitted &gt; 0.8):.1f}%)\")\nprint(f\"    True:      {jnp.sum(corr_true &gt; 0.8)}/{N} ({100*jnp.mean(corr_true &gt; 0.8):.1f}%)\")\n\nprint(f\"  Neurons with correlation &gt; 0.5:\")\nprint(f\"    Recovered: {jnp.sum(corr_recovered &gt; 0.5)}/{N} ({100*jnp.mean(corr_recovered &gt; 0.5):.1f}%)\")\nprint(f\"    Fitted:    {jnp.sum(corr_fitted &gt; 0.5)}/{N} ({100*jnp.mean(corr_fitted &gt; 0.5):.1f}%)\")\nprint(f\"    True:      {jnp.sum(corr_true &gt; 0.5)}/{N} ({100*jnp.mean(corr_true &gt; 0.5):.1f}%)\")\n\n# Compute RMSE and R\u00b2 for all parameter sets\nrmse_recovered = jnp.sqrt(jnp.mean((obs_pred_recovered - train_obs)**2))\nrmse_fitted = jnp.sqrt(jnp.mean((obs_pred_fitted - train_obs)**2))\nrmse_true = jnp.sqrt(jnp.mean((obs_pred_true - train_obs)**2))\n\nr2_recovered = compute_r_squared(train_obs.flatten(), obs_pred_recovered.flatten())\nr2_fitted = compute_r_squared(train_obs.flatten(), obs_pred_fitted.flatten())\nr2_true = compute_r_squared(train_obs.flatten(), obs_pred_true.flatten())\n\nprint(f\"\\n\ud83d\udcc8 Prediction Quality Metrics:\")\nprint(f\"  RMSE:\")\nprint(f\"    Recovered: {rmse_recovered:.4f}\")\nprint(f\"    Fitted:    {rmse_fitted:.4f}\")\nprint(f\"    True:      {rmse_true:.4f}\")\nprint(f\"  R\u00b2:\")\nprint(f\"    Recovered: {r2_recovered:.4f}\")\nprint(f\"    Fitted:    {r2_fitted:.4f}\")\nprint(f\"    True:      {r2_true:.4f}\")\n\nprint(\"\u2705 Observation prediction analysis completed!\")\n</pre> # Generate observation predictions from recovered parameters and compare to true observations print(\"\\n\ud83d\udd0d OBSERVATION PREDICTIONS FROM RECOVERED PARAMETERS\") print(\"=\" * 60)  # Generate predictions using recovered parameters on training data print(\"Generating predictions from recovered parameters...\")  # Use the fitted latent states but with recovered emission matrix obs_pred_recovered = states_fitted @ C_rec.T  # Predict using recovered C obs_pred_fitted = states_fitted @ params_fitted.emissions.weights.T  # Predict using fitted C #obs_pred_true = states_true @ C_true.T  # Predict using true C and true states obs_pred_true =observations # Compute correlation coefficients between true and predicted observations def compute_correlations(y_true, y_pred):     \"\"\"Compute correlation coefficient for each neuron\"\"\"     correlations = []     for neuron in range(y_true.shape[1]):         corr = jnp.corrcoef(y_true[:, neuron], y_pred[:, neuron])[0, 1]         correlations.append(corr)     return jnp.array(correlations)  # Compute correlations for both recovered and fitted parameters corr_recovered = compute_correlations(train_obs, obs_pred_recovered) corr_fitted = compute_correlations(train_obs, obs_pred_fitted) corr_true = compute_correlations(train_obs, obs_pred_true)  print(f\"\u2705 Prediction Results:\") print(f\"  Recovered params - Mean correlation: {jnp.nanmean(corr_recovered):.4f}\") print(f\"  Recovered params - Correlation range: [{jnp.nanmin(corr_recovered):.4f}, {jnp.nanmax(corr_recovered):.4f}]\") print(f\"  Fitted params - Mean correlation: {jnp.nanmean(corr_fitted):.4f}\") print(f\"  Fitted params - Correlation range: [{jnp.nanmin(corr_fitted):.4f}, {jnp.nanmax(corr_fitted):.4f}]\") print(f\"  True params - Mean correlation: {jnp.nanmean(corr_true):.4f}\") print(f\"  True params - Correlation range: [{jnp.nanmin(corr_true):.4f}, {jnp.nanmax(corr_true):.4f}]\")  # Create comprehensive visualization fig = plt.figure(figsize=(20, 16))  # Select neurons to plot (evenly spaced across all neurons) n_neurons_to_plot = 6 neuron_indices = jnp.linspace(0, N-1, n_neurons_to_plot).astype(int) time_window = slice(0, min(150, T))  # Show first 150 time points t_range = jnp.arange(time_window.start, time_window.start + len(train_obs[time_window, 0]))  # Plot 1: Individual neuron traces (recovered parameters) for i, neuron_idx in enumerate(neuron_indices):     ax = plt.subplot(3, 3, i+1)          # True vs predicted for this neuron     true_trace = train_obs[time_window, neuron_idx]     pred_trace_recovered = obs_pred_recovered[time_window, neuron_idx]     pred_trace_fitted = obs_pred_fitted[time_window, neuron_idx]          ax.plot(t_range, true_trace, 'k-', linewidth=2, alpha=0.8, label='True')     ax.plot(t_range, pred_trace_recovered, 'r--', linewidth=2, alpha=0.8, label='Predicted (Recovered)')     ax.plot(t_range, pred_trace_fitted, 'b:', linewidth=2, alpha=0.6, label='Predicted (Fitted)')          # Add correlation coefficients     corr_rec = corr_recovered[neuron_idx]     corr_fit = corr_fitted[neuron_idx]     ax.text(0.02, 0.98, f'r_rec = {corr_rec:.3f}\\nr_fit = {corr_fit:.3f}',             transform=ax.transAxes, verticalalignment='top',             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))          ax.set_xlabel('Time')     ax.set_ylabel('Activity')     ax.set_title(f'Neuron {neuron_idx}')     ax.legend()     ax.grid(True, alpha=0.3)  # Plot 2: Histogram of correlation coefficients ax_hist = plt.subplot(3, 3, 7) ax_hist.hist(corr_recovered, bins=15, alpha=0.7, color='red', edgecolor='black',               label=f'Recovered (\u03bc={jnp.nanmean(corr_recovered):.3f})') ax_hist.hist(corr_fitted, bins=15, alpha=0.5, color='blue', edgecolor='black',              label=f'Fitted (\u03bc={jnp.nanmean(corr_fitted):.3f})') ax_hist.hist(corr_true, bins=15, alpha=0.3, color='green', edgecolor='black',              label=f'True (\u03bc={jnp.nanmean(corr_true):.3f})') ax_hist.axvline(jnp.nanmean(corr_recovered), color='red', linestyle='--', linewidth=2) ax_hist.axvline(jnp.nanmean(corr_fitted), color='blue', linestyle='--', linewidth=2) ax_hist.axvline(jnp.nanmean(corr_true), color='green', linestyle='--', linewidth=2) ax_hist.set_xlabel('Correlation Coefficient') ax_hist.set_ylabel('Number of Neurons') ax_hist.set_title('Distribution of Neuron-wise Correlations') ax_hist.legend() ax_hist.grid(True, alpha=0.3)  # Plot 3: Scatter plot of correlations (recovered vs fitted) ax_scatter = plt.subplot(3, 3, 8) ax_scatter.scatter(corr_fitted, corr_recovered, alpha=0.7, s=50, c='purple') ax_scatter.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Agreement') ax_scatter.set_xlabel('Fitted Parameters Correlation') ax_scatter.set_ylabel('Recovered Parameters Correlation') ax_scatter.set_title('Recovered vs Fitted Correlations') ax_scatter.legend() ax_scatter.grid(True, alpha=0.3)  # Plot 4: Summary statistics comparison ax_summary = plt.subplot(3, 3, 9) metrics = ['Mean Corr', 'Median Corr', 'Std Corr'] recovered_metrics = [jnp.nanmean(corr_recovered), jnp.nanmedian(corr_recovered), jnp.nanstd(corr_recovered)] fitted_metrics = [jnp.nanmean(corr_fitted), jnp.nanmedian(corr_fitted), jnp.nanstd(corr_fitted)] true_metrics = [jnp.nanmean(corr_true), jnp.nanmedian(corr_true), jnp.nanstd(corr_true)]  x_pos = jnp.arange(len(metrics)) width = 0.25  bars1 = ax_summary.bar(x_pos - width, recovered_metrics, width, label='Recovered', alpha=0.8, color='red') bars2 = ax_summary.bar(x_pos, fitted_metrics, width, label='Fitted', alpha=0.8, color='blue') bars3 = ax_summary.bar(x_pos + width, true_metrics, width, label='True', alpha=0.8, color='green')  ax_summary.set_xlabel('Metric') ax_summary.set_ylabel('Value') ax_summary.set_title('Summary Statistics Comparison') ax_summary.set_xticks(x_pos) ax_summary.set_xticklabels(metrics) ax_summary.legend() ax_summary.grid(True, alpha=0.3)  # Add value labels on bars for bars in [bars1, bars2, bars3]:     for bar in bars:         height = bar.get_height()         ax_summary.text(bar.get_x() + bar.get_width()/2., height + 0.01,                        f'{height:.3f}', ha='center', va='bottom', fontsize=8)  plt.suptitle('Observation Predictions: True vs Recovered vs Fitted Parameters', fontsize=16, fontweight='bold') plt.tight_layout() plt.show()  # Additional detailed analysis print(f\"\\n\ud83d\udcca Detailed Analysis:\") print(f\"  Neurons with correlation &gt; 0.8:\") print(f\"    Recovered: {jnp.sum(corr_recovered &gt; 0.8)}/{N} ({100*jnp.mean(corr_recovered &gt; 0.8):.1f}%)\") print(f\"    Fitted:    {jnp.sum(corr_fitted &gt; 0.8)}/{N} ({100*jnp.mean(corr_fitted &gt; 0.8):.1f}%)\") print(f\"    True:      {jnp.sum(corr_true &gt; 0.8)}/{N} ({100*jnp.mean(corr_true &gt; 0.8):.1f}%)\")  print(f\"  Neurons with correlation &gt; 0.5:\") print(f\"    Recovered: {jnp.sum(corr_recovered &gt; 0.5)}/{N} ({100*jnp.mean(corr_recovered &gt; 0.5):.1f}%)\") print(f\"    Fitted:    {jnp.sum(corr_fitted &gt; 0.5)}/{N} ({100*jnp.mean(corr_fitted &gt; 0.5):.1f}%)\") print(f\"    True:      {jnp.sum(corr_true &gt; 0.5)}/{N} ({100*jnp.mean(corr_true &gt; 0.5):.1f}%)\")  # Compute RMSE and R\u00b2 for all parameter sets rmse_recovered = jnp.sqrt(jnp.mean((obs_pred_recovered - train_obs)**2)) rmse_fitted = jnp.sqrt(jnp.mean((obs_pred_fitted - train_obs)**2)) rmse_true = jnp.sqrt(jnp.mean((obs_pred_true - train_obs)**2))  r2_recovered = compute_r_squared(train_obs.flatten(), obs_pred_recovered.flatten()) r2_fitted = compute_r_squared(train_obs.flatten(), obs_pred_fitted.flatten()) r2_true = compute_r_squared(train_obs.flatten(), obs_pred_true.flatten())  print(f\"\\n\ud83d\udcc8 Prediction Quality Metrics:\") print(f\"  RMSE:\") print(f\"    Recovered: {rmse_recovered:.4f}\") print(f\"    Fitted:    {rmse_fitted:.4f}\") print(f\"    True:      {rmse_true:.4f}\") print(f\"  R\u00b2:\") print(f\"    Recovered: {r2_recovered:.4f}\") print(f\"    Fitted:    {r2_fitted:.4f}\") print(f\"    True:      {r2_true:.4f}\")  print(\"\u2705 Observation prediction analysis completed!\") <pre>\n\ud83d\udd0d OBSERVATION PREDICTIONS FROM RECOVERED PARAMETERS\n============================================================\nGenerating predictions from recovered parameters...\n\u2705 Prediction Results:\n  Recovered params - Mean correlation: 0.5070\n  Recovered params - Correlation range: [0.4203, 0.6130]\n  Fitted params - Mean correlation: 0.5112\n  Fitted params - Correlation range: [0.4338, 0.6130]\n  True params - Mean correlation: 0.0082\n  True params - Correlation range: [-0.1064, 0.1562]\n</pre> <pre>\n\ud83d\udcca Detailed Analysis:\n  Neurons with correlation &gt; 0.8:\n    Recovered: 0/20 (0.0%)\n    Fitted:    0/20 (0.0%)\n    True:      0/20 (0.0%)\n  Neurons with correlation &gt; 0.5:\n    Recovered: 11/20 (55.0%)\n    Fitted:    11/20 (55.0%)\n    True:      0/20 (0.0%)\n\n\ud83d\udcc8 Prediction Quality Metrics:\n  RMSE:\n    Recovered: 0.1319\n    Fitted:    0.1319\n    True:      0.4664\n  R\u00b2:\n    Recovered: 0.2245\n    Fitted:    0.2251\n    True:      -8.6921\n\u2705 Observation prediction analysis completed!\n</pre> In\u00a0[10]: Copied! <pre>plt.figure(figsize=(12, 4))\nsns.heatmap(np.array(obs_pred_fitted),center=0, cmap='bwr', cbar=True)\nplt.title('Observations predicted')\nplt.ylabel('Time')\nplt.xlabel('Neuron')\nplt.show()\n\n\nplt.figure(figsize=(12, 4))\nsns.heatmap(np.array(obs_pred_true),center=0, cmap='bwr', cbar=True)\nplt.title('Observations true')\nplt.ylabel('Time')\nplt.xlabel('Neuron')\nplt.show()\n\nplt.figure(figsize=(12, 4))\nsns.heatmap(np.array(obs_pred_recovered),center=0, cmap='bwr', cbar=True)\nplt.title('Observations recovered')\nplt.ylabel('Time')\nplt.xlabel('Neuron')\nplt.show()\n</pre> plt.figure(figsize=(12, 4)) sns.heatmap(np.array(obs_pred_fitted),center=0, cmap='bwr', cbar=True) plt.title('Observations predicted') plt.ylabel('Time') plt.xlabel('Neuron') plt.show()   plt.figure(figsize=(12, 4)) sns.heatmap(np.array(obs_pred_true),center=0, cmap='bwr', cbar=True) plt.title('Observations true') plt.ylabel('Time') plt.xlabel('Neuron') plt.show()  plt.figure(figsize=(12, 4)) sns.heatmap(np.array(obs_pred_recovered),center=0, cmap='bwr', cbar=True) plt.title('Observations recovered') plt.ylabel('Time') plt.xlabel('Neuron') plt.show() In\u00a0[11]: Copied! <pre># Latent trajectory comparison\nfig, axes = plt.subplots(3, 2, figsize=(16, 12))\naxes = axes.flatten()\n\n# Plot first 6 dimensions (all dimensions in our case)\ntime_points = jnp.arange(T)\ncolors = plt.cm.Set1(np.linspace(0, 1, D))\n\nfor i in range(D):\n    ax = axes[i]\n    \n    # Plot true and fitted trajectories\n    ax.plot(time_points, states_true[:, i], color=colors[i], linewidth=2, \n           label=f'True (Dim {i+1})', alpha=0.8)\n    ax.plot(time_points, states_fitted[:, i], color=colors[i], linewidth=2, \n           linestyle='--', label=f'Fitted (Dim {i+1})', alpha=0.8)\n    \n    # Add R\u00b2 score\n    r2_score = latent_r2_per_dim[i]\n    ax.text(0.02, 0.98, f'R\u00b2 = {r2_score:.3f}', transform=ax.transAxes, \n           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    ax.set_xlabel('Time')\n    ax.set_ylabel(f'State {i+1}')\n    ax.set_title(f'Latent Dimension {i+1} {\"(Excitatory)\" if i &lt; 3 else \"(Inhibitory)\"}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n# Residual analysis\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Latent residuals over time\nlatent_residuals = states_fitted - states_true\nfor i in range(min(D-1, D)):\n    axes[0, 0].plot(latent_residuals[:, i], label=f'Dim {i+1}', alpha=0.7)\naxes[0, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\naxes[0, 0].set_title('Latent State Residuals Over Time')\naxes[0, 0].set_xlabel('Time')\naxes[0, 0].set_ylabel('Residual (Fitted - True)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Observation residuals over time\nobs_residuals = observations_pred - observations\naxes[0, 1].plot(jnp.mean(jnp.abs(obs_residuals), axis=1), color='red', linewidth=2)\naxes[0, 1].set_title('Mean Absolute Observation Residuals')\naxes[0, 1].set_xlabel('Time')\naxes[0, 1].set_ylabel('Mean |Residual|')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Residual distributions\naxes[1, 0].hist(latent_residuals.flatten(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\naxes[1, 0].set_title('Latent Residual Distribution')\naxes[1, 0].set_xlabel('Residual Value')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].hist(obs_residuals.flatten(), bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\naxes[1, 1].set_title('Observation Residual Distribution')\naxes[1, 1].set_xlabel('Residual Value')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\nprint(\"\u2705 Latent trajectory comparison plots created\")\n</pre> # Latent trajectory comparison fig, axes = plt.subplots(3, 2, figsize=(16, 12)) axes = axes.flatten()  # Plot first 6 dimensions (all dimensions in our case) time_points = jnp.arange(T) colors = plt.cm.Set1(np.linspace(0, 1, D))  for i in range(D):     ax = axes[i]          # Plot true and fitted trajectories     ax.plot(time_points, states_true[:, i], color=colors[i], linewidth=2,             label=f'True (Dim {i+1})', alpha=0.8)     ax.plot(time_points, states_fitted[:, i], color=colors[i], linewidth=2,             linestyle='--', label=f'Fitted (Dim {i+1})', alpha=0.8)          # Add R\u00b2 score     r2_score = latent_r2_per_dim[i]     ax.text(0.02, 0.98, f'R\u00b2 = {r2_score:.3f}', transform=ax.transAxes,             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))          ax.set_xlabel('Time')     ax.set_ylabel(f'State {i+1}')     ax.set_title(f'Latent Dimension {i+1} {\"(Excitatory)\" if i &lt; 3 else \"(Inhibitory)\"}')     ax.legend()     ax.grid(True, alpha=0.3)  plt.tight_layout() plt.show()   # Residual analysis fig, axes = plt.subplots(2, 2, figsize=(14, 10))  # Latent residuals over time latent_residuals = states_fitted - states_true for i in range(min(D-1, D)):     axes[0, 0].plot(latent_residuals[:, i], label=f'Dim {i+1}', alpha=0.7) axes[0, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5) axes[0, 0].set_title('Latent State Residuals Over Time') axes[0, 0].set_xlabel('Time') axes[0, 0].set_ylabel('Residual (Fitted - True)') axes[0, 0].legend() axes[0, 0].grid(True, alpha=0.3)  # Observation residuals over time obs_residuals = observations_pred - observations axes[0, 1].plot(jnp.mean(jnp.abs(obs_residuals), axis=1), color='red', linewidth=2) axes[0, 1].set_title('Mean Absolute Observation Residuals') axes[0, 1].set_xlabel('Time') axes[0, 1].set_ylabel('Mean |Residual|') axes[0, 1].grid(True, alpha=0.3)  # Residual distributions axes[1, 0].hist(latent_residuals.flatten(), bins=30, alpha=0.7, color='skyblue', edgecolor='black') axes[1, 0].set_title('Latent Residual Distribution') axes[1, 0].set_xlabel('Residual Value') axes[1, 0].set_ylabel('Frequency') axes[1, 0].grid(True, alpha=0.3)  axes[1, 1].hist(obs_residuals.flatten(), bins=30, alpha=0.7, color='lightcoral', edgecolor='black') axes[1, 1].set_title('Observation Residual Distribution') axes[1, 1].set_xlabel('Residual Value') axes[1, 1].set_ylabel('Frequency') axes[1, 1].grid(True, alpha=0.3)  plt.tight_layout() plt.show() print(\"\u2705 Latent trajectory comparison plots created\") <pre>\u2705 Latent trajectory comparison plots created\n</pre> In\u00a0[12]: Copied! <pre># Numerical stability analysis\nprint(\"Performing numerical stability checks...\")\n\n# 1. Check eigenvalues of fitted dynamics matrix\nA_fitted = params_fitted.dynamics.weights\neigenvals_fitted = jnp.linalg.eigvals(A_fitted)\nmax_eigenval = jnp.max(jnp.abs(eigenvals_fitted))\nis_stable = max_eigenval &lt; 1.0\n\nprint(\"\u2705 Dynamics Matrix Stability:\")\nprint(f\"  Eigenvalues: {eigenvals_fitted}\")\nprint(f\"  Max eigenvalue magnitude: {max_eigenval:.4f}\")\nprint(f\"  System is {'stable' if is_stable else 'UNSTABLE'}\")\nprint(f\"  Stability margin: {1.0 - max_eigenval:.4f}\")\n\n# 2. Check positive semi-definiteness of Q and R\nQ_fitted = params_fitted.dynamics.cov\nR_fitted = params_fitted.emissions.cov\n\n# Check Q (process noise)\nQ_eigenvals = jnp.linalg.eigvals(Q_fitted)\nQ_is_psd = jnp.all(Q_eigenvals &gt;= -1e-10)  # Allow small numerical errors\nQ_condition_number = jnp.max(Q_eigenvals) / jnp.max(Q_eigenvals[Q_eigenvals &gt; 1e-10])\n\nprint(f\"\\nProcess Noise Covariance (Q):\")\nprint(f\"  Eigenvalues: {Q_eigenvals}\")\nprint(f\"  Is positive semi-definite: {Q_is_psd}\")\nprint(f\"  Condition number: {Q_condition_number:.2e}\")\nprint(f\"  Determinant: {jnp.linalg.det(Q_fitted):.2e}\")\n\n# Check R (observation noise)\nR_eigenvals = jnp.linalg.eigvals(R_fitted)\nR_is_psd = jnp.all(R_eigenvals &gt;= -1e-10)\nR_condition_number = jnp.max(R_eigenvals) / jnp.max(R_eigenvals[R_eigenvals &gt; 1e-10])\n\nprint(f\"\\nObservation Noise Covariance (R):\")\nprint(f\"  Eigenvalues range: [{jnp.min(R_eigenvals):.2e}, {jnp.max(R_eigenvals):.2e}]\")\nprint(f\"  Is positive semi-definite: {R_is_psd}\")\nprint(f\"  Condition number: {R_condition_number:.2e}\")\nprint(f\"  Determinant: {jnp.linalg.det(R_fitted):.2e}\")\n\n# 3. Check matrix norms and conditioning\nA_condition = jnp.linalg.cond(A_fitted)\nC_condition = jnp.linalg.cond(params_fitted.emissions.weights)\n\nprint(f\"\\nMatrix Conditioning:\")\nprint(f\"  Dynamics matrix (A) condition number: {A_condition:.2e}\")\nprint(f\"  Emission matrix (C) condition number:  {C_condition:.2e}\")\n\n# 4. Parameter magnitudes\nA_norm = jnp.linalg.norm(A_fitted, 'fro')\nC_norm = jnp.linalg.norm(params_fitted.emissions.weights, 'fro')\n\nprint(f\"\\nParameter Magnitudes:\")\nprint(f\"  ||A||_F: {A_norm:.3f}\")\nprint(f\"  ||C||_F: {C_norm:.3f}\")\nprint(f\"  ||Q||_F: {jnp.linalg.norm(Q_fitted, 'fro'):.3f}\")\nprint(f\"  ||R||_F: {jnp.linalg.norm(R_fitted, 'fro'):.3f}\")\n</pre> # Numerical stability analysis print(\"Performing numerical stability checks...\")  # 1. Check eigenvalues of fitted dynamics matrix A_fitted = params_fitted.dynamics.weights eigenvals_fitted = jnp.linalg.eigvals(A_fitted) max_eigenval = jnp.max(jnp.abs(eigenvals_fitted)) is_stable = max_eigenval &lt; 1.0  print(\"\u2705 Dynamics Matrix Stability:\") print(f\"  Eigenvalues: {eigenvals_fitted}\") print(f\"  Max eigenvalue magnitude: {max_eigenval:.4f}\") print(f\"  System is {'stable' if is_stable else 'UNSTABLE'}\") print(f\"  Stability margin: {1.0 - max_eigenval:.4f}\")  # 2. Check positive semi-definiteness of Q and R Q_fitted = params_fitted.dynamics.cov R_fitted = params_fitted.emissions.cov  # Check Q (process noise) Q_eigenvals = jnp.linalg.eigvals(Q_fitted) Q_is_psd = jnp.all(Q_eigenvals &gt;= -1e-10)  # Allow small numerical errors Q_condition_number = jnp.max(Q_eigenvals) / jnp.max(Q_eigenvals[Q_eigenvals &gt; 1e-10])  print(f\"\\nProcess Noise Covariance (Q):\") print(f\"  Eigenvalues: {Q_eigenvals}\") print(f\"  Is positive semi-definite: {Q_is_psd}\") print(f\"  Condition number: {Q_condition_number:.2e}\") print(f\"  Determinant: {jnp.linalg.det(Q_fitted):.2e}\")  # Check R (observation noise) R_eigenvals = jnp.linalg.eigvals(R_fitted) R_is_psd = jnp.all(R_eigenvals &gt;= -1e-10) R_condition_number = jnp.max(R_eigenvals) / jnp.max(R_eigenvals[R_eigenvals &gt; 1e-10])  print(f\"\\nObservation Noise Covariance (R):\") print(f\"  Eigenvalues range: [{jnp.min(R_eigenvals):.2e}, {jnp.max(R_eigenvals):.2e}]\") print(f\"  Is positive semi-definite: {R_is_psd}\") print(f\"  Condition number: {R_condition_number:.2e}\") print(f\"  Determinant: {jnp.linalg.det(R_fitted):.2e}\")  # 3. Check matrix norms and conditioning A_condition = jnp.linalg.cond(A_fitted) C_condition = jnp.linalg.cond(params_fitted.emissions.weights)  print(f\"\\nMatrix Conditioning:\") print(f\"  Dynamics matrix (A) condition number: {A_condition:.2e}\") print(f\"  Emission matrix (C) condition number:  {C_condition:.2e}\")  # 4. Parameter magnitudes A_norm = jnp.linalg.norm(A_fitted, 'fro') C_norm = jnp.linalg.norm(params_fitted.emissions.weights, 'fro')  print(f\"\\nParameter Magnitudes:\") print(f\"  ||A||_F: {A_norm:.3f}\") print(f\"  ||C||_F: {C_norm:.3f}\") print(f\"  ||Q||_F: {jnp.linalg.norm(Q_fitted, 'fro'):.3f}\") print(f\"  ||R||_F: {jnp.linalg.norm(R_fitted, 'fro'):.3f}\") <pre>Performing numerical stability checks...\n\u2705 Dynamics Matrix Stability:\n  Eigenvalues: [-0.03228583+0.j  0.13439604+0.j  0.47820723+0.j  0.57843116+0.j]\n  Max eigenvalue magnitude: 0.5784\n  System is stable\n  Stability margin: 0.4216\n\nProcess Noise Covariance (Q):\n  Eigenvalues: [0.8711756 +0.j 0.14652781+0.j 0.27469565+0.j 0.35579113+0.j]\n  Is positive semi-definite: True\n  Condition number: 1.00e+00+0.00e+00j\n  Determinant: 1.25e-02\n\nObservation Noise Covariance (R):\n  Eigenvalues range: [2.48e-02+0.00e+00j, 2.03e-01+0.00e+00j]\n  Is positive semi-definite: True\n  Condition number: 1.00e+00+0.00e+00j\n  Determinant: 5.13e-23\n\nMatrix Conditioning:\n  Dynamics matrix (A) condition number: 1.89e+01\n  Emission matrix (C) condition number:  2.42e+00\n\nParameter Magnitudes:\n  ||A||_F: 0.791\n  ||C||_F: 2.000\n  ||Q||_F: 0.991\n  ||R||_F: 0.438\n</pre> In\u00a0[13]: Copied! <pre># Create stability visualization\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Eigenvalue plot for dynamics\neigenvals_true = jnp.linalg.eigvals(A_true)\neigenvals_fitted = jnp.linalg.eigvals(A_fitted)\n\n# Plot in complex plane\naxes[0, 0].scatter(jnp.real(eigenvals_true), jnp.imag(eigenvals_true), \n                  c='blue', s=80, label='True', alpha=0.7, marker='o')\naxes[0, 0].scatter(jnp.real(eigenvals_fitted), jnp.imag(eigenvals_fitted), \n                  c='red', s=80, label='Fitted', alpha=0.7, marker='x')\n\n# Draw unit circle\ntheta = jnp.linspace(0, 2*jnp.pi, 100)\naxes[0, 0].plot(jnp.cos(theta), jnp.sin(theta), 'k--', alpha=0.5, label='Unit Circle')\naxes[0, 0].set_xlabel('Real Part')\naxes[0, 0].set_ylabel('Imaginary Part')\naxes[0, 0].set_title('Eigenvalues of Dynamics Matrix')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].axis('equal')\n\n# 2. Covariance eigenvalues\nQ_eigenvals = jnp.linalg.eigvals(Q_fitted)\nR_eigenvals = jnp.linalg.eigvals(R_fitted)\n\naxes[0, 1].bar(range(len(Q_eigenvals)), Q_eigenvals, alpha=0.7, label='Q (Process)', color='skyblue')\naxes[0, 1].set_xlabel('Eigenvalue Index')\naxes[0, 1].set_ylabel('Eigenvalue')\naxes[0, 1].set_title('Process Noise Eigenvalues')\naxes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Observation noise eigenvalues (show subset)\nn_show = min(20, len(R_eigenvals))\naxes[1, 0].bar(range(n_show), R_eigenvals[:n_show], alpha=0.7, color='lightcoral')\naxes[1, 0].set_xlabel('Eigenvalue Index')\naxes[1, 0].set_ylabel('Eigenvalue')\naxes[1, 0].set_title(f'Observation Noise Eigenvalues (first {n_show})')\naxes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Condition numbers\nmatrices = ['A (Dynamics)', 'C (Emissions)', 'Q (Process)', 'R (Observation)']\ncondition_numbers = [\n    jnp.linalg.cond(A_fitted),\n    jnp.linalg.cond(params_fitted.emissions.weights),\n    jnp.linalg.cond(Q_fitted),\n    jnp.linalg.cond(R_fitted)\n]\n\nbars = axes[1, 1].bar(matrices, condition_numbers, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\naxes[1, 1].set_ylabel('Condition Number (log scale)')\naxes[1, 1].set_yscale('log')\naxes[1, 1].set_title('Matrix Condition Numbers')\naxes[1, 1].tick_params(axis='x', rotation=45)\n\n# Add value labels\nfor bar, value in zip(bars, condition_numbers):\n    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,\n                   f'{value:.1e}', ha='center', va='bottom', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2705 Numerical stability visualization created\")\n</pre> # Create stability visualization fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # 1. Eigenvalue plot for dynamics eigenvals_true = jnp.linalg.eigvals(A_true) eigenvals_fitted = jnp.linalg.eigvals(A_fitted)  # Plot in complex plane axes[0, 0].scatter(jnp.real(eigenvals_true), jnp.imag(eigenvals_true),                    c='blue', s=80, label='True', alpha=0.7, marker='o') axes[0, 0].scatter(jnp.real(eigenvals_fitted), jnp.imag(eigenvals_fitted),                    c='red', s=80, label='Fitted', alpha=0.7, marker='x')  # Draw unit circle theta = jnp.linspace(0, 2*jnp.pi, 100) axes[0, 0].plot(jnp.cos(theta), jnp.sin(theta), 'k--', alpha=0.5, label='Unit Circle') axes[0, 0].set_xlabel('Real Part') axes[0, 0].set_ylabel('Imaginary Part') axes[0, 0].set_title('Eigenvalues of Dynamics Matrix') axes[0, 0].legend() axes[0, 0].grid(True, alpha=0.3) axes[0, 0].axis('equal')  # 2. Covariance eigenvalues Q_eigenvals = jnp.linalg.eigvals(Q_fitted) R_eigenvals = jnp.linalg.eigvals(R_fitted)  axes[0, 1].bar(range(len(Q_eigenvals)), Q_eigenvals, alpha=0.7, label='Q (Process)', color='skyblue') axes[0, 1].set_xlabel('Eigenvalue Index') axes[0, 1].set_ylabel('Eigenvalue') axes[0, 1].set_title('Process Noise Eigenvalues') axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5) axes[0, 1].grid(True, alpha=0.3)  # 3. Observation noise eigenvalues (show subset) n_show = min(20, len(R_eigenvals)) axes[1, 0].bar(range(n_show), R_eigenvals[:n_show], alpha=0.7, color='lightcoral') axes[1, 0].set_xlabel('Eigenvalue Index') axes[1, 0].set_ylabel('Eigenvalue') axes[1, 0].set_title(f'Observation Noise Eigenvalues (first {n_show})') axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5) axes[1, 0].grid(True, alpha=0.3)  # 4. Condition numbers matrices = ['A (Dynamics)', 'C (Emissions)', 'Q (Process)', 'R (Observation)'] condition_numbers = [     jnp.linalg.cond(A_fitted),     jnp.linalg.cond(params_fitted.emissions.weights),     jnp.linalg.cond(Q_fitted),     jnp.linalg.cond(R_fitted) ]  bars = axes[1, 1].bar(matrices, condition_numbers, color=['skyblue', 'lightcoral', 'lightgreen', 'gold']) axes[1, 1].set_ylabel('Condition Number (log scale)') axes[1, 1].set_yscale('log') axes[1, 1].set_title('Matrix Condition Numbers') axes[1, 1].tick_params(axis='x', rotation=45)  # Add value labels for bar, value in zip(bars, condition_numbers):     axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,                    f'{value:.1e}', ha='center', va='bottom', rotation=45)  plt.tight_layout() plt.show()  print(\"\u2705 Numerical stability visualization created\") <pre>/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/matplotlib/transforms.py:762: ComplexWarning: Casting complex values to real discards the imaginary part\n  points = np.asarray(points, float)\n</pre> <pre>\u2705 Numerical stability visualization created\n</pre> In\u00a0[14]: Copied! <pre># Compute one-step ahead predictions for validation data\nprint(\" Computing one-step ahead predictions...\")\n\ndef compute_one_step_predictions(model, params, observations):\n    \"\"\"\n    Compute one-step ahead predictions using filtering\n    \"\"\"\n    T, N = observations.shape\n    predictions = jnp.zeros_like(observations)\n    \n    # Get filtered posterior\n    posterior = model.filter(params, observations)\n    filtered_means = posterior[0] # (T, D)\n    \n    # One-step predictions: y_{t+1|t} = C * x_{t+1|t} = C * A * x_{t|t}\n    A = params.dynamics.weights\n    C = params.emissions.weights\n    \n    for t in range(T-1):\n        # Predict next latent state: x_{t+1|t} = A * x_{t|t}\n        x_pred = A @ filtered_means[t]\n        # Predict next observation: y_{t+1|t} = C * x_{t+1|t}\n        predictions = predictions.at[t+1].set(C @ x_pred)\n    \n    return predictions\n\n# Compute predictions on test data\ntest_predictions = compute_one_step_predictions(ctds, params_fitted, test_obs)\n\n# Compute prediction errors\nprediction_errors = test_predictions[1:] - test_obs[1:]  # Skip first time point\nprediction_errors_flat = prediction_errors.flatten()\n\n# RMSE and NRMSE metrics\nrmse_per_neuron = jnp.sqrt(jnp.mean(prediction_errors**2, axis=0))\nrmse_overall = jnp.sqrt(jnp.mean(prediction_errors**2))\n\n# Normalized RMSE (by standard deviation of each channel)\ntest_obs_std = jnp.std(test_obs, axis=0)\nnrmse_per_neuron = rmse_per_neuron / test_obs_std\nnrmse_overall = jnp.mean(nrmse_per_neuron)\n\n# Mean Absolute Error\nmae_per_neuron = jnp.mean(jnp.abs(prediction_errors), axis=0)\nmae_overall = jnp.mean(mae_per_neuron)\n\nprint(f\"\u2705 One-step ahead prediction metrics:\")\nprint(f\"  Overall RMSE:     {rmse_overall:.4f}\")\nprint(f\"  Overall NRMSE:    {nrmse_overall:.4f}\")\nprint(f\"  Overall MAE:      {mae_overall:.4f}\")\nprint(f\"  RMSE range:       [{jnp.min(rmse_per_neuron):.4f}, {jnp.max(rmse_per_neuron):.4f}]\")\nprint(f\"  NRMSE range:      [{jnp.min(nrmse_per_neuron):.4f}, {jnp.max(nrmse_per_neuron):.4f}]\")\n\n# R\u00b2 for predictions\npred_r2_per_neuron = jnp.array([\n    compute_r_squared(test_obs[1:, i], test_predictions[1:, i])\n    for i in range(N)\n])\npred_r2_overall = jnp.mean(pred_r2_per_neuron)\n\nprint(f\"  Prediction R\u00b2:    {pred_r2_overall:.4f}\")\nprint(f\"  R\u00b2 range:         [{jnp.min(pred_r2_per_neuron):.4f}, {jnp.max(pred_r2_per_neuron):.4f}]\")\n</pre> # Compute one-step ahead predictions for validation data print(\" Computing one-step ahead predictions...\")  def compute_one_step_predictions(model, params, observations):     \"\"\"     Compute one-step ahead predictions using filtering     \"\"\"     T, N = observations.shape     predictions = jnp.zeros_like(observations)          # Get filtered posterior     posterior = model.filter(params, observations)     filtered_means = posterior[0] # (T, D)          # One-step predictions: y_{t+1|t} = C * x_{t+1|t} = C * A * x_{t|t}     A = params.dynamics.weights     C = params.emissions.weights          for t in range(T-1):         # Predict next latent state: x_{t+1|t} = A * x_{t|t}         x_pred = A @ filtered_means[t]         # Predict next observation: y_{t+1|t} = C * x_{t+1|t}         predictions = predictions.at[t+1].set(C @ x_pred)          return predictions  # Compute predictions on test data test_predictions = compute_one_step_predictions(ctds, params_fitted, test_obs)  # Compute prediction errors prediction_errors = test_predictions[1:] - test_obs[1:]  # Skip first time point prediction_errors_flat = prediction_errors.flatten()  # RMSE and NRMSE metrics rmse_per_neuron = jnp.sqrt(jnp.mean(prediction_errors**2, axis=0)) rmse_overall = jnp.sqrt(jnp.mean(prediction_errors**2))  # Normalized RMSE (by standard deviation of each channel) test_obs_std = jnp.std(test_obs, axis=0) nrmse_per_neuron = rmse_per_neuron / test_obs_std nrmse_overall = jnp.mean(nrmse_per_neuron)  # Mean Absolute Error mae_per_neuron = jnp.mean(jnp.abs(prediction_errors), axis=0) mae_overall = jnp.mean(mae_per_neuron)  print(f\"\u2705 One-step ahead prediction metrics:\") print(f\"  Overall RMSE:     {rmse_overall:.4f}\") print(f\"  Overall NRMSE:    {nrmse_overall:.4f}\") print(f\"  Overall MAE:      {mae_overall:.4f}\") print(f\"  RMSE range:       [{jnp.min(rmse_per_neuron):.4f}, {jnp.max(rmse_per_neuron):.4f}]\") print(f\"  NRMSE range:      [{jnp.min(nrmse_per_neuron):.4f}, {jnp.max(nrmse_per_neuron):.4f}]\")  # R\u00b2 for predictions pred_r2_per_neuron = jnp.array([     compute_r_squared(test_obs[1:, i], test_predictions[1:, i])     for i in range(N) ]) pred_r2_overall = jnp.mean(pred_r2_per_neuron)  print(f\"  Prediction R\u00b2:    {pred_r2_overall:.4f}\") print(f\"  R\u00b2 range:         [{jnp.min(pred_r2_per_neuron):.4f}, {jnp.max(pred_r2_per_neuron):.4f}]\") <pre> Computing one-step ahead predictions...\n\u2705 One-step ahead prediction metrics:\n  Overall RMSE:     0.0648\n  Overall NRMSE:    1.0048\n  Overall MAE:      0.0511\n  RMSE range:       [0.0494, 0.1039]\n  NRMSE range:      [0.9019, 1.0917]\n  Prediction R\u00b2:    -0.0110\n  R\u00b2 range:         [-0.1860, 0.1888]\n</pre> In\u00a0[15]: Copied! <pre># Visualize one-step ahead prediction performance\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Prediction errors histogram\naxes[0, 0].hist(prediction_errors_flat, bins=50, alpha=0.7, color='red', density=True)\naxes[0, 0].set_xlabel('Prediction Error')\naxes[0, 0].set_ylabel('Density')\naxes[0, 0].set_title('Distribution of One-Step Prediction Errors')\naxes[0, 0].axvline(0, color='black', linestyle='--', alpha=0.5)\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. RMSE per neuron\nneuron_indices = jnp.arange(N)\naxes[0, 1].bar(neuron_indices, rmse_per_neuron, alpha=0.7, color='orange')\naxes[0, 1].set_xlabel('Neuron Index')\naxes[0, 1].set_ylabel('RMSE')\naxes[0, 1].set_title('RMSE per Neuron (One-Step Ahead)')\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. NRMSE per neuron\nbars = axes[0, 2].bar(neuron_indices, nrmse_per_neuron, alpha=0.7, color='purple')\naxes[0, 2].set_xlabel('Neuron Index')\naxes[0, 2].set_ylabel('Normalized RMSE')\naxes[0, 2].set_title('NRMSE per Neuron (One-Step Ahead)')\naxes[0, 2].axhline(1.0, color='red', linestyle='--', alpha=0.7, label='NRMSE = 1')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# Color bars by performance (green = good, red = poor)\nfor i, bar in enumerate(bars):\n    if nrmse_per_neuron[i] &lt; 0.5:\n        bar.set_color('green')\n    elif nrmse_per_neuron[i] &lt; 1.0:\n        bar.set_color('orange')\n    else:\n        bar.set_color('red')\n\n\n# 4. Sample time traces of predictions vs observations\nsample_neurons = [0, N//4, N//2, 3*N//4] if N &gt;= 4 else [0, min(1, N-1)]\ntime_window = slice(10, min(90, T))  # Ensure we don't exceed data length\n\nfor i, neuron_idx in enumerate(sample_neurons[:2]):\n    if i &gt;= 2:\n        break\n    ax = axes[1, i]\n    \n    # Create time range that matches the actual data slice\n    data_slice = test_obs[time_window, neuron_idx]\n    pred_slice = test_predictions[time_window, neuron_idx]\n    t_range = jnp.arange(time_window.start, time_window.start + len(data_slice))\n    \n    ax.plot(t_range, data_slice, 'b-', alpha=0.7, \n            label='True', linewidth=2)\n    ax.plot(t_range, pred_slice, 'r--', alpha=0.8, \n            label='Predicted', linewidth=2)\n    \n    ax.set_xlabel('Time')\n    ax.set_ylabel('Activity')\n    ax.set_title(f'Predictions vs True (Neuron {neuron_idx})')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n\n# 5. Prediction R\u00b2 per neuron\nbars_r2 = axes[1, 2].bar(neuron_indices, pred_r2_per_neuron, alpha=0.7)\naxes[1, 2].set_xlabel('Neuron Index')\naxes[1, 2].set_ylabel('R\u00b2')\naxes[1, 2].set_title('Prediction R\u00b2 per Neuron')\naxes[1, 2].axhline(0.0, color='red', linestyle='--', alpha=0.7)\naxes[1, 2].grid(True, alpha=0.3)\n\n# Color R\u00b2 bars by performance\nfor i, bar in enumerate(bars_r2):\n    if pred_r2_per_neuron[i] &gt; 0.8:\n        bar.set_color('darkgreen')\n    elif pred_r2_per_neuron[i] &gt; 0.5:\n        bar.set_color('green')\n    elif pred_r2_per_neuron[i] &gt; 0.2:\n        bar.set_color('orange')\n    else:\n        bar.set_color('red')\n\nplt.suptitle('One-Step Ahead Prediction Analysis', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(f\"\\n\ud83d\udcca One-Step Ahead Prediction Summary:\")\nprint(f\"  Neurons with NRMSE &lt; 0.5: {jnp.sum(nrmse_per_neuron &lt; 0.5)}/{N} ({100*jnp.mean(nrmse_per_neuron &lt; 0.5):.1f}%)\")\nprint(f\"  Neurons with R\u00b2 &gt; 0.5:    {jnp.sum(pred_r2_per_neuron &gt; 0.5)}/{N} ({100*jnp.mean(pred_r2_per_neuron &gt; 0.5):.1f}%)\")\nprint(f\"  Neurons with R\u00b2 &gt; 0.8:    {jnp.sum(pred_r2_per_neuron &gt; 0.8)}/{N} ({100*jnp.mean(pred_r2_per_neuron &gt; 0.8):.1f}%)\")\n</pre> # Visualize one-step ahead prediction performance fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # 1. Prediction errors histogram axes[0, 0].hist(prediction_errors_flat, bins=50, alpha=0.7, color='red', density=True) axes[0, 0].set_xlabel('Prediction Error') axes[0, 0].set_ylabel('Density') axes[0, 0].set_title('Distribution of One-Step Prediction Errors') axes[0, 0].axvline(0, color='black', linestyle='--', alpha=0.5) axes[0, 0].grid(True, alpha=0.3)  # 2. RMSE per neuron neuron_indices = jnp.arange(N) axes[0, 1].bar(neuron_indices, rmse_per_neuron, alpha=0.7, color='orange') axes[0, 1].set_xlabel('Neuron Index') axes[0, 1].set_ylabel('RMSE') axes[0, 1].set_title('RMSE per Neuron (One-Step Ahead)') axes[0, 1].grid(True, alpha=0.3)  # 3. NRMSE per neuron bars = axes[0, 2].bar(neuron_indices, nrmse_per_neuron, alpha=0.7, color='purple') axes[0, 2].set_xlabel('Neuron Index') axes[0, 2].set_ylabel('Normalized RMSE') axes[0, 2].set_title('NRMSE per Neuron (One-Step Ahead)') axes[0, 2].axhline(1.0, color='red', linestyle='--', alpha=0.7, label='NRMSE = 1') axes[0, 2].legend() axes[0, 2].grid(True, alpha=0.3)  # Color bars by performance (green = good, red = poor) for i, bar in enumerate(bars):     if nrmse_per_neuron[i] &lt; 0.5:         bar.set_color('green')     elif nrmse_per_neuron[i] &lt; 1.0:         bar.set_color('orange')     else:         bar.set_color('red')   # 4. Sample time traces of predictions vs observations sample_neurons = [0, N//4, N//2, 3*N//4] if N &gt;= 4 else [0, min(1, N-1)] time_window = slice(10, min(90, T))  # Ensure we don't exceed data length  for i, neuron_idx in enumerate(sample_neurons[:2]):     if i &gt;= 2:         break     ax = axes[1, i]          # Create time range that matches the actual data slice     data_slice = test_obs[time_window, neuron_idx]     pred_slice = test_predictions[time_window, neuron_idx]     t_range = jnp.arange(time_window.start, time_window.start + len(data_slice))          ax.plot(t_range, data_slice, 'b-', alpha=0.7,              label='True', linewidth=2)     ax.plot(t_range, pred_slice, 'r--', alpha=0.8,              label='Predicted', linewidth=2)          ax.set_xlabel('Time')     ax.set_ylabel('Activity')     ax.set_title(f'Predictions vs True (Neuron {neuron_idx})')     ax.legend()     ax.grid(True, alpha=0.3)   # 5. Prediction R\u00b2 per neuron bars_r2 = axes[1, 2].bar(neuron_indices, pred_r2_per_neuron, alpha=0.7) axes[1, 2].set_xlabel('Neuron Index') axes[1, 2].set_ylabel('R\u00b2') axes[1, 2].set_title('Prediction R\u00b2 per Neuron') axes[1, 2].axhline(0.0, color='red', linestyle='--', alpha=0.7) axes[1, 2].grid(True, alpha=0.3)  # Color R\u00b2 bars by performance for i, bar in enumerate(bars_r2):     if pred_r2_per_neuron[i] &gt; 0.8:         bar.set_color('darkgreen')     elif pred_r2_per_neuron[i] &gt; 0.5:         bar.set_color('green')     elif pred_r2_per_neuron[i] &gt; 0.2:         bar.set_color('orange')     else:         bar.set_color('red')  plt.suptitle('One-Step Ahead Prediction Analysis', fontsize=16, fontweight='bold') plt.tight_layout() plt.show()  # Summary statistics print(f\"\\n\ud83d\udcca One-Step Ahead Prediction Summary:\") print(f\"  Neurons with NRMSE &lt; 0.5: {jnp.sum(nrmse_per_neuron &lt; 0.5)}/{N} ({100*jnp.mean(nrmse_per_neuron &lt; 0.5):.1f}%)\") print(f\"  Neurons with R\u00b2 &gt; 0.5:    {jnp.sum(pred_r2_per_neuron &gt; 0.5)}/{N} ({100*jnp.mean(pred_r2_per_neuron &gt; 0.5):.1f}%)\") print(f\"  Neurons with R\u00b2 &gt; 0.8:    {jnp.sum(pred_r2_per_neuron &gt; 0.8)}/{N} ({100*jnp.mean(pred_r2_per_neuron &gt; 0.8):.1f}%)\") <pre>\n\ud83d\udcca One-Step Ahead Prediction Summary:\n  Neurons with NRMSE &lt; 0.5: 0/20 (0.0%)\n  Neurons with R\u00b2 &gt; 0.5:    0/20 (0.0%)\n  Neurons with R\u00b2 &gt; 0.8:    0/20 (0.0%)\n</pre>"},{"location":"examples/ctds_model_validation/#ctds-single-region","title":"CTDS Single-Region\u00b6","text":""},{"location":"examples/ctds_model_validation/#what-this-notebook-does","title":"What this notebook does\u00b6","text":"<ol> <li>Data Generation - Create synthetic neural data with cell-type constraints</li> <li>Model Fitting - Initialize and fit CTDS using EM algorithm</li> <li>Accuracy Metrics - Compute parameter recovery errors and R\u00b2 scores</li> <li>Visualization Plots - EM convergence, heatmaps, trajectory comparisons</li> <li>Numerical Stability - Eigenvalue analysis and matrix conditioning</li> </ol>"},{"location":"examples/ctds_model_validation/#current-findings","title":"Current findings\u00b6","text":"<ul> <li>EM converges reliably on well-conditioned synthetic data (condition number &lt; ~20).</li> <li>One-step predictions are essentially flat vs baseline:<ul> <li>NRMSE \u2248 1.0 (no gain over mean predictor)</li> <li>R\u00b2 \u2248 0 or negative across neurons.</li> </ul> </li> <li>Suggests that either:<ul> <li>Solver instabilities (jaxopt CDQP in M-step), or</li> <li>High observation noise / poor conditioning are preventing dynamics from being used effectively.</li> </ul> </li> </ul>"},{"location":"examples/ctds_model_validation/#whats-not-implemented-yet","title":"What\u2019s not implemented yet\u00b6","text":"<ul> <li>\u274c Alignment of recovered parameters (blockwise permutation + scaling).</li> <li>\u274c Solver ablations (LS vs projected LS vs CDQP).</li> <li>\u274c Additional validation metrics (predictive log-likelihood, multi-step forecasts, whitened innovations, imputation).</li> <li>\u274c CPU JIT (GPU JIT works; CPU blocked by XLA layout).</li> </ul>"},{"location":"examples/ctds_model_validation/#1-define-ground-truth-model-parameters","title":"1. Define Ground Truth Model Parameters\u00b6","text":"<p>We'll create a small, stable dynamical system with Dale's law constraints:</p> <ul> <li>State dimension: D = 6 (3 excitatory + 3 inhibitory dimensions)</li> <li>Observation dimension: N = 20 neurons</li> <li>Cell types: 2 types (excitatory and inhibitory)</li> <li>Time steps: T = 200</li> </ul>"},{"location":"examples/ctds_model_validation/#3-initialize-and-fit-ctds-model","title":"3. Initialize and Fit CTDS Model\u00b6","text":"<p>Initialize the model from observations and fit using EM algorithm.</p>"},{"location":"examples/ctds_model_validation/#4-compute-accuracy-metrics","title":"4. Compute Accuracy Metrics\u00b6","text":"<p>Calculate various error metrics to assess model performance.</p>"},{"location":"examples/ctds_model_validation/#7-numerical-stability-checks","title":"7. Numerical Stability Checks\u00b6","text":"<p>Verify that the fitted model has good numerical properties.</p>"},{"location":"examples/ctds_model_validation/#8-validation-metrics-and-forecasting-performance","title":"8. Validation Metrics and Forecasting Performance\u00b6","text":"<p>We evaluate the fitted model's predictive capabilities using held-out data through:</p> <ol> <li>One-step ahead predictions - How well can the model predict the next time step?</li> <li>Multi-step forecasting - How does prediction accuracy decay over longer horizons?</li> <li>Comparison with baselines - Performance relative to simple forecasting methods</li> </ol>"},{"location":"examples/ctds_model_validation/#81-one-step-ahead-prediction-analysis","title":"8.1 One-Step Ahead Prediction Analysis\u00b6","text":""},{"location":"examples/ctds_model_validation/","title":"CTDS Model Validation","text":"<p>For detailed validation procedures and examples, see the associated Jupyter notebook:</p> <p>View Notebook</p> <p>This notebook covers: - Cross-validation procedures - Model selection criteria - Performance metrics - Diagnostic plots and visualizations</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>CTDS requires Python 3.9 or later and is built on JAX for high-performance computing.</p>"},{"location":"getting-started/installation/#install-from-pypi-when-released","title":"Install from PyPI (when released)","text":"<pre><code>pip install ctds\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":""},{"location":"getting-started/installation/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/kasherri/CTDS.git\ncd CTDS\n</code></pre>"},{"location":"getting-started/installation/#create-environment","title":"Create Environment","text":"<p>We recommend using conda or virtualenv:</p> <pre><code># Using conda\nconda create -n ctds python=3.10\nconda activate ctds\n\n# Or using virtualenv\npython -m venv ctds-env\nsource ctds-env/bin/activate  # On Windows: ctds-env\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/installation/#install-dependencies","title":"Install Dependencies","text":"<p>For basic usage:</p> <pre><code>pip install -e .\n</code></pre> <p>For development (includes testing and documentation tools):</p> <pre><code>pip install -e \".[dev,docs]\"\n</code></pre> <p>For examples (includes matplotlib, jupyter):</p> <pre><code>pip install -e \".[examples]\"\n</code></pre> <p>For everything:</p> <pre><code>pip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test that the installation works:</p> <pre><code>import ctds\nprint(f\"CTDS version: {ctds.__version__}\")\n\n# Create a simple model\nmodel = ctds.CTDS(state_dim=5, emission_dim=10)\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":"<p>For GPU acceleration, install JAX with CUDA support:</p> <pre><code># For CUDA 12\npip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n# For CUDA 11\npip install -U \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre> <p>Verify GPU is available:</p> <pre><code>import jax\nprint(f\"JAX devices: {jax.devices()}\")\nprint(f\"GPU available: {len(jax.devices('gpu')) &gt; 0}\")\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>ImportError: No module named 'dynamax' <pre><code>pip install dynamax\n</code></pre></p> <p>JAX installation issues - See the JAX installation guide - For M1/M2 Macs, ensure you're using the correct JAX build</p> <p>Optimization solver issues <pre><code>pip install --upgrade jaxopt\n</code></pre></p>"},{"location":"getting-started/installation/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use JAX's JIT compilation for production code</li> <li>Enable 64-bit precision if needed: <code>jax.config.update(\"jax_enable_x64\", True)</code></li> <li>For large models, consider using GPU acceleration</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>CTDS</li> </ul>"},{"location":"reference/ctds/","title":"Ctds","text":""},{"location":"reference/ctds/#ctds","title":"ctds","text":"<p>Cell-Type Dynamical Systems (CTDS).</p> <p>A JAX-native implementation for modeling neural population dynamics with biologically-constrained connectivity and Dale's law.</p>"},{"location":"reference/ctds/#ctds-classes","title":"Classes","text":""},{"location":"reference/ctds/#ctds.CTDS","title":"CTDS","text":"<pre><code>CTDS(\n    emission_dim,\n    cell_types,\n    cell_sign,\n    cell_type_dimensions,\n    cell_type_mask,\n    region_identity=None,\n    inputs_dim=None,\n    state_dim=None,\n)\n</code></pre> <p>               Bases: <code>SSM</code></p> <p>Cell-Type Dynamical System with Dale's law constraints.</p> <p>A linear state-space model for neural population dynamics that enforces biologically plausible sign constraints based on cell type identity.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Number of observed neurons (N).</p> required <code>(Array, shape(K))</code> <p>Cell type labels as contiguous integers [0, 1, ..., K-1].</p> required <code>(Array, shape(K))</code> <p>Sign constraints: +1 for excitatory, -1 for inhibitory cell types.</p> required <code>(Array, shape(K))</code> <p>Latent dimensions allocated to each cell type.</p> required <code>(Array, shape(N))</code> <p>Cell type assignment for each observed neuron.</p> required <code>(Array, shape(N))</code> <p>Brain region identity for each neuron (for future multi-region support).</p> <code>None</code> <code>int</code> <p>Dimension of exogenous inputs.</p> <code>None</code> <code>int</code> <p>Total latent state dimension (computed from cell_type_dimensions if None).</p> <code>None</code> Notes <p>The CTDS model implements the linear state-space equations:</p> <p>State evolution: \\(\\(x_{t+1} = A x_t + B u_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, Q)\\)\\)</p> <p>Observations: \\(\\(y_t = C x_t + D u_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, R)\\)\\)</p> <p>where Dale's law constraints enforce that: - Excitatory neurons (cell_sign[k] = +1) have non-negative connection weights - Inhibitory neurons (cell_sign[k] = -1) have non-positive connection weights</p> <p>The model supports block-structured connectivity patterns reflecting cell-type-specific dynamics and connectivity.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If cell_types is not a contiguous range [0, 1, ..., K-1].</p> <code>ValueError</code> <p>If cell_type_mask contains invalid cell type indices.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create CTDS model with 2 cell types\n&gt;&gt;&gt; cell_types = jnp.array([0, 1])\n&gt;&gt;&gt; cell_sign = jnp.array([1, -1])  # excitatory, inhibitory\n&gt;&gt;&gt; cell_type_dimensions = jnp.array([3, 2])  # 3 + 2 = 5 total latent dims\n&gt;&gt;&gt; cell_type_mask = jnp.array([0, 0, 0, 1, 1])  # 3 exc + 2 inh neurons\n&gt;&gt;&gt; \n&gt;&gt;&gt; ctds = CTDS(\n...     emission_dim=5,\n...     cell_types=cell_types,\n...     cell_sign=cell_sign, \n...     cell_type_dimensions=cell_type_dimensions,\n...     cell_type_mask=cell_type_mask\n... )\n</code></pre>"},{"location":"reference/ctds/#ctds.CTDS(emission_dim)","title":"<code>emission_dim</code>","text":""},{"location":"reference/ctds/#ctds.CTDS(cell_types)","title":"<code>cell_types</code>","text":""},{"location":"reference/ctds/#ctds.CTDS(cell_sign)","title":"<code>cell_sign</code>","text":""},{"location":"reference/ctds/#ctds.CTDS(cell_type_dimensions)","title":"<code>cell_type_dimensions</code>","text":""},{"location":"reference/ctds/#ctds.CTDS(cell_type_mask)","title":"<code>cell_type_mask</code>","text":""},{"location":"reference/ctds/#ctds.CTDS(region_identity)","title":"<code>region_identity</code>","text":""},{"location":"reference/ctds/#ctds.CTDS(inputs_dim)","title":"<code>inputs_dim</code>","text":""},{"location":"reference/ctds/#ctds.CTDS(state_dim)","title":"<code>state_dim</code>","text":""},{"location":"reference/ctds/#ctds.CTDS-attributes","title":"Attributes","text":""},{"location":"reference/ctds/#ctds.CTDS.emission_shape","title":"emission_shape  <code>property</code>","text":"<pre><code>emission_shape\n</code></pre> <p>Shape of emission observations.</p>"},{"location":"reference/ctds/#ctds.CTDS.inputs_shape","title":"inputs_shape  <code>property</code>","text":"<pre><code>inputs_shape\n</code></pre> <p>Shape of exogenous inputs, if any.</p>"},{"location":"reference/ctds/#ctds.CTDS-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.CTDS.initial_distribution","title":"initial_distribution","text":"<pre><code>initial_distribution(params, inputs)\n</code></pre> <p>p(z1) = N(mu0, Sigma0).</p>"},{"location":"reference/ctds/#ctds.CTDS.transition_distribution","title":"transition_distribution","text":"<pre><code>transition_distribution(params, state, inputs)\n</code></pre> <p>p(z_{t+1} | z_t) = N(A z_t + B u_t, Q). If you do not use inputs, set B u_t = 0.</p>"},{"location":"reference/ctds/#ctds.CTDS.emission_distribution","title":"emission_distribution","text":"<pre><code>emission_distribution(params, state, inputs=None)\n</code></pre> <p>p(y_t | z_t) = N(C z_t, R).</p>"},{"location":"reference/ctds/#ctds.CTDS.initialize_dynamics","title":"initialize_dynamics","text":"<pre><code>initialize_dynamics(V_list, U)\n</code></pre> <p>Initialize dynamics parameters from block factorization results.</p> <p>Parameters:</p> Name Type Description Default <code>list of Arrays</code> <p>List of (N, D_k) factor matrices for each cell type k.</p> required <code>(Array, shape(N, D))</code> <p>Concatenated emission weight matrix.</p> required <p>Returns:</p> Type Description <code>ParamsCTDSDynamics</code> <p>Initialized dynamics parameters with Dale's law sign corrections.</p> Notes <p>Constructs the dynamics matrix A as A = V_dale^T @ U where V_dale applies sign corrections: positive for excitatory cell types, negative for inhibitory cell types to respect Dale's law.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If V_list length doesn't match number of cell types or if dimensions are inconsistent across matrices.</p>"},{"location":"reference/ctds/#ctds.CTDS.initialize_dynamics(V_list)","title":"<code>V_list</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.initialize_dynamics(U)","title":"<code>U</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.initialize_emissions","title":"initialize_emissions","text":"<pre><code>initialize_emissions(Y, U_list, state_dim)\n</code></pre> <p>Initialize emission parameters from block factorization results.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(N, T))</code> <p>Observed emission sequence.</p> required <code>list of Arrays</code> <p>List of (N_k, D_k) emission factor matrices for each cell type k.</p> required <code>int</code> <p>Total latent state dimension D.</p> required <p>Returns:</p> Type Description <code>ParamsCTDSEmissions</code> <p>Initialized emission parameters with block-diagonal structure.</p> Notes <p>Constructs a block-diagonal emission matrix C by concatenating cell-type-specific blocks with appropriate zero padding. The  observation noise covariance R is initialized as diagonal with empirical variances from the data.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the sum of latent dimensions doesn't match state_dim.</p>"},{"location":"reference/ctds/#ctds.CTDS.initialize_emissions(Y)","title":"<code>Y</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.initialize_emissions(U_list)","title":"<code>U_list</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.initialize_emissions(state_dim)","title":"<code>state_dim</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.initialize","title":"initialize","text":"<pre><code>initialize(observations)\n</code></pre> <p>Initialize CTDS parameters from observed emission data.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(N, T))</code> <p>Observed emission sequence with N neurons over T timesteps.</p> required <p>Returns:</p> Type Description <code>ParamsCTDS</code> <p>Initialized model parameters.</p> Notes <p>Initialization proceeds via: 1. Estimate linear recurrent connectivity matrix J using constrained regression 2. Apply blockwise NMF to J respecting cell-type structure 3. Build block-diagonal emission and dynamics matrices 4. Set small initial state variance and process noise</p> <p>The method respects Dale's law constraints throughout the initialization.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If observations shape doesn't match expected emission_dim.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; observations = jnp.ones((10, 100))  # 10 neurons, 100 timesteps\n&gt;&gt;&gt; params = ctds.initialize(observations)\n</code></pre>"},{"location":"reference/ctds/#ctds.CTDS.initialize(observations)","title":"<code>observations</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.e_step","title":"e_step","text":"<pre><code>e_step(params, emissions, inputs=None)\n</code></pre> <p>Expectation step: compute sufficient statistics via RTS smoothing.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Current model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>SufficientStats</code> <p>Sufficient statistics for M-step parameter updates: - latent_mean: E[x_t], shape (T, D) - latent_second_moment: E[x_t x_t^T], shape (T, D, D) - cross_time_moment: E[x_{t+1} x_t^T], shape (T-1, D, D) - loglik: marginal log-likelihood - T: number of timesteps</p> Notes <p>Uses the RTS (Rauch-Tung-Striebel) smoother to compute posterior moments of the latent states given all observations. These moments are sufficient for updating model parameters in the M-step.</p>"},{"location":"reference/ctds/#ctds.CTDS.e_step(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.e_step(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.e_step(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.initialize_m_step_state","title":"initialize_m_step_state","text":"<pre><code>initialize_m_step_state(params, props=None)\n</code></pre> <p>Initialize M-step state tracking for convergence diagnostics.</p>"},{"location":"reference/ctds/#ctds.CTDS.m_step","title":"m_step","text":"<pre><code>m_step(params, batch_stats, m_step_state, props=None)\n</code></pre> <p>Maximization step: update parameters using sufficient statistics.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Current model parameters.</p> required <code>ParamsCTDS</code> <p>Unused parameter for compatibility with base class.</p> <code>None</code> <code>SufficientStats</code> <p>Sufficient statistics averaged across batch sequences.</p> required <code>M_Step_State</code> <p>Convergence tracking state.</p> required <p>Returns:</p> Name Type Description <code>params_updated</code> <code>ParamsCTDS</code> <p>Updated model parameters after M-step.</p> <code>m_step_state_updated</code> <code>M_Step_State</code> <p>Updated convergence tracking state.</p> Notes <p>The M-step updates parameters by solving constrained optimization problems:</p> <p>Dynamics matrix A: Solved via constrained QP respecting Dale's law: \\(\\(\\min_A \\|A\\|_F^2 \\text{ s.t. } A_{ij} \\geq 0 \\text{ if neuron } j \\text{ is excitatory}\\)\\)</p> <p>Process noise Q: Updated via residual covariance after A update.</p> <p>Emission matrix C: Block-wise non-negative least squares (NNLS)  maintaining cell-type structure and Dale's law sign constraints.</p> <p>Observation noise R: Diagonal residual variance estimation.</p> <p>Initial state: Set to first timestep posterior moments.</p> <p>A gauge-fixing step normalizes column scales to improve numerical stability.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If batch_stats arrays don't have proper batch dimensions.</p>"},{"location":"reference/ctds/#ctds.CTDS.m_step(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.m_step(props)","title":"<code>props</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.m_step(batch_stats)","title":"<code>batch_stats</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.m_step(m_step_state)","title":"<code>m_step_state</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.fit_em","title":"fit_em","text":"<pre><code>fit_em(\n    params,\n    batch_emissions,\n    batch_inputs=None,\n    num_iters=20,\n    verbose=True,\n)\n</code></pre> <p>Expectation\u2013Maximization training loop for the Cell-Type Dynamical System.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Initial model parameters.</p> required <code>(Array, shape(B, T, N))</code> <p>Observation sequence(s). Single sequence (T, N) or batch (B, T, N).</p> required <code>(Array, shape(T, U) or (B, T, U))</code> <p>Exogenous input sequence(s) aligned to emissions.</p> <code>None</code> <code>int</code> <p>Maximum number of EM iterations.</p> <code>100</code> <code>bool</code> <p>Whether to display progress and convergence information.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>params_fitted</code> <code>ParamsCTDS</code> <p>Updated parameters after EM convergence.</p> <code>log_probs</code> <code>(Array, shape(num_iters_run))</code> <p>Marginal log-likelihood trace across iterations.</p> Notes <p>The E-step computes smoothed moments \\(E[x_t]\\), \\(E[x_t x_t^T]\\), \\(E[x_{t+1}x_t^T]\\) using the RTS smoother. The M-step updates \\((A,B)\\) and \\((C,D)\\) via  sign-constrained least squares; \\((Q,R)\\) from residual covariances.</p> <p>Convergence is detected when the relative change in log-likelihood falls below 1e-6.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input shapes are inconsistent or contain NaNs/Infs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; params_fitted, lls = ctds.fit_em(\n...     params0, emissions, num_iters=50, verbose=True\n... )\n&gt;&gt;&gt; print(f\"Final log-likelihood: {lls[-1]:.2f}\")\n</code></pre>"},{"location":"reference/ctds/#ctds.CTDS.fit_em(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.fit_em(batch_emissions)","title":"<code>batch_emissions</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.fit_em(batch_inputs)","title":"<code>batch_inputs</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.fit_em(num_iters)","title":"<code>num_iters</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.fit_em(verbose)","title":"<code>verbose</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.sample","title":"sample","text":"<pre><code>sample(params, key, num_timesteps, inputs=None)\n</code></pre> <p>Generate samples from the CTDS model.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Model parameters for sampling.</p> required <code>PRNGKey</code> <p>Random key for stochastic operations.</p> required <code>int</code> <p>Number of timesteps to sample.</p> required <code>(Array, shape(T, U))</code> <p>Optional exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>latent_states</code> <code>(Array, shape(T, D))</code> <p>Sampled latent state trajectories.</p> <code>emissions</code> <code>(Array, shape(T, N))</code> <p>Sampled emission observations.</p> Notes <p>Generates a forward sample by simulating the state-space model:</p> \\[x_{t+1} = A x_t + B u_t + \\varepsilon_t$$ $$y_t = C x_t + D u_t + \\eta_t\\] <p>where noise terms are drawn from their respective Gaussian distributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; key = jax.random.PRNGKey(0)\n&gt;&gt;&gt; states, obs = ctds.sample(params, key, num_timesteps=100)\n&gt;&gt;&gt; print(f\"States shape: {states.shape}, Obs shape: {obs.shape}\")\n</code></pre>"},{"location":"reference/ctds/#ctds.CTDS.sample(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.sample(key)","title":"<code>key</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.sample(num_timesteps)","title":"<code>num_timesteps</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.sample(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.forecast","title":"forecast","text":"<pre><code>forecast(\n    params, emissions, num_steps, inputs=None, key=None\n)\n</code></pre> <p>Forecast future emissions given observed history.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Fitted model parameters.</p> required <code>(Array, shape(T_obs, N))</code> <p>Observed emission history for conditioning.</p> required <code>int</code> <p>Number of future timesteps to forecast.</p> required <code>(Array, shape(T_obs + num_steps, U))</code> <p>Exogenous inputs for observed + forecast periods.</p> <code>None</code> <code>PRNGKey</code> <p>Random key for stochastic forecasts. If None, returns mean predictions.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>forecasts</code> <code>(Array, shape(num_steps, N))</code> <p>Predicted future emissions.</p> Notes <p>Computes forecast by: 1. Running the smoother on observed data to get final posterior state 2. Forward-simulating the dynamics for <code>num_steps</code> timesteps 3. Generating emission predictions via the observation model</p> <p>If <code>key</code> is provided, includes process and observation noise. Otherwise returns the deterministic mean trajectory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Deterministic forecast\n&gt;&gt;&gt; y_pred = ctds.forecast(params, emissions[-50:], num_steps=20)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Stochastic forecast with uncertainty\n&gt;&gt;&gt; key = jax.random.PRNGKey(42)\n&gt;&gt;&gt; y_pred = ctds.forecast(params, emissions[-50:], num_steps=20, key=key)\n</code></pre>"},{"location":"reference/ctds/#ctds.CTDS.forecast(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.forecast(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.forecast(num_steps)","title":"<code>num_steps</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.forecast(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.forecast(key)","title":"<code>key</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.log_prob","title":"log_prob","text":"<pre><code>log_prob(params, states, emissions, inputs=None)\n</code></pre> <p>Compute log-probability of state-emission trajectory.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>(Array, shape(T, D))</code> <p>Latent state trajectory.</p> required <code>(Array, shape(T, N))</code> <p>Observation sequence.</p> required <code>(Array, optional, shape(T, M))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>log_prob</code> <code>float</code> <p>Joint log-probability of states and emissions.</p>"},{"location":"reference/ctds/#ctds.CTDS.log_prob(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.log_prob(states)","title":"<code>states</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.log_prob(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.log_prob(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.filter","title":"filter","text":"<pre><code>filter(params, emissions, inputs=None)\n</code></pre> <p>Run forward Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observation sequence.</p> required <code>(Array, optional, shape(T, M))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>FilterResults</code> <p>Filtered means, covariances, and log-likelihood.</p>"},{"location":"reference/ctds/#ctds.CTDS.filter(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.filter(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.filter(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.smoother","title":"smoother  <code>staticmethod</code>","text":"<pre><code>smoother(params, emissions, inputs=None)\n</code></pre> <p>Run RTS smoother for posterior inference.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observation sequence.</p> required <code>(Array, optional, shape(T, M))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>SmootherResults</code> <p>Smoothed means, covariances, and log-likelihood.</p>"},{"location":"reference/ctds/#ctds.CTDS.smoother(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.smoother(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.smoother(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.marginal_log_prob","title":"marginal_log_prob","text":"<pre><code>marginal_log_prob(params, emissions, inputs=None)\n</code></pre> <p>Compute marginal log-likelihood of emissions.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observation sequence.</p> required <code>(Array, optional, shape(T, M))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>log_prob</code> <code>float</code> <p>Marginal log-likelihood p(y_{1:T}).</p>"},{"location":"reference/ctds/#ctds.CTDS.marginal_log_prob(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.marginal_log_prob(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.CTDS.marginal_log_prob(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS","title":"BaseCTDS","text":"<pre><code>BaseCTDS(\n    backend, dynamics_fn, emissions_fn, constraints=None\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for Cell-Type Dynamical Systems.</p> <p>This class defines the interface for CTDS models with different inference backends and constraint handling approaches.</p> <p>Parameters:</p> Name Type Description Default <code>InferenceBackend</code> <p>Inference backend (e.g., DynamaxLGSSMBackend).</p> required <code>Callable</code> <p>Function computing transition dynamics (may include nonlinearities).</p> required <code>Callable</code> <p>Function computing observation model.</p> required <code>ParamsCTDSConstraints</code> <p>Cell-type and Dale's law constraints.</p> <code>None</code>"},{"location":"reference/ctds/#ctds.BaseCTDS(backend)","title":"<code>backend</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS(dynamics_fn)","title":"<code>dynamics_fn</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS(emissions_fn)","title":"<code>emissions_fn</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS(constraints)","title":"<code>constraints</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.BaseCTDS.initialize","title":"initialize  <code>abstractmethod</code>","text":"<pre><code>initialize(Y, mask, **kwargs)\n</code></pre> <p>Initialize model parameters from observed data.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(N, T))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(N))</code> <p>Boolean mask for valid observations.</p> required <p>Additional initialization arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ParamsCTDS</code> <p>Initialized model parameters.</p>"},{"location":"reference/ctds/#ctds.BaseCTDS.initialize(Y)","title":"<code>Y</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS.initialize(mask)","title":"<code>mask</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS.initialize(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS.infer","title":"infer","text":"<pre><code>infer(params, emissions, inputs=None)\n</code></pre> <p>Run inference using the attached backend.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>Posterior statistics from the inference backend.</code>"},{"location":"reference/ctds/#ctds.BaseCTDS.infer(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS.infer(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS.infer(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS.m_step","title":"m_step  <code>abstractmethod</code>","text":"<pre><code>m_step(params, stats)\n</code></pre> <p>Update model parameters given sufficient statistics.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Current model parameters.</p> required <code>SufficientStats</code> <p>Sufficient statistics from E-step.</p> required <p>Returns:</p> Type Description <code>ParamsCTDS</code> <p>Updated model parameters.</p>"},{"location":"reference/ctds/#ctds.BaseCTDS.m_step(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.BaseCTDS.m_step(stats)","title":"<code>stats</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDS","title":"ParamsCTDS","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Complete CTDS model parameters with biological constraints.</p> <p>This structure encapsulates all parameters for a Cell-Type Dynamical System, including initial conditions, dynamics, emissions, and cell-type constraints.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDSInitial</code> <p>Initial state distribution parameters.</p> required <code>ParamsCTDSDynamics</code> <p>State transition and process noise parameters.</p> required <code>ParamsCTDSEmissions</code> <p>Observation model parameters.</p> required <code>ParamsCTDSConstraints</code> <p>Dale's law and cell-type structure constraints.</p> required <code>(Array, shape(N, T))</code> <p>Observed emission data (for reference during fitting).</p> required Notes <p>The full CTDS model equations are:</p> <p>State evolution: \\(\\(x_{t+1} = A x_t + B u_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, Q)\\)\\)</p> <p>Emissions: \\(\\(y_t = C x_t + D u_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, R)\\)\\)</p> <p>where A, C respect Dale's law sign constraints based on cell types.</p> See Also <p>to_lgssm : Convert to Dynamax-compatible format for inference.</p>"},{"location":"reference/ctds/#ctds.ParamsCTDS(initial)","title":"<code>initial</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDS(dynamics)","title":"<code>dynamics</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDS(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDS(constraints)","title":"<code>constraints</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDS(observations)","title":"<code>observations</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDS-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.ParamsCTDS.to_lgssm","title":"to_lgssm","text":"<pre><code>to_lgssm()\n</code></pre> <p>Convert CTDS parameters to Dynamax LGSSM format for inference.</p> <p>Returns:</p> Type Description <code>ParamsLGSSM</code> <p>Dynamax-compatible parameter structure.</p> Notes <p>This conversion enables use of Dynamax's optimized inference routines while preserving CTDS-specific structure. Biases are set to zero since CTDS uses centered parameterization.</p>"},{"location":"reference/ctds/#ctds.ParamsCTDSDynamics","title":"ParamsCTDSDynamics","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Dynamics parameters for the CTDS latent state evolution.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D, D))</code> <p>State transition matrix A in x_{t+1} = A x_t + B u_t + \u03b5_t.</p> required <code>(Array, shape(D, D) or (D,))</code> <p>Process noise covariance Q (full matrix or diagonal variances).</p> required <code>(Array, shape(D, U) or (T, D, U))</code> <p>Input weight matrix B (static or time-varying).</p> required <code>(Array, shape(D))</code> <p>Cell-type assignment mask for each latent dimension: 1 for excitatory, -1 for inhibitory, 0 for unassigned.</p> required Notes <p>The dynamics follow the linear state-space evolution: \\(\\(x_{t+1} = A x_t + B u_t + \\varepsilon_t, \\quad  \\varepsilon_t \\sim \\mathcal{N}(0, Q)\\)\\) where Dale's law constraints apply to enforce biologically plausible sign patterns in the transition matrix A.</p>"},{"location":"reference/ctds/#ctds.ParamsCTDSDynamics(weights)","title":"<code>weights</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSDynamics(cov)","title":"<code>cov</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSDynamics(input_weights)","title":"<code>input_weights</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSDynamics(dynamics_mask)","title":"<code>dynamics_mask</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSEmissions","title":"ParamsCTDSEmissions","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Emission model parameters mapping latent states to observations.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(N, D))</code> <p>Emission weight matrix C in y_t = C x_t + D u_t + \u03b7_t. Each row corresponds to one observed neuron.</p> required <code>(Array, shape(N, N) or (N,))</code> <p>Observation noise covariance R (full matrix or diagonal variances).</p> required <code>(Array, shape(K, 2))</code> <p>Start and end column indices for each cell type's emission block.</p> required <code>(Array, shape(K, 2), optional)</code> <p>Dimensions of left zero-padding for each cell type block.</p> required <code>(Array, shape(K, 2))</code> <p>Dimensions of right zero-padding for each cell type block.</p> required Notes <p>The emission model follows: \\(\\(y_t = C x_t + D u_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, R)\\)\\) Dale's law sign constraints apply row-wise: excitatory neurons have non-negative weights, inhibitory neurons have non-positive weights. The block structure allows modeling cell-type-specific connectivity patterns.</p>"},{"location":"reference/ctds/#ctds.ParamsCTDSEmissions(weights)","title":"<code>weights</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSEmissions(cov)","title":"<code>cov</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSEmissions(emission_dims)","title":"<code>emission_dims</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSEmissions(left_padding_dims)","title":"<code>left_padding_dims</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSEmissions(right_padding_dims)","title":"<code>right_padding_dims</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSInitial","title":"ParamsCTDSInitial","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Initial state distribution parameters for CTDS: p(x_1) = N(x_1 | mean, cov).</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D))</code> <p>Initial state mean vector.</p> required <code>(Array, shape(D, D) or (D,))</code> <p>Initial state covariance matrix (full) or diagonal variances.</p> required Notes <p>For the CTDS model, the initial state follows a multivariate Gaussian: \\(\\(x_1 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\)\\) where D is the total latent state dimension across all cell types.</p>"},{"location":"reference/ctds/#ctds.ParamsCTDSInitial(mean)","title":"<code>mean</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSInitial(cov)","title":"<code>cov</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSConstraints","title":"ParamsCTDSConstraints","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Biological constraints for enforcing Dale's law and cell-type structure.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(K))</code> <p>Cell type labels as contiguous integers [0, 1, ..., K-1].</p> required <code>(Array, shape(K))</code> <p>Sign for each cell type: +1 (excitatory), -1 (inhibitory).</p> required <code>(Array, shape(K))</code> <p>Number of latent dimensions allocated to each cell type.</p> required <code>(Array, shape(N))</code> <p>Cell type assignment for each observed neuron.</p> required Notes <p>These constraints enforce Dale's law: excitatory neurons have non-negative synaptic weights, inhibitory neurons have non-positive weights. The cell_type_mask maps each observed neuron to its cell type, enabling block-structured computations and biologically informed regularization.</p> <p>The constraint cell_types[i] corresponds to cell_sign[i] and  cell_type_dimensions[i]. For neuron j, cell_type_mask[j] gives its type index.</p>"},{"location":"reference/ctds/#ctds.ParamsCTDSConstraints(cell_types)","title":"<code>cell_types</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSConstraints(cell_sign)","title":"<code>cell_sign</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSConstraints(cell_type_dimensions)","title":"<code>cell_type_dimensions</code>","text":""},{"location":"reference/ctds/#ctds.ParamsCTDSConstraints(cell_type_mask)","title":"<code>cell_type_mask</code>","text":""},{"location":"reference/ctds/#ctds.SufficientStats","title":"SufficientStats","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Sufficient statistics for CTDS parameter updates in the M-step.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(T, D))</code> <p>Posterior mean of latent states E[x_t].</p> required <code>(Array, shape(T, D, D))</code> <p>Posterior second moment E[x_t x_t^T].</p> required <code>(Array, shape(T - 1, D, D))</code> <p>Cross-time moments E[x_{t+1} x_t^T] for dynamics updates.</p> required <code>float</code> <p>Marginal log-likelihood of observed data.</p> required <code>int</code> <p>Number of time steps.</p> required Notes <p>These statistics are computed in the E-step and used in the M-step to update model parameters via closed-form solutions or constrained optimization routines.</p>"},{"location":"reference/ctds/#ctds.SufficientStats(latent_mean)","title":"<code>latent_mean</code>","text":""},{"location":"reference/ctds/#ctds.SufficientStats(latent_second_moment)","title":"<code>latent_second_moment</code>","text":""},{"location":"reference/ctds/#ctds.SufficientStats(cross_time_moment)","title":"<code>cross_time_moment</code>","text":""},{"location":"reference/ctds/#ctds.SufficientStats(loglik)","title":"<code>loglik</code>","text":""},{"location":"reference/ctds/#ctds.SufficientStats(T)","title":"<code>T</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend","title":"InferenceBackend","text":"<p>               Bases: <code>Protocol</code></p> <p>Abstract interface for CTDS inference backends.</p> <p>This protocol defines the required methods that any CTDS inference backend must implement. Concrete backends can use different algorithms (e.g., exact Kalman smoothing, variational inference, particle filtering) while maintaining a consistent interface.</p> <p>Methods:</p> Name Description <code>e_step : Compute sufficient statistics for EM algorithm</code> <code>smoother : Compute posterior state estimates</code> <code>filter : Compute forward filtered estimates</code> <code>posterior_sample : Sample from posterior</code> Notes <p>All implementations must be JAX-compatible and support JIT compilation. Backends should handle time-major format (T, D) for emissions and states.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; backend = DynamaxLGSSMBackend()\n&gt;&gt;&gt; stats, loglik = backend.e_step(params, emissions)\n&gt;&gt;&gt; means, covs = backend.smoother(params, emissions)\n</code></pre>"},{"location":"reference/ctds/#ctds.InferenceBackend-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.e_step","title":"e_step","text":"<pre><code>e_step(params, emissions, inputs=None)\n</code></pre> <p>E-step of EM: compute expected sufficient statistics.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Current model parameters.</p> required <code>(Array, shape(T, D))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>stats</code> <code>SufficientStats</code> <p>Expected sufficient statistics for M-step.</p> <code>loglik</code> <code>float</code> <p>Marginal log-likelihood of emissions.</p> Notes <p>Sufficient statistics typically include: - E[x_t]: posterior means - E[x_t x_t^T]: posterior second moments - E[x_t x_{t-1}^T]: cross-time moments</p>"},{"location":"reference/ctds/#ctds.InferenceBackend.e_step(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.e_step(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.e_step(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.smoother","title":"smoother","text":"<pre><code>smoother(params, emissions, inputs=None)\n</code></pre> <p>Compute posterior means and covariances for all time steps.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>(Array, shape(T, D))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>means</code> <code>(Array, shape(T, K))</code> <p>Posterior state means.</p> <code>covariances</code> <code>(Array, shape(T, K, K))</code> <p>Posterior state covariances.</p> Notes <p>This is the primary inference method providing full posterior estimates conditioned on all observations y_{1:T}.</p>"},{"location":"reference/ctds/#ctds.InferenceBackend.smoother(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.smoother(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.smoother(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.filter","title":"filter","text":"<pre><code>filter(params, emissions, inputs=None)\n</code></pre> <p>Compute forward filtered means and covariances.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>(Array, shape(T, D))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>means</code> <code>(Array, shape(T, K))</code> <p>Filtered state means.</p> <code>covariances</code> <code>(Array, shape(T, K, K))</code> <p>Filtered state covariances.</p> Notes <p>Optional method providing causal estimates p(x_t | y_{1:t}). Useful for online inference and real-time applications.</p>"},{"location":"reference/ctds/#ctds.InferenceBackend.filter(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.filter(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.filter(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.posterior_sample","title":"posterior_sample","text":"<pre><code>posterior_sample(key, params, emissions, inputs=None)\n</code></pre> <p>Sample a posterior trajectory of latent states.</p> <p>Parameters:</p> Name Type Description Default <code>PRNGKey</code> <p>Random key for sampling.</p> required <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>(Array, shape(T, D))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>states</code> <code>(Array, shape(T, K))</code> <p>Sampled latent state trajectory.</p> Notes <p>Optional method for sampling from p(x_{1:T} | y_{1:T}). Useful for uncertainty quantification and model validation.</p>"},{"location":"reference/ctds/#ctds.InferenceBackend.posterior_sample(key)","title":"<code>key</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.posterior_sample(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.posterior_sample(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.InferenceBackend.posterior_sample(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend","title":"DynamaxLGSSMBackend","text":"<p>Dynamax-based exact inference backend for CTDS.</p> <p>This backend implements exact Bayesian inference for CTDS models using Dynamax's efficient linear Gaussian state-space model routines. It provides optimal posterior estimates when the CTDS model satisfies linearity and Gaussian noise assumptions.</p> <p>Methods:</p> Name Description <code>e_step : Compute sufficient statistics using RTS smoother</code> <code>filter : Compute forward Kalman filter estimates  </code> <code>smoother : Compute RTS smoother estimates</code> <code>posterior_sample : Sample posterior trajectories</code> Notes <p>All methods are static and JAX-compatible. The backend automatically converts CTDS parameters to Dynamax LGSSM format using the <code>to_lgssm()</code> method from ParamsCTDS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; backend = DynamaxLGSSMBackend()\n&gt;&gt;&gt; stats, loglik = backend.e_step(params, emissions)\n&gt;&gt;&gt; posterior_means, posterior_covs = backend.smoother(params, emissions)\n</code></pre>"},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.e_step","title":"e_step  <code>staticmethod</code>","text":"<pre><code>e_step(params, emissions, inputs=None)\n</code></pre> <p>Compute expected sufficient statistics using RTS smoother.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>CTDS model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>stats</code> <code>SufficientStats</code> <p>Expected sufficient statistics containing: - latent_mean: E[x_t], shape (T, D) - latent_second_moment: E[x_t x_t^T], shape (T, D, D) - cross_time_moment: E[x_t x_{t-1}^T], shape (T-1, D, D) - loglik: marginal log-likelihood - T: sequence length</p> <code>loglik</code> <code>float</code> <p>Marginal log-likelihood p(y_{1:T}).</p> Notes <p>Uses Dynamax's RTS smoother for optimal posterior estimates under linear Gaussian assumptions. Sufficient statistics are computed from smoothed posterior moments.</p>"},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.e_step(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.e_step(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.e_step(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.filter","title":"filter  <code>staticmethod</code>","text":"<pre><code>filter(params, emissions, inputs=None)\n</code></pre> <p>Compute forward Kalman filter estimates.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>CTDS model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>filtered_means</code> <code>(Array, shape(T, D))</code> <p>Forward filtered state means E[x_t | y_{1:t}].</p> <code>filtered_covariances</code> <code>(Array, shape(T, D, D))</code> <p>Forward filtered state covariances Cov[x_t | y_{1:t}].</p> Notes <p>Provides causal estimates suitable for online applications. Uses Dynamax's forward Kalman filter implementation.</p>"},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.filter(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.filter(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.filter(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.smoother","title":"smoother  <code>staticmethod</code>","text":"<pre><code>smoother(params, emissions, inputs=None)\n</code></pre> <p>Compute RTS smoother estimates.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDS</code> <p>CTDS model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>smoothed_means</code> <code>(Array, shape(T, D))</code> <p>Smoothed state means E[x_t | y_{1:T}].</p> <code>smoothed_covariances</code> <code>(Array, shape(T, D, D))</code> <p>Smoothed state covariances Cov[x_t | y_{1:T}].</p> Notes <p>Provides optimal posterior estimates using full observation sequence. Uses Dynamax's Rauch-Tung-Striebel (RTS) smoother implementation.</p>"},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.smoother(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.smoother(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.smoother(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.posterior_sample","title":"posterior_sample  <code>staticmethod</code>","text":"<pre><code>posterior_sample(key, params, emissions, inputs=None)\n</code></pre> <p>Sample posterior trajectory of latent states.</p> <p>Parameters:</p> Name Type Description Default <code>PRNGKey</code> <p>Random key for sampling.</p> required <code>ParamsCTDS</code> <p>CTDS model parameters.</p> required <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>states</code> <code>(Array, shape(T, D))</code> <p>Sampled latent state trajectory from p(x_{1:T} | y_{1:T}).</p> Notes <p>Uses Dynamax's posterior sampling algorithm which first runs the smoother then samples backwards from the posterior.</p>"},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.posterior_sample(key)","title":"<code>key</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.posterior_sample(params)","title":"<code>params</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.posterior_sample(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.DynamaxLGSSMBackend.posterior_sample(inputs)","title":"<code>inputs</code>","text":""},{"location":"reference/ctds/#ctds-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.solve_dale_QP","title":"solve_dale_QP","text":"<pre><code>solve_dale_QP(Q, c, mask, key)\n</code></pre> <p>Solve quadratic program with Dale's law constraints.</p> <p>Solves the box-constrained quadratic program:     minimize    (\u00bd) * x^T Q x + c^T x     subject to:         x[i] &gt;= 0      if mask[i] is True  (excitatory neuron)         x[i] &lt;= 0      if mask[i] is False (inhibitory neuron)</p> <p>This enforces Dale's law: excitatory neurons have non-negative weights, inhibitory neurons have non-positive weights.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D, D))</code> <p>Positive semi-definite Hessian matrix.</p> required <code>(Array, shape(D))</code> <p>Linear coefficient vector.</p> required <code>(Array, shape(D))</code> <p>Boolean mask where True indicates excitatory cell, False inhibitory.</p> required <code>PRNGKey</code> <p>Random key for solver initialization.</p> required <p>Returns:</p> Name Type Description <code>solution</code> <code>(Array, shape(D))</code> <p>Optimal solution vector satisfying Dale's law constraints.</p> Notes <p>Uses BoxCDQP solver with appropriately set box constraints: - Excitatory cells: [1e-12, inf] (effectively &gt;= 0) - Inhibitory cells: [-inf, -1e-12] (effectively &lt;= 0)</p> <p>The small epsilon values prevent numerical issues at zero.</p>"},{"location":"reference/ctds/#ctds.solve_dale_QP(Q)","title":"<code>Q</code>","text":""},{"location":"reference/ctds/#ctds.solve_dale_QP(c)","title":"<code>c</code>","text":""},{"location":"reference/ctds/#ctds.solve_dale_QP(mask)","title":"<code>mask</code>","text":""},{"location":"reference/ctds/#ctds.solve_dale_QP(key)","title":"<code>key</code>","text":""},{"location":"reference/ctds/#ctds.solve_constrained_QP","title":"solve_constrained_QP","text":"<pre><code>solve_constrained_QP(\n    Q, c, mask, isExcitatory, init_x, key=PRNGKey(0)\n)\n</code></pre> <p>Solve constrained quadratic program with cell-type-specific constraints.</p> <p>Solves the box-constrained quadratic program:     minimize    (\u00bd) * x^T Q x + c^T x with constraints depending on cell type and matrix structure.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D, D))</code> <p>Positive semi-definite Hessian matrix.</p> required <code>(Array, shape(D))</code> <p>Linear coefficient vector.</p> required <code>(Array, shape(D))</code> <p>Boolean mask where True indicates off-diagonal elements, False indicates diagonal elements.</p> required <code>bool</code> <p>If True, solve for excitatory neuron (non-negative off-diagonal); if False, solve for inhibitory neuron (non-positive off-diagonal).</p> required <code>(Array, shape(D))</code> <p>Initial solution guess.</p> required <code>PRNGKey</code> <p>Random key for solver initialization.</p> <code>PRNGKey(0)</code> <p>Returns:</p> Name Type Description <code>solution</code> <code>(Array, shape(D))</code> <p>Optimal solution satisfying cell-type constraints.</p> Notes <p>Constraint structure: - Excitatory: off-diagonal &gt;= 0, diagonal unconstrained - Inhibitory: off-diagonal &lt;= 0, diagonal unconstrained</p> <p>Uses conditional computation via jax.lax.cond for efficiency.</p>"},{"location":"reference/ctds/#ctds.solve_constrained_QP(Q)","title":"<code>Q</code>","text":""},{"location":"reference/ctds/#ctds.solve_constrained_QP(c)","title":"<code>c</code>","text":""},{"location":"reference/ctds/#ctds.solve_constrained_QP(mask)","title":"<code>mask</code>","text":""},{"location":"reference/ctds/#ctds.solve_constrained_QP(isExcitatory)","title":"<code>isExcitatory</code>","text":""},{"location":"reference/ctds/#ctds.solve_constrained_QP(init_x)","title":"<code>init_x</code>","text":""},{"location":"reference/ctds/#ctds.solve_constrained_QP(key)","title":"<code>key</code>","text":""},{"location":"reference/ctds/#ctds.compute_sufficient_statistics","title":"compute_sufficient_statistics","text":"<pre><code>compute_sufficient_statistics(posterior)\n</code></pre> <p>Compute sufficient statistics from smoothed posterior for EM algorithm.</p> <p>Transforms Dynamax posterior results into the sufficient statistics format required for CTDS M-step parameter updates.</p> <p>Parameters:</p> Name Type Description Default <code>PosteriorGSSMSmoothed</code> <p>Dynamax smoother output containing: - smoothed_means : Array, shape (T, K)     Posterior state means E[x_t | y_{1:T}] - smoothed_covariances : Array, shape (T, K, K)     Posterior state covariances Cov[x_t | y_{1:T}] - smoothed_cross_covariances : Array, shape (T-1, K, K)     Cross-time covariances Cov[x_t, x_{t-1} | y_{1:T}] - marginal_loglik : float     Marginal log-likelihood p(y_{1:T})</p> required <p>Returns:</p> Name Type Description <code>stats</code> <code>SufficientStats</code> <p>Sufficient statistics containing: - latent_mean : Array, shape (T, K)     E[x_t | y_{1:T}] - latent_second_moment : Array, shape (T, K, K)     E[x_t x_t^T | y_{1:T}] = Cov + mean \u2297 mean - cross_time_moment : Array, shape (T-1, K, K)     E[x_t x_{t-1}^T | y_{1:T}] - loglik : float     Marginal log-likelihood - T : int     Sequence length</p> Notes <p>Second moments are computed using the identity: E[x_t x_t^T] = Cov[x_t] + E[x_t] E[x_t]^T</p> <p>These statistics are sufficient for maximum likelihood estimation in the M-step of the EM algorithm.</p>"},{"location":"reference/ctds/#ctds.compute_sufficient_statistics(posterior)","title":"<code>posterior</code>","text":""},{"location":"reference/ctds/#ctds.estimate_J","title":"estimate_J","text":"<pre><code>estimate_J(Y, mask)\n</code></pre> <p>Estimate the Dale matrix J from neural activity Y. Args:     Y: (N, T) array of neural activity, where N is number of neurons and T is timepoints.     mask: (N,) boolean array indicating excitatory (True) vs inhibitory (False) neurons. Returns:     J: (N, N) estimated Dale matrix, where J[i, j] is the weight from neuron j to neuron i.</p>"},{"location":"reference/ctds/#ctds.blockwise_NMF","title":"blockwise_NMF","text":"<pre><code>blockwise_NMF(J, cell_constraints)\n</code></pre> <p>Perform block-wise Non-negative Matrix Factorization (NMF) on the absolute Dale matrix |J|. Each block corresponds to a specific cell type, and NMF is applied separately to each block. Args:     J: (N, N) Dale matrix where J[i, j] is the weight from neuron j to neuron i.     cell_constraints: ParamsCellConstraints containing cell type information.         - cell_types: Array of shape (k,) with unique cell type labels.         - cell_type_dimensions: Array of shape (k,) with latent dimensions per cell type.         - cell_type_mask: Array of shape (N,) with cell type label for each neuron. Returns:     List of (U, V) tuples for each cell type, where:         - U: jnp.ndarray(N_type, D_type) matrix of basis vectors for this cell type.         - V: jnp.ndarray(N, D_type) matrix of coefficients for this cell type.</p>"},{"location":"reference/ctds/#ctds-modules","title":"Modules","text":""},{"location":"reference/ctds/#ctds.inference","title":"inference","text":"<p>Inference backends for Cell-Type Dynamical Systems (CTDS).</p> <p>This module provides abstract interfaces and concrete implementations for performing posterior inference in CTDS models. The main components are:</p> <ol> <li> <p>InferenceBackend Protocol: Abstract interface defining required methods    for any CTDS inference backend.</p> </li> <li> <p>DynamaxLGSSMBackend: Concrete implementation using Dynamax's linear    Gaussian state-space model routines for exact inference.</p> </li> </ol> <p>Classes:</p> Name Description <code>InferenceBackend : Protocol</code> <p>Abstract interface for CTDS inference backends.</p> <code>DynamaxLGSSMBackend : class</code> <p>Dynamax-based exact inference implementation.</p> Notes <p>All implementations must be JAX-compatible and support JIT compilation for high-performance inference on large neural datasets.</p>"},{"location":"reference/ctds/#ctds.inference-classes","title":"Classes","text":""},{"location":"reference/ctds/#ctds.inference.InferenceBackend","title":"InferenceBackend","text":"<p>               Bases: <code>Protocol</code></p> <p>Abstract interface for CTDS inference backends.</p> <p>This protocol defines the required methods that any CTDS inference backend must implement. Concrete backends can use different algorithms (e.g., exact Kalman smoothing, variational inference, particle filtering) while maintaining a consistent interface.</p> <p>Methods:</p> Name Description <code>e_step : Compute sufficient statistics for EM algorithm</code> <code>smoother : Compute posterior state estimates</code> <code>filter : Compute forward filtered estimates</code> <code>posterior_sample : Sample from posterior</code> Notes <p>All implementations must be JAX-compatible and support JIT compilation. Backends should handle time-major format (T, D) for emissions and states.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; backend = DynamaxLGSSMBackend()\n&gt;&gt;&gt; stats, loglik = backend.e_step(params, emissions)\n&gt;&gt;&gt; means, covs = backend.smoother(params, emissions)\n</code></pre>"},{"location":"reference/ctds/#ctds.inference.InferenceBackend-functions","title":"Functions","text":"e_step \u00b6 <pre><code>e_step(params, emissions, inputs=None)\n</code></pre> <p>E-step of EM: compute expected sufficient statistics.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Current model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, D))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>stats</code> <code>SufficientStats</code> <p>Expected sufficient statistics for M-step.</p> <code>loglik</code> <code>float</code> <p>Marginal log-likelihood of emissions.</p> Notes <p>Sufficient statistics typically include: - E[x_t]: posterior means - E[x_t x_t^T]: posterior second moments - E[x_t x_{t-1}^T]: cross-time moments</p> <code></code> smoother \u00b6 <pre><code>smoother(params, emissions, inputs=None)\n</code></pre> <p>Compute posterior means and covariances for all time steps.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, D))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>means</code> <code>(Array, shape(T, K))</code> <p>Posterior state means.</p> <code>covariances</code> <code>(Array, shape(T, K, K))</code> <p>Posterior state covariances.</p> Notes <p>This is the primary inference method providing full posterior estimates conditioned on all observations y_{1:T}.</p> <code></code> filter \u00b6 <pre><code>filter(params, emissions, inputs=None)\n</code></pre> <p>Compute forward filtered means and covariances.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, D))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>means</code> <code>(Array, shape(T, K))</code> <p>Filtered state means.</p> <code>covariances</code> <code>(Array, shape(T, K, K))</code> <p>Filtered state covariances.</p> Notes <p>Optional method providing causal estimates p(x_t | y_{1:t}). Useful for online inference and real-time applications.</p> <code></code> posterior_sample \u00b6 <pre><code>posterior_sample(key, params, emissions, inputs=None)\n</code></pre> <p>Sample a posterior trajectory of latent states.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> \u00b6 <code>PRNGKey</code> <p>Random key for sampling.</p> required <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, D))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>states</code> <code>(Array, shape(T, K))</code> <p>Sampled latent state trajectory.</p> Notes <p>Optional method for sampling from p(x_{1:T} | y_{1:T}). Useful for uncertainty quantification and model validation.</p>"},{"location":"reference/ctds/#ctds.inference.DynamaxLGSSMBackend","title":"DynamaxLGSSMBackend","text":"<p>Dynamax-based exact inference backend for CTDS.</p> <p>This backend implements exact Bayesian inference for CTDS models using Dynamax's efficient linear Gaussian state-space model routines. It provides optimal posterior estimates when the CTDS model satisfies linearity and Gaussian noise assumptions.</p> <p>Methods:</p> Name Description <code>e_step : Compute sufficient statistics using RTS smoother</code> <code>filter : Compute forward Kalman filter estimates  </code> <code>smoother : Compute RTS smoother estimates</code> <code>posterior_sample : Sample posterior trajectories</code> Notes <p>All methods are static and JAX-compatible. The backend automatically converts CTDS parameters to Dynamax LGSSM format using the <code>to_lgssm()</code> method from ParamsCTDS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; backend = DynamaxLGSSMBackend()\n&gt;&gt;&gt; stats, loglik = backend.e_step(params, emissions)\n&gt;&gt;&gt; posterior_means, posterior_covs = backend.smoother(params, emissions)\n</code></pre>"},{"location":"reference/ctds/#ctds.inference.DynamaxLGSSMBackend-functions","title":"Functions","text":"e_step <code>staticmethod</code> \u00b6 <pre><code>e_step(params, emissions, inputs=None)\n</code></pre> <p>Compute expected sufficient statistics using RTS smoother.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>CTDS model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>stats</code> <code>SufficientStats</code> <p>Expected sufficient statistics containing: - latent_mean: E[x_t], shape (T, D) - latent_second_moment: E[x_t x_t^T], shape (T, D, D) - cross_time_moment: E[x_t x_{t-1}^T], shape (T-1, D, D) - loglik: marginal log-likelihood - T: sequence length</p> <code>loglik</code> <code>float</code> <p>Marginal log-likelihood p(y_{1:T}).</p> Notes <p>Uses Dynamax's RTS smoother for optimal posterior estimates under linear Gaussian assumptions. Sufficient statistics are computed from smoothed posterior moments.</p> <code></code> filter <code>staticmethod</code> \u00b6 <pre><code>filter(params, emissions, inputs=None)\n</code></pre> <p>Compute forward Kalman filter estimates.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>CTDS model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>filtered_means</code> <code>(Array, shape(T, D))</code> <p>Forward filtered state means E[x_t | y_{1:t}].</p> <code>filtered_covariances</code> <code>(Array, shape(T, D, D))</code> <p>Forward filtered state covariances Cov[x_t | y_{1:t}].</p> Notes <p>Provides causal estimates suitable for online applications. Uses Dynamax's forward Kalman filter implementation.</p> <code></code> smoother <code>staticmethod</code> \u00b6 <pre><code>smoother(params, emissions, inputs=None)\n</code></pre> <p>Compute RTS smoother estimates.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>CTDS model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>smoothed_means</code> <code>(Array, shape(T, D))</code> <p>Smoothed state means E[x_t | y_{1:T}].</p> <code>smoothed_covariances</code> <code>(Array, shape(T, D, D))</code> <p>Smoothed state covariances Cov[x_t | y_{1:T}].</p> Notes <p>Provides optimal posterior estimates using full observation sequence. Uses Dynamax's Rauch-Tung-Striebel (RTS) smoother implementation.</p> <code></code> posterior_sample <code>staticmethod</code> \u00b6 <pre><code>posterior_sample(key, params, emissions, inputs=None)\n</code></pre> <p>Sample posterior trajectory of latent states.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> \u00b6 <code>PRNGKey</code> <p>Random key for sampling.</p> required <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>CTDS model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>states</code> <code>(Array, shape(T, D))</code> <p>Sampled latent state trajectory from p(x_{1:T} | y_{1:T}).</p> Notes <p>Uses Dynamax's posterior sampling algorithm which first runs the smoother then samples backwards from the posterior.</p>"},{"location":"reference/ctds/#ctds.inference-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.models","title":"models","text":"<p>Cell-Type Dynamical Systems (CTDS) implementation.</p> <p>This module provides the core CTDS model class for analyzing neural population dynamics with cell-type-specific constraints and Dale's law enforcement.</p>"},{"location":"reference/ctds/#ctds.models-classes","title":"Classes","text":""},{"location":"reference/ctds/#ctds.models.BaseCTDS","title":"BaseCTDS","text":"<pre><code>BaseCTDS(\n    backend, dynamics_fn, emissions_fn, constraints=None\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for Cell-Type Dynamical Systems.</p> <p>This class defines the interface for CTDS models with different inference backends and constraint handling approaches.</p> <p>Parameters:</p> Name Type Description Default <code>InferenceBackend</code> <p>Inference backend (e.g., DynamaxLGSSMBackend).</p> required <code>Callable</code> <p>Function computing transition dynamics (may include nonlinearities).</p> required <code>Callable</code> <p>Function computing observation model.</p> required <code>ParamsCTDSConstraints</code> <p>Cell-type and Dale's law constraints.</p> <code>None</code>"},{"location":"reference/ctds/#ctds.models.BaseCTDS(backend)","title":"<code>backend</code>","text":""},{"location":"reference/ctds/#ctds.models.BaseCTDS(dynamics_fn)","title":"<code>dynamics_fn</code>","text":""},{"location":"reference/ctds/#ctds.models.BaseCTDS(emissions_fn)","title":"<code>emissions_fn</code>","text":""},{"location":"reference/ctds/#ctds.models.BaseCTDS(constraints)","title":"<code>constraints</code>","text":""},{"location":"reference/ctds/#ctds.models.BaseCTDS-functions","title":"Functions","text":"initialize <code>abstractmethod</code> \u00b6 <pre><code>initialize(Y, mask, **kwargs)\n</code></pre> <p>Initialize model parameters from observed data.</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> \u00b6 <code>(Array, shape(N, T))</code> <p>Observed emission sequence.</p> required <code>mask</code> \u00b6 <code>(Array, shape(N))</code> <p>Boolean mask for valid observations.</p> required <code>**kwargs</code> \u00b6 <p>Additional initialization arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ParamsCTDS</code> <p>Initialized model parameters.</p> <code></code> infer \u00b6 <pre><code>infer(params, emissions, inputs=None)\n</code></pre> <p>Run inference using the attached backend.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>Posterior statistics from the inference backend.</code> <code></code> m_step <code>abstractmethod</code> \u00b6 <pre><code>m_step(params, stats)\n</code></pre> <p>Update model parameters given sufficient statistics.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Current model parameters.</p> required <code>stats</code> \u00b6 <code>SufficientStats</code> <p>Sufficient statistics from E-step.</p> required <p>Returns:</p> Type Description <code>ParamsCTDS</code> <p>Updated model parameters.</p>"},{"location":"reference/ctds/#ctds.models.CTDS","title":"CTDS","text":"<pre><code>CTDS(\n    emission_dim,\n    cell_types,\n    cell_sign,\n    cell_type_dimensions,\n    cell_type_mask,\n    region_identity=None,\n    inputs_dim=None,\n    state_dim=None,\n)\n</code></pre> <p>               Bases: <code>SSM</code></p> <p>Cell-Type Dynamical System with Dale's law constraints.</p> <p>A linear state-space model for neural population dynamics that enforces biologically plausible sign constraints based on cell type identity.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Number of observed neurons (N).</p> required <code>(Array, shape(K))</code> <p>Cell type labels as contiguous integers [0, 1, ..., K-1].</p> required <code>(Array, shape(K))</code> <p>Sign constraints: +1 for excitatory, -1 for inhibitory cell types.</p> required <code>(Array, shape(K))</code> <p>Latent dimensions allocated to each cell type.</p> required <code>(Array, shape(N))</code> <p>Cell type assignment for each observed neuron.</p> required <code>(Array, shape(N))</code> <p>Brain region identity for each neuron (for future multi-region support).</p> <code>None</code> <code>int</code> <p>Dimension of exogenous inputs.</p> <code>None</code> <code>int</code> <p>Total latent state dimension (computed from cell_type_dimensions if None).</p> <code>None</code> Notes <p>The CTDS model implements the linear state-space equations:</p> <p>State evolution: \\(\\(x_{t+1} = A x_t + B u_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, Q)\\)\\)</p> <p>Observations: \\(\\(y_t = C x_t + D u_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, R)\\)\\)</p> <p>where Dale's law constraints enforce that: - Excitatory neurons (cell_sign[k] = +1) have non-negative connection weights - Inhibitory neurons (cell_sign[k] = -1) have non-positive connection weights</p> <p>The model supports block-structured connectivity patterns reflecting cell-type-specific dynamics and connectivity.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If cell_types is not a contiguous range [0, 1, ..., K-1].</p> <code>ValueError</code> <p>If cell_type_mask contains invalid cell type indices.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create CTDS model with 2 cell types\n&gt;&gt;&gt; cell_types = jnp.array([0, 1])\n&gt;&gt;&gt; cell_sign = jnp.array([1, -1])  # excitatory, inhibitory\n&gt;&gt;&gt; cell_type_dimensions = jnp.array([3, 2])  # 3 + 2 = 5 total latent dims\n&gt;&gt;&gt; cell_type_mask = jnp.array([0, 0, 0, 1, 1])  # 3 exc + 2 inh neurons\n&gt;&gt;&gt; \n&gt;&gt;&gt; ctds = CTDS(\n...     emission_dim=5,\n...     cell_types=cell_types,\n...     cell_sign=cell_sign, \n...     cell_type_dimensions=cell_type_dimensions,\n...     cell_type_mask=cell_type_mask\n... )\n</code></pre>"},{"location":"reference/ctds/#ctds.models.CTDS(emission_dim)","title":"<code>emission_dim</code>","text":""},{"location":"reference/ctds/#ctds.models.CTDS(cell_types)","title":"<code>cell_types</code>","text":""},{"location":"reference/ctds/#ctds.models.CTDS(cell_sign)","title":"<code>cell_sign</code>","text":""},{"location":"reference/ctds/#ctds.models.CTDS(cell_type_dimensions)","title":"<code>cell_type_dimensions</code>","text":""},{"location":"reference/ctds/#ctds.models.CTDS(cell_type_mask)","title":"<code>cell_type_mask</code>","text":""},{"location":"reference/ctds/#ctds.models.CTDS(region_identity)","title":"<code>region_identity</code>","text":""},{"location":"reference/ctds/#ctds.models.CTDS(inputs_dim)","title":"<code>inputs_dim</code>","text":""},{"location":"reference/ctds/#ctds.models.CTDS(state_dim)","title":"<code>state_dim</code>","text":""},{"location":"reference/ctds/#ctds.models.CTDS-attributes","title":"Attributes","text":"emission_shape <code>property</code> \u00b6 <pre><code>emission_shape\n</code></pre> <p>Shape of emission observations.</p> <code></code> inputs_shape <code>property</code> \u00b6 <pre><code>inputs_shape\n</code></pre> <p>Shape of exogenous inputs, if any.</p>"},{"location":"reference/ctds/#ctds.models.CTDS-functions","title":"Functions","text":"initial_distribution \u00b6 <pre><code>initial_distribution(params, inputs)\n</code></pre> <p>p(z1) = N(mu0, Sigma0).</p> <code></code> transition_distribution \u00b6 <pre><code>transition_distribution(params, state, inputs)\n</code></pre> <p>p(z_{t+1} | z_t) = N(A z_t + B u_t, Q). If you do not use inputs, set B u_t = 0.</p> <code></code> emission_distribution \u00b6 <pre><code>emission_distribution(params, state, inputs=None)\n</code></pre> <p>p(y_t | z_t) = N(C z_t, R).</p> <code></code> initialize_dynamics \u00b6 <pre><code>initialize_dynamics(V_list, U)\n</code></pre> <p>Initialize dynamics parameters from block factorization results.</p> <p>Parameters:</p> Name Type Description Default <code>V_list</code> \u00b6 <code>list of Arrays</code> <p>List of (N, D_k) factor matrices for each cell type k.</p> required <code>U</code> \u00b6 <code>(Array, shape(N, D))</code> <p>Concatenated emission weight matrix.</p> required <p>Returns:</p> Type Description <code>ParamsCTDSDynamics</code> <p>Initialized dynamics parameters with Dale's law sign corrections.</p> Notes <p>Constructs the dynamics matrix A as A = V_dale^T @ U where V_dale applies sign corrections: positive for excitatory cell types, negative for inhibitory cell types to respect Dale's law.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If V_list length doesn't match number of cell types or if dimensions are inconsistent across matrices.</p> <code></code> initialize_emissions \u00b6 <pre><code>initialize_emissions(Y, U_list, state_dim)\n</code></pre> <p>Initialize emission parameters from block factorization results.</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> \u00b6 <code>(Array, shape(N, T))</code> <p>Observed emission sequence.</p> required <code>U_list</code> \u00b6 <code>list of Arrays</code> <p>List of (N_k, D_k) emission factor matrices for each cell type k.</p> required <code>state_dim</code> \u00b6 <code>int</code> <p>Total latent state dimension D.</p> required <p>Returns:</p> Type Description <code>ParamsCTDSEmissions</code> <p>Initialized emission parameters with block-diagonal structure.</p> Notes <p>Constructs a block-diagonal emission matrix C by concatenating cell-type-specific blocks with appropriate zero padding. The  observation noise covariance R is initialized as diagonal with empirical variances from the data.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the sum of latent dimensions doesn't match state_dim.</p> <code></code> initialize \u00b6 <pre><code>initialize(observations)\n</code></pre> <p>Initialize CTDS parameters from observed emission data.</p> <p>Parameters:</p> Name Type Description Default <code>observations</code> \u00b6 <code>(Array, shape(N, T))</code> <p>Observed emission sequence with N neurons over T timesteps.</p> required <p>Returns:</p> Type Description <code>ParamsCTDS</code> <p>Initialized model parameters.</p> Notes <p>Initialization proceeds via: 1. Estimate linear recurrent connectivity matrix J using constrained regression 2. Apply blockwise NMF to J respecting cell-type structure 3. Build block-diagonal emission and dynamics matrices 4. Set small initial state variance and process noise</p> <p>The method respects Dale's law constraints throughout the initialization.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If observations shape doesn't match expected emission_dim.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; observations = jnp.ones((10, 100))  # 10 neurons, 100 timesteps\n&gt;&gt;&gt; params = ctds.initialize(observations)\n</code></pre> <code></code> e_step \u00b6 <pre><code>e_step(params, emissions, inputs=None)\n</code></pre> <p>Expectation step: compute sufficient statistics via RTS smoothing.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Current model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observed emission sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>SufficientStats</code> <p>Sufficient statistics for M-step parameter updates: - latent_mean: E[x_t], shape (T, D) - latent_second_moment: E[x_t x_t^T], shape (T, D, D) - cross_time_moment: E[x_{t+1} x_t^T], shape (T-1, D, D) - loglik: marginal log-likelihood - T: number of timesteps</p> Notes <p>Uses the RTS (Rauch-Tung-Striebel) smoother to compute posterior moments of the latent states given all observations. These moments are sufficient for updating model parameters in the M-step.</p> <code></code> initialize_m_step_state \u00b6 <pre><code>initialize_m_step_state(params, props=None)\n</code></pre> <p>Initialize M-step state tracking for convergence diagnostics.</p> <code></code> m_step \u00b6 <pre><code>m_step(params, batch_stats, m_step_state, props=None)\n</code></pre> <p>Maximization step: update parameters using sufficient statistics.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Current model parameters.</p> required <code>props</code> \u00b6 <code>ParamsCTDS</code> <p>Unused parameter for compatibility with base class.</p> <code>None</code> <code>batch_stats</code> \u00b6 <code>SufficientStats</code> <p>Sufficient statistics averaged across batch sequences.</p> required <code>m_step_state</code> \u00b6 <code>M_Step_State</code> <p>Convergence tracking state.</p> required <p>Returns:</p> Name Type Description <code>params_updated</code> <code>ParamsCTDS</code> <p>Updated model parameters after M-step.</p> <code>m_step_state_updated</code> <code>M_Step_State</code> <p>Updated convergence tracking state.</p> Notes <p>The M-step updates parameters by solving constrained optimization problems:</p> <p>Dynamics matrix A: Solved via constrained QP respecting Dale's law: \\(\\(\\min_A \\|A\\|_F^2 \\text{ s.t. } A_{ij} \\geq 0 \\text{ if neuron } j \\text{ is excitatory}\\)\\)</p> <p>Process noise Q: Updated via residual covariance after A update.</p> <p>Emission matrix C: Block-wise non-negative least squares (NNLS)  maintaining cell-type structure and Dale's law sign constraints.</p> <p>Observation noise R: Diagonal residual variance estimation.</p> <p>Initial state: Set to first timestep posterior moments.</p> <p>A gauge-fixing step normalizes column scales to improve numerical stability.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If batch_stats arrays don't have proper batch dimensions.</p> <code></code> fit_em \u00b6 <pre><code>fit_em(\n    params,\n    batch_emissions,\n    batch_inputs=None,\n    num_iters=20,\n    verbose=True,\n)\n</code></pre> <p>Expectation\u2013Maximization training loop for the Cell-Type Dynamical System.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Initial model parameters.</p> required <code>batch_emissions</code> \u00b6 <code>(Array, shape(B, T, N))</code> <p>Observation sequence(s). Single sequence (T, N) or batch (B, T, N).</p> required <code>batch_inputs</code> \u00b6 <code>(Array, shape(T, U) or (B, T, U))</code> <p>Exogenous input sequence(s) aligned to emissions.</p> <code>None</code> <code>num_iters</code> \u00b6 <code>int</code> <p>Maximum number of EM iterations.</p> <code>100</code> <code>verbose</code> \u00b6 <code>bool</code> <p>Whether to display progress and convergence information.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>params_fitted</code> <code>ParamsCTDS</code> <p>Updated parameters after EM convergence.</p> <code>log_probs</code> <code>(Array, shape(num_iters_run))</code> <p>Marginal log-likelihood trace across iterations.</p> Notes <p>The E-step computes smoothed moments \\(E[x_t]\\), \\(E[x_t x_t^T]\\), \\(E[x_{t+1}x_t^T]\\) using the RTS smoother. The M-step updates \\((A,B)\\) and \\((C,D)\\) via  sign-constrained least squares; \\((Q,R)\\) from residual covariances.</p> <p>Convergence is detected when the relative change in log-likelihood falls below 1e-6.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input shapes are inconsistent or contain NaNs/Infs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; params_fitted, lls = ctds.fit_em(\n...     params0, emissions, num_iters=50, verbose=True\n... )\n&gt;&gt;&gt; print(f\"Final log-likelihood: {lls[-1]:.2f}\")\n</code></pre> <code></code> sample \u00b6 <pre><code>sample(params, key, num_timesteps, inputs=None)\n</code></pre> <p>Generate samples from the CTDS model.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters for sampling.</p> required <code>key</code> \u00b6 <code>PRNGKey</code> <p>Random key for stochastic operations.</p> required <code>num_timesteps</code> \u00b6 <code>int</code> <p>Number of timesteps to sample.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T, U))</code> <p>Optional exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>latent_states</code> <code>(Array, shape(T, D))</code> <p>Sampled latent state trajectories.</p> <code>emissions</code> <code>(Array, shape(T, N))</code> <p>Sampled emission observations.</p> Notes <p>Generates a forward sample by simulating the state-space model:</p> \\[x_{t+1} = A x_t + B u_t + \\varepsilon_t$$ $$y_t = C x_t + D u_t + \\eta_t\\] <p>where noise terms are drawn from their respective Gaussian distributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; key = jax.random.PRNGKey(0)\n&gt;&gt;&gt; states, obs = ctds.sample(params, key, num_timesteps=100)\n&gt;&gt;&gt; print(f\"States shape: {states.shape}, Obs shape: {obs.shape}\")\n</code></pre> <code></code> forecast \u00b6 <pre><code>forecast(\n    params, emissions, num_steps, inputs=None, key=None\n)\n</code></pre> <p>Forecast future emissions given observed history.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Fitted model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T_obs, N))</code> <p>Observed emission history for conditioning.</p> required <code>num_steps</code> \u00b6 <code>int</code> <p>Number of future timesteps to forecast.</p> required <code>inputs</code> \u00b6 <code>(Array, shape(T_obs + num_steps, U))</code> <p>Exogenous inputs for observed + forecast periods.</p> <code>None</code> <code>key</code> \u00b6 <code>PRNGKey</code> <p>Random key for stochastic forecasts. If None, returns mean predictions.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>forecasts</code> <code>(Array, shape(num_steps, N))</code> <p>Predicted future emissions.</p> Notes <p>Computes forecast by: 1. Running the smoother on observed data to get final posterior state 2. Forward-simulating the dynamics for <code>num_steps</code> timesteps 3. Generating emission predictions via the observation model</p> <p>If <code>key</code> is provided, includes process and observation noise. Otherwise returns the deterministic mean trajectory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Deterministic forecast\n&gt;&gt;&gt; y_pred = ctds.forecast(params, emissions[-50:], num_steps=20)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Stochastic forecast with uncertainty\n&gt;&gt;&gt; key = jax.random.PRNGKey(42)\n&gt;&gt;&gt; y_pred = ctds.forecast(params, emissions[-50:], num_steps=20, key=key)\n</code></pre> <code></code> log_prob \u00b6 <pre><code>log_prob(params, states, emissions, inputs=None)\n</code></pre> <p>Compute log-probability of state-emission trajectory.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>states</code> \u00b6 <code>(Array, shape(T, D))</code> <p>Latent state trajectory.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observation sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, optional, shape(T, M))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>log_prob</code> <code>float</code> <p>Joint log-probability of states and emissions.</p> <code></code> filter \u00b6 <pre><code>filter(params, emissions, inputs=None)\n</code></pre> <p>Run forward Kalman filter.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observation sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, optional, shape(T, M))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>FilterResults</code> <p>Filtered means, covariances, and log-likelihood.</p> <code></code> smoother <code>staticmethod</code> \u00b6 <pre><code>smoother(params, emissions, inputs=None)\n</code></pre> <p>Run RTS smoother for posterior inference.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observation sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, optional, shape(T, M))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>SmootherResults</code> <p>Smoothed means, covariances, and log-likelihood.</p> <code></code> marginal_log_prob \u00b6 <pre><code>marginal_log_prob(params, emissions, inputs=None)\n</code></pre> <p>Compute marginal log-likelihood of emissions.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> \u00b6 <code>ParamsCTDS</code> <p>Model parameters.</p> required <code>emissions</code> \u00b6 <code>(Array, shape(T, N))</code> <p>Observation sequence.</p> required <code>inputs</code> \u00b6 <code>(Array, optional, shape(T, M))</code> <p>Exogenous input sequence.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>log_prob</code> <code>float</code> <p>Marginal log-likelihood p(y_{1:T}).</p>"},{"location":"reference/ctds/#ctds.models-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.params","title":"params","text":"<p>Core parameter structures for Cell-Type Dynamical Systems (CTDS).</p> <p>This module defines the parameter classes that encapsulate all model components including initial distributions, dynamics, emissions, and biological constraints.</p>"},{"location":"reference/ctds/#ctds.params-classes","title":"Classes","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSInitial","title":"ParamsCTDSInitial","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Initial state distribution parameters for CTDS: p(x_1) = N(x_1 | mean, cov).</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D))</code> <p>Initial state mean vector.</p> required <code>(Array, shape(D, D) or (D,))</code> <p>Initial state covariance matrix (full) or diagonal variances.</p> required Notes <p>For the CTDS model, the initial state follows a multivariate Gaussian: \\(\\(x_1 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\)\\) where D is the total latent state dimension across all cell types.</p>"},{"location":"reference/ctds/#ctds.params.ParamsCTDSInitial(mean)","title":"<code>mean</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSInitial(cov)","title":"<code>cov</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSDynamics","title":"ParamsCTDSDynamics","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Dynamics parameters for the CTDS latent state evolution.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D, D))</code> <p>State transition matrix A in x_{t+1} = A x_t + B u_t + \u03b5_t.</p> required <code>(Array, shape(D, D) or (D,))</code> <p>Process noise covariance Q (full matrix or diagonal variances).</p> required <code>(Array, shape(D, U) or (T, D, U))</code> <p>Input weight matrix B (static or time-varying).</p> required <code>(Array, shape(D))</code> <p>Cell-type assignment mask for each latent dimension: 1 for excitatory, -1 for inhibitory, 0 for unassigned.</p> required Notes <p>The dynamics follow the linear state-space evolution: \\(\\(x_{t+1} = A x_t + B u_t + \\varepsilon_t, \\quad  \\varepsilon_t \\sim \\mathcal{N}(0, Q)\\)\\) where Dale's law constraints apply to enforce biologically plausible sign patterns in the transition matrix A.</p>"},{"location":"reference/ctds/#ctds.params.ParamsCTDSDynamics(weights)","title":"<code>weights</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSDynamics(cov)","title":"<code>cov</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSDynamics(input_weights)","title":"<code>input_weights</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSDynamics(dynamics_mask)","title":"<code>dynamics_mask</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSEmissions","title":"ParamsCTDSEmissions","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Emission model parameters mapping latent states to observations.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(N, D))</code> <p>Emission weight matrix C in y_t = C x_t + D u_t + \u03b7_t. Each row corresponds to one observed neuron.</p> required <code>(Array, shape(N, N) or (N,))</code> <p>Observation noise covariance R (full matrix or diagonal variances).</p> required <code>(Array, shape(K, 2))</code> <p>Start and end column indices for each cell type's emission block.</p> required <code>(Array, shape(K, 2), optional)</code> <p>Dimensions of left zero-padding for each cell type block.</p> required <code>(Array, shape(K, 2))</code> <p>Dimensions of right zero-padding for each cell type block.</p> required Notes <p>The emission model follows: \\(\\(y_t = C x_t + D u_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, R)\\)\\) Dale's law sign constraints apply row-wise: excitatory neurons have non-negative weights, inhibitory neurons have non-positive weights. The block structure allows modeling cell-type-specific connectivity patterns.</p>"},{"location":"reference/ctds/#ctds.params.ParamsCTDSEmissions(weights)","title":"<code>weights</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSEmissions(cov)","title":"<code>cov</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSEmissions(emission_dims)","title":"<code>emission_dims</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSEmissions(left_padding_dims)","title":"<code>left_padding_dims</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSEmissions(right_padding_dims)","title":"<code>right_padding_dims</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSConstraints","title":"ParamsCTDSConstraints","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Biological constraints for enforcing Dale's law and cell-type structure.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(K))</code> <p>Cell type labels as contiguous integers [0, 1, ..., K-1].</p> required <code>(Array, shape(K))</code> <p>Sign for each cell type: +1 (excitatory), -1 (inhibitory).</p> required <code>(Array, shape(K))</code> <p>Number of latent dimensions allocated to each cell type.</p> required <code>(Array, shape(N))</code> <p>Cell type assignment for each observed neuron.</p> required Notes <p>These constraints enforce Dale's law: excitatory neurons have non-negative synaptic weights, inhibitory neurons have non-positive weights. The cell_type_mask maps each observed neuron to its cell type, enabling block-structured computations and biologically informed regularization.</p> <p>The constraint cell_types[i] corresponds to cell_sign[i] and  cell_type_dimensions[i]. For neuron j, cell_type_mask[j] gives its type index.</p>"},{"location":"reference/ctds/#ctds.params.ParamsCTDSConstraints(cell_types)","title":"<code>cell_types</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSConstraints(cell_sign)","title":"<code>cell_sign</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSConstraints(cell_type_dimensions)","title":"<code>cell_type_dimensions</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDSConstraints(cell_type_mask)","title":"<code>cell_type_mask</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDS","title":"ParamsCTDS","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Complete CTDS model parameters with biological constraints.</p> <p>This structure encapsulates all parameters for a Cell-Type Dynamical System, including initial conditions, dynamics, emissions, and cell-type constraints.</p> <p>Parameters:</p> Name Type Description Default <code>ParamsCTDSInitial</code> <p>Initial state distribution parameters.</p> required <code>ParamsCTDSDynamics</code> <p>State transition and process noise parameters.</p> required <code>ParamsCTDSEmissions</code> <p>Observation model parameters.</p> required <code>ParamsCTDSConstraints</code> <p>Dale's law and cell-type structure constraints.</p> required <code>(Array, shape(N, T))</code> <p>Observed emission data (for reference during fitting).</p> required Notes <p>The full CTDS model equations are:</p> <p>State evolution: \\(\\(x_{t+1} = A x_t + B u_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, Q)\\)\\)</p> <p>Emissions: \\(\\(y_t = C x_t + D u_t + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, R)\\)\\)</p> <p>where A, C respect Dale's law sign constraints based on cell types.</p> See Also <p>to_lgssm : Convert to Dynamax-compatible format for inference.</p>"},{"location":"reference/ctds/#ctds.params.ParamsCTDS(initial)","title":"<code>initial</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDS(dynamics)","title":"<code>dynamics</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDS(emissions)","title":"<code>emissions</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDS(constraints)","title":"<code>constraints</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDS(observations)","title":"<code>observations</code>","text":""},{"location":"reference/ctds/#ctds.params.ParamsCTDS-functions","title":"Functions","text":"to_lgssm \u00b6 <pre><code>to_lgssm()\n</code></pre> <p>Convert CTDS parameters to Dynamax LGSSM format for inference.</p> <p>Returns:</p> Type Description <code>ParamsLGSSM</code> <p>Dynamax-compatible parameter structure.</p> Notes <p>This conversion enables use of Dynamax's optimized inference routines while preserving CTDS-specific structure. Biases are set to zero since CTDS uses centered parameterization.</p>"},{"location":"reference/ctds/#ctds.params.M_Step_State","title":"M_Step_State","text":"<p>               Bases: <code>NamedTuple</code></p> <p>State tracking for M-step convergence diagnostics.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>Current iteration number.</p> required <code>(Array, shape(num_iters))</code> <p>Frobenius norm changes in dynamics matrix A across iterations.</p> required <code>(Array, shape(num_iters))</code> <p>Frobenius norm changes in emission matrix C across iterations.</p> required <code>(Array, shape(num_iters))</code> <p>Frobenius norm changes in process noise Q across iterations.</p> required <code>(Array, shape(num_iters))</code> <p>Frobenius norm changes in observation noise R across iterations.</p> required"},{"location":"reference/ctds/#ctds.params.M_Step_State(iteration)","title":"<code>iteration</code>","text":""},{"location":"reference/ctds/#ctds.params.M_Step_State(delta_A)","title":"<code>delta_A</code>","text":""},{"location":"reference/ctds/#ctds.params.M_Step_State(delta_C)","title":"<code>delta_C</code>","text":""},{"location":"reference/ctds/#ctds.params.M_Step_State(delta_Q)","title":"<code>delta_Q</code>","text":""},{"location":"reference/ctds/#ctds.params.M_Step_State(delta_R)","title":"<code>delta_R</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSSmoothed","title":"PosteriorCTDSSmoothed","text":"<p>               Bases: <code>NamedTuple</code></p> <p>RTS-smoothed posterior over latent states.</p> <p>Parameters:</p> Name Type Description Default <code>float</code> <p>Marginal log-likelihood of the observed data.</p> required <code>(Array, shape(T, D))</code> <p>Posterior mean estimates for all time steps.</p> required <code>(Array, shape(T, D, D))</code> <p>Posterior covariance estimates for all time steps.</p> required <code>(Array, shape(T - 1, D, D))</code> <p>Cross-time posterior covariances E[x_{t+1} x_t^T | y_{1:T}].</p> required"},{"location":"reference/ctds/#ctds.params.PosteriorCTDSSmoothed(marginal_loglik)","title":"<code>marginal_loglik</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSSmoothed(smoothed_means)","title":"<code>smoothed_means</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSSmoothed(smoothed_covariances)","title":"<code>smoothed_covariances</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSSmoothed(smoothed_cross_covariances)","title":"<code>smoothed_cross_covariances</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSFiltered","title":"PosteriorCTDSFiltered","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Kalman filtered posterior over latent states.</p> <p>Parameters:</p> Name Type Description Default <code>float</code> <p>Marginal log-likelihood of the observed data.</p> required <code>(Array, shape(T, D))</code> <p>Filtered mean estimates for all time steps.</p> required <code>(Array, shape(T, D, D))</code> <p>Filtered covariance estimates for all time steps.</p> required <code>(Array, shape(T, D))</code> <p>One-step-ahead predicted means.</p> required <code>(Array, shape(T, D, D))</code> <p>One-step-ahead predicted covariances.</p> required"},{"location":"reference/ctds/#ctds.params.PosteriorCTDSFiltered(marginal_loglik)","title":"<code>marginal_loglik</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSFiltered(filtered_means)","title":"<code>filtered_means</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSFiltered(filtered_covariances)","title":"<code>filtered_covariances</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSFiltered(predicted_means)","title":"<code>predicted_means</code>","text":""},{"location":"reference/ctds/#ctds.params.PosteriorCTDSFiltered(predicted_covariances)","title":"<code>predicted_covariances</code>","text":""},{"location":"reference/ctds/#ctds.params.SufficientStats","title":"SufficientStats","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Sufficient statistics for CTDS parameter updates in the M-step.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(T, D))</code> <p>Posterior mean of latent states E[x_t].</p> required <code>(Array, shape(T, D, D))</code> <p>Posterior second moment E[x_t x_t^T].</p> required <code>(Array, shape(T - 1, D, D))</code> <p>Cross-time moments E[x_{t+1} x_t^T] for dynamics updates.</p> required <code>float</code> <p>Marginal log-likelihood of observed data.</p> required <code>int</code> <p>Number of time steps.</p> required Notes <p>These statistics are computed in the E-step and used in the M-step to update model parameters via closed-form solutions or constrained optimization routines.</p>"},{"location":"reference/ctds/#ctds.params.SufficientStats(latent_mean)","title":"<code>latent_mean</code>","text":""},{"location":"reference/ctds/#ctds.params.SufficientStats(latent_second_moment)","title":"<code>latent_second_moment</code>","text":""},{"location":"reference/ctds/#ctds.params.SufficientStats(cross_time_moment)","title":"<code>cross_time_moment</code>","text":""},{"location":"reference/ctds/#ctds.params.SufficientStats(loglik)","title":"<code>loglik</code>","text":""},{"location":"reference/ctds/#ctds.params.SufficientStats(T)","title":"<code>T</code>","text":""},{"location":"reference/ctds/#ctds.simulation_utilis","title":"simulation_utilis","text":""},{"location":"reference/ctds/#ctds.simulation_utilis-classes","title":"Classes","text":""},{"location":"reference/ctds/#ctds.simulation_utilis-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.simulation_utilis.generate_observations_from_states","title":"generate_observations_from_states","text":"<pre><code>generate_observations_from_states(\n    states, C, R, key=PRNGKey(42)\n)\n</code></pre> <p>Generate observations from states using the emission model: y_t = C @ x_t + v_t where v_t ~ N(0, R) is emission noise.</p> <p>Args:     states: (D, T) array of latent states over time     C: (N, D) emission matrix mapping states to observations     R: (N, N) emission covariance matrix or (N,) diagonal covariance     key: Random key for noise generation</p> <p>Returns:     observations: (N, T) array of observations</p>"},{"location":"reference/ctds/#ctds.simulation_utilis.generate_full_rank_matrix","title":"generate_full_rank_matrix","text":"<pre><code>generate_full_rank_matrix(key, T, N)\n</code></pre> <p>Generate an n x m matrix with linearly independent columns and reasonable conditioning.</p>"},{"location":"reference/ctds/#ctds.simulation_utilis.generate_nonneg_matrix","title":"generate_nonneg_matrix","text":"<pre><code>generate_nonneg_matrix(\n    key, n, p, noise=0.1, col_scale=1.0\n)\n</code></pre> <p>n x p nonnegative matrix with independent columns, suitable for emissions/design.</p>"},{"location":"reference/ctds/#ctds.simulation_utilis.generate_CTDS_Params","title":"generate_CTDS_Params","text":"<pre><code>generate_CTDS_Params(\n    N=50,\n    T=500,\n    D=8,\n    K=2,\n    excitatory_fraction=0.5,\n    noise_level=0.1,\n    dynamics_strength=0.8,\n    seed=PRNGKey(42),\n)\n</code></pre> <p>Generate synthetic neural data with cell-type structure and Dale's law constraints.</p> <p>Args:     N: Number of neurons     T: Number of time points     D: Total latent state dimensions     K: Number of cell types (default 2: excitatory/inhibitory)     excitatory_fraction: Fraction of neurons that are excitatory     noise_level: Standard deviation of observation noise     dynamics_strength: Maximum eigenvalue magnitude for stable dynamics     seed: Random seed for reproducibility</p> <p>Returns:     observations: (T, N) array of neural activity     constraints: ParamsCTDSConstraints object with cell type information</p>"},{"location":"reference/ctds/#ctds.simulation_utilis.generate_synthetic_data","title":"generate_synthetic_data","text":"<pre><code>generate_synthetic_data(\n    num_samples,\n    num_timesteps,\n    state_dim,\n    emission_dim,\n    cell_types=2,\n    key=PRNGKey(42),\n)\n</code></pre> <p>Generates synthetic state and observation data using a CTDS model. Args:     num_samples (int): Number of samples to generate.     num_timesteps (int): Number of timesteps per sample.     state_dim (int): Dimensionality of the latent state space.     emission_dim (int): Dimensionality of the emission/observation space.     cell_types (int, optional): Number of cell types. Defaults to 2.     key (jax.random.PRNGKey, optional): JAX random key for reproducibility. Defaults to jr.PRNGKey(42). Returns:     Tuple[         Float[Array, \"num_samples num_timesteps state_dim\"],         Float[Array, \"num_samples num_timesteps emission_dim\"],         CTDS,         ParamsCTDS     ]:          - states: Synthetic latent states.         - observations: Synthetic observations/emissions.         - ctds: The CTDS model instance used for generation.         - ctds_params: The parameters used for the CTDS model.</p>"},{"location":"reference/ctds/#ctds.simulation_utilis.hungarian_perm_block","title":"hungarian_perm_block","text":"<pre><code>hungarian_perm_block(\n    C_true_blk, C_rec_blk, use_abs_corr=True\n)\n</code></pre> <p>Build a permutation (as index array) mapping recovered columns to true columns for one block. Returns perm indices <code>perm</code> such that C_rec_blk[:, perm] aligns to C_true_blk column order.</p>"},{"location":"reference/ctds/#ctds.simulation_utilis.align_single_region_ctds","title":"align_single_region_ctds","text":"<pre><code>align_single_region_ctds(\n    C_true,\n    C_rec,\n    A_rec,\n    Q_rec,\n    block_sizes,\n    ridge=1e-06,\n    use_abs_corr=True,\n)\n</code></pre> <p>Single-region alignment that preserves cell-type blocks:   1) blockwise permutation via Hungarian assignment on column correlations   2) per-axis diagonal scaling via ridge-stabilized OLS   3) apply the induced similarity transform to (A, Q)</p>"},{"location":"reference/ctds/#ctds.utils","title":"utils","text":"<p>Utility functions for Cell-Type Dynamical Systems (CTDS).</p> <p>This module provides core computational utilities for CTDS parameter estimation, constraint handling, and optimization. The main components include:</p> <ol> <li> <p>Constrained Optimization: Quadratic programming solvers for Dale's law    and cell-type constraints (solve_dale_QP, solve_constrained_QP).</p> </li> <li> <p>Non-negative Matrix Factorization: Block-wise NMF routines for    decomposing connectivity matrices while preserving cell-type structure    (blockwise_NMF, blockwise_NNLS).</p> </li> <li> <p>Connectivity Estimation: Functions for estimating recurrent connectivity    from neural activity data (estimate_J).</p> </li> <li> <p>Sufficient Statistics: Computation of expected statistics for EM    algorithm (compute_sufficient_statistics).</p> </li> <li> <p>Validation Utilities: Functions for checking optimization conditions    and matrix properties (check_qp_condition, is_positive_semidefinite).</p> </li> </ol> <p>Functions:</p> Name Description <code>solve_dale_QP : Solve quadratic program with Dale's law constraints</code> <code>solve_constrained_QP : General constrained quadratic programming</code> <code>blockwise_NNLS : Non-negative least squares with block structure</code> <code>estimate_J : Estimate connectivity matrix from neural activity</code> <code>blockwise_NMF : Block-wise non-negative matrix factorization</code> <code>compute_sufficient_statistics : Compute EM sufficient statistics</code> Notes <p>All functions are JAX-compatible and designed for high-performance optimization on neural population data. Many routines handle Dale's law constraints ensuring biological plausibility of connectivity estimates.</p>"},{"location":"reference/ctds/#ctds.utils-classes","title":"Classes","text":""},{"location":"reference/ctds/#ctds.utils-functions","title":"Functions","text":""},{"location":"reference/ctds/#ctds.utils.solve_dale_QP","title":"solve_dale_QP","text":"<pre><code>solve_dale_QP(Q, c, mask, key)\n</code></pre> <p>Solve quadratic program with Dale's law constraints.</p> <p>Solves the box-constrained quadratic program:     minimize    (\u00bd) * x^T Q x + c^T x     subject to:         x[i] &gt;= 0      if mask[i] is True  (excitatory neuron)         x[i] &lt;= 0      if mask[i] is False (inhibitory neuron)</p> <p>This enforces Dale's law: excitatory neurons have non-negative weights, inhibitory neurons have non-positive weights.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D, D))</code> <p>Positive semi-definite Hessian matrix.</p> required <code>(Array, shape(D))</code> <p>Linear coefficient vector.</p> required <code>(Array, shape(D))</code> <p>Boolean mask where True indicates excitatory cell, False inhibitory.</p> required <code>PRNGKey</code> <p>Random key for solver initialization.</p> required <p>Returns:</p> Name Type Description <code>solution</code> <code>(Array, shape(D))</code> <p>Optimal solution vector satisfying Dale's law constraints.</p> Notes <p>Uses BoxCDQP solver with appropriately set box constraints: - Excitatory cells: [1e-12, inf] (effectively &gt;= 0) - Inhibitory cells: [-inf, -1e-12] (effectively &lt;= 0)</p> <p>The small epsilon values prevent numerical issues at zero.</p>"},{"location":"reference/ctds/#ctds.utils.solve_dale_QP(Q)","title":"<code>Q</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_dale_QP(c)","title":"<code>c</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_dale_QP(mask)","title":"<code>mask</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_dale_QP(key)","title":"<code>key</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_constrained_QP","title":"solve_constrained_QP","text":"<pre><code>solve_constrained_QP(\n    Q, c, mask, isExcitatory, init_x, key=PRNGKey(0)\n)\n</code></pre> <p>Solve constrained quadratic program with cell-type-specific constraints.</p> <p>Solves the box-constrained quadratic program:     minimize    (\u00bd) * x^T Q x + c^T x with constraints depending on cell type and matrix structure.</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D, D))</code> <p>Positive semi-definite Hessian matrix.</p> required <code>(Array, shape(D))</code> <p>Linear coefficient vector.</p> required <code>(Array, shape(D))</code> <p>Boolean mask where True indicates off-diagonal elements, False indicates diagonal elements.</p> required <code>bool</code> <p>If True, solve for excitatory neuron (non-negative off-diagonal); if False, solve for inhibitory neuron (non-positive off-diagonal).</p> required <code>(Array, shape(D))</code> <p>Initial solution guess.</p> required <code>PRNGKey</code> <p>Random key for solver initialization.</p> <code>PRNGKey(0)</code> <p>Returns:</p> Name Type Description <code>solution</code> <code>(Array, shape(D))</code> <p>Optimal solution satisfying cell-type constraints.</p> Notes <p>Constraint structure: - Excitatory: off-diagonal &gt;= 0, diagonal unconstrained - Inhibitory: off-diagonal &lt;= 0, diagonal unconstrained</p> <p>Uses conditional computation via jax.lax.cond for efficiency.</p>"},{"location":"reference/ctds/#ctds.utils.solve_constrained_QP(Q)","title":"<code>Q</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_constrained_QP(c)","title":"<code>c</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_constrained_QP(mask)","title":"<code>mask</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_constrained_QP(isExcitatory)","title":"<code>isExcitatory</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_constrained_QP(init_x)","title":"<code>init_x</code>","text":""},{"location":"reference/ctds/#ctds.utils.solve_constrained_QP(key)","title":"<code>key</code>","text":""},{"location":"reference/ctds/#ctds.utils.jaxOpt_NNLS","title":"jaxOpt_NNLS","text":"<pre><code>jaxOpt_NNLS(Q, c, init_x)\n</code></pre> <p>Solve non-negative least squares using quadratic programming.</p> <p>Solves the constrained quadratic program:     minimize    (\u00bd) * x^T Q x + c^T x     subject to:  x &gt;= 0</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(D, D))</code> <p>Positive semi-definite Hessian matrix.</p> required <code>(Array, shape(D))</code> <p>Linear coefficient vector.</p> required <code>(Array, shape(D))</code> <p>Initial solution guess.</p> required <p>Returns:</p> Name Type Description <code>solution</code> <code>(Array, shape(D))</code> <p>Non-negative optimal solution.</p> Notes <p>Uses BoxOSQP solver with non-negativity constraints. Equivalent to solving min ||Ax - b||^2 s.t. x &gt;= 0 where Q = A^T A and c = -A^T b.</p>"},{"location":"reference/ctds/#ctds.utils.jaxOpt_NNLS(Q)","title":"<code>Q</code>","text":""},{"location":"reference/ctds/#ctds.utils.jaxOpt_NNLS(c)","title":"<code>c</code>","text":""},{"location":"reference/ctds/#ctds.utils.jaxOpt_NNLS(init_x)","title":"<code>init_x</code>","text":""},{"location":"reference/ctds/#ctds.utils.Optax_NNLS","title":"Optax_NNLS","text":"<pre><code>Optax_NNLS(A, b, iters=1000)\n</code></pre> <p>Solve non-negative least squares using Optax.</p> <p>Solves: minimize ||Ax - b||^2 subject to x &gt;= 0</p> <p>Parameters:</p> Name Type Description Default <code>(Array, shape(M, N))</code> <p>Design matrix.</p> required <code>(Array, shape(M))</code> <p>Target vector.</p> required <code>int</code> <p>Maximum number of iterations (default: 1000).</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>solution</code> <code>(Array, shape(N))</code> <p>Non-negative least squares solution.</p> Notes <p>Uses Optax's built-in NNLS solver which implements an active-set algorithm.</p>"},{"location":"reference/ctds/#ctds.utils.Optax_NNLS(A)","title":"<code>A</code>","text":""},{"location":"reference/ctds/#ctds.utils.Optax_NNLS(b)","title":"<code>b</code>","text":""},{"location":"reference/ctds/#ctds.utils.Optax_NNLS(iters)","title":"<code>iters</code>","text":""},{"location":"reference/ctds/#ctds.utils.estimate_J","title":"estimate_J","text":"<pre><code>estimate_J(Y, mask)\n</code></pre> <p>Estimate the Dale matrix J from neural activity Y. Args:     Y: (N, T) array of neural activity, where N is number of neurons and T is timepoints.     mask: (N,) boolean array indicating excitatory (True) vs inhibitory (False) neurons. Returns:     J: (N, N) estimated Dale matrix, where J[i, j] is the weight from neuron j to neuron i.</p>"},{"location":"reference/ctds/#ctds.utils.blockwise_NMF","title":"blockwise_NMF","text":"<pre><code>blockwise_NMF(J, cell_constraints)\n</code></pre> <p>Perform block-wise Non-negative Matrix Factorization (NMF) on the absolute Dale matrix |J|. Each block corresponds to a specific cell type, and NMF is applied separately to each block. Args:     J: (N, N) Dale matrix where J[i, j] is the weight from neuron j to neuron i.     cell_constraints: ParamsCellConstraints containing cell type information.         - cell_types: Array of shape (k,) with unique cell type labels.         - cell_type_dimensions: Array of shape (k,) with latent dimensions per cell type.         - cell_type_mask: Array of shape (N,) with cell type label for each neuron. Returns:     List of (U, V) tuples for each cell type, where:         - U: jnp.ndarray(N_type, D_type) matrix of basis vectors for this cell type.         - V: jnp.ndarray(N, D_type) matrix of coefficients for this cell type.</p>"},{"location":"reference/ctds/#ctds.utils.build_U","title":"build_U","text":"<pre><code>build_U(U1, U2)\n</code></pre> <p>Constructs block-diagonal emission matrix:     U = [[U1,  0 ],          [ 0,  U2]]</p> <p>Args:     U1: shape (NE, K1), excitatory emission matrix     U2: shape (NI, K2), inhibitory emission matrix</p> <p>Returns:     U: shape (NE + NI, K1 + K2), block-diagonal emission matrix</p>"},{"location":"reference/ctds/#ctds.utils.build_A","title":"build_A","text":"<pre><code>build_A(U, V_dale)\n</code></pre> <p>Compute initial latent dynamics matrix:   A = V_dale^T @ U</p>"},{"location":"reference/ctds/#ctds.utils.compute_sufficient_statistics","title":"compute_sufficient_statistics","text":"<pre><code>compute_sufficient_statistics(posterior)\n</code></pre> <p>Compute sufficient statistics from smoothed posterior for EM algorithm.</p> <p>Transforms Dynamax posterior results into the sufficient statistics format required for CTDS M-step parameter updates.</p> <p>Parameters:</p> Name Type Description Default <code>PosteriorGSSMSmoothed</code> <p>Dynamax smoother output containing: - smoothed_means : Array, shape (T, K)     Posterior state means E[x_t | y_{1:T}] - smoothed_covariances : Array, shape (T, K, K)     Posterior state covariances Cov[x_t | y_{1:T}] - smoothed_cross_covariances : Array, shape (T-1, K, K)     Cross-time covariances Cov[x_t, x_{t-1} | y_{1:T}] - marginal_loglik : float     Marginal log-likelihood p(y_{1:T})</p> required <p>Returns:</p> Name Type Description <code>stats</code> <code>SufficientStats</code> <p>Sufficient statistics containing: - latent_mean : Array, shape (T, K)     E[x_t | y_{1:T}] - latent_second_moment : Array, shape (T, K, K)     E[x_t x_t^T | y_{1:T}] = Cov + mean \u2297 mean - cross_time_moment : Array, shape (T-1, K, K)     E[x_t x_{t-1}^T | y_{1:T}] - loglik : float     Marginal log-likelihood - T : int     Sequence length</p> Notes <p>Second moments are computed using the identity: E[x_t x_t^T] = Cov[x_t] + E[x_t] E[x_t]^T</p> <p>These statistics are sufficient for maximum likelihood estimation in the M-step of the EM algorithm.</p>"},{"location":"reference/ctds/#ctds.utils.compute_sufficient_statistics(posterior)","title":"<code>posterior</code>","text":""},{"location":"reference/ctds/#ctds.utils.simple_blockwise_nmf","title":"simple_blockwise_nmf","text":"<pre><code>simple_blockwise_nmf(J, mask, D_E, D_I)\n</code></pre> <p>TODO: docustring</p>"},{"location":"reference/ctds/#ctds.utils.check_qp_condition","title":"check_qp_condition","text":"<pre><code>check_qp_condition(Q)\n</code></pre> <p>Check the quadratic programming (QP) condition for the given matrix.</p>"},{"location":"reference/ctds/#ctds.utils.is_positive_semidefinite","title":"is_positive_semidefinite","text":"<pre><code>is_positive_semidefinite(Q)\n</code></pre> <p>Check if the given matrix is positive semidefinite.</p>"},{"location":"reference/ctds/#ctds.utils.linreg_to_quadratic_form","title":"linreg_to_quadratic_form","text":"<pre><code>linreg_to_quadratic_form(A, b)\n</code></pre> <p>Convert linear regression problem ||Ax - b||\u00b2 to quadratic form \u00bdx^T P x + q^T x + r</p> <p>The linear regression objective ||Ax - b||\u00b2 expands to: (Ax - b)^T (Ax - b) = x^T A^T A x - 2 b^T A x + b^T b</p> <p>In standard quadratic form \u00bdx^T P x + q^T x + r: - P = 2 * A^T A  (factor of 2 because of the \u00bd in standard form) - q = -2 * A^T b - r = b^T b</p> <p>Args:     A: Design matrix of shape (m, n)     b: Target vector of shape (m,)</p> <p>Returns:     P: Quadratic coefficient matrix of shape (n, n)     q: Linear coefficient vector of shape (n,)     r: Constant scalar</p>"},{"location":"user-guide/mathematical-framework/","title":"Mathematical Framework","text":""},{"location":"user-guide/mathematical-framework/#overview","title":"Overview","text":"<p>Cell-Type Dynamical Systems (CTDS) model neural population dynamics using linear state-space models with biological constraints. This page provides the mathematical foundation for understanding how CTDS works.</p>"},{"location":"user-guide/mathematical-framework/#state-space-formulation","title":"State-Space Formulation","text":""},{"location":"user-guide/mathematical-framework/#core-equations","title":"Core Equations","text":"<p>The CTDS model consists of two main equations:</p> <p>Latent State Dynamics: \\(\\(x_{t+1} = A x_t + B u_t + w_t\\)\\)</p> <p>where: - \\(x_t \\in \\mathbb{R}^D\\) is the latent state at time \\(t\\) - \\(A \\in \\mathbb{R}^{D \\times D}\\) is the dynamics matrix (recurrent connectivity) - \\(B \\in \\mathbb{R}^{D \\times U}\\) is the input matrix (currently not implemented) - \\(u_t \\in \\mathbb{R}^U\\) are exogenous inputs - \\(w_t \\sim \\mathcal{N}(0, Q)\\) is process noise with covariance \\(Q \\in \\mathbb{R}^{D \\times D}\\)</p> <p>Observation Model: \\(\\(y_t = C x_t + v_t\\)\\)</p> <p>where: - \\(y_t \\in \\mathbb{R}^N\\) are the observed neural activities - \\(C \\in \\mathbb{R}^{N \\times D}\\) is the emission matrix (observation model) - \\(v_t \\sim \\mathcal{N}(0, R)\\) is observation noise with covariance \\(R \\in \\mathbb{R}^{N \\times N}\\)</p>"},{"location":"user-guide/mathematical-framework/#initial-conditions","title":"Initial Conditions","text":"<p>The initial state follows a Gaussian distribution: \\(\\(x_1 \\sim \\mathcal{N}(\\mu_1, \\Sigma_1)\\)\\)</p>"},{"location":"user-guide/mathematical-framework/#cell-type-structure","title":"Cell-Type Structure","text":""},{"location":"user-guide/mathematical-framework/#connectivity-factorization","title":"Connectivity Factorization","text":"<p>The key innovation of CTDS is factorizing the dynamics matrix to respect cell-type structure:</p> \\[A = U V^T\\] <p>where: - \\(U \\in \\mathbb{R}^{D \\times K}\\) contains cell-type factors - \\(V \\in \\mathbb{R}^{D \\times K}\\) contains connectivity weights - \\(K\\) is the number of cell types (typically \\(K=2\\) for excitatory/inhibitory)</p>"},{"location":"user-guide/mathematical-framework/#dales-law-constraints","title":"Dale's Law Constraints","text":"<p>Dale's law states that neurons release the same neurotransmitter at all synapses. In CTDS, this translates to sign constraints:</p> <p>For dynamics matrix \\(A\\): - \\(A_{ij} \\geq 0\\) if source neuron \\(j\\) is excitatory - \\(A_{ij} \\leq 0\\) if source neuron \\(j\\) is inhibitory</p> <p>For emission matrix \\(C\\): - \\(C_{ij} \\geq 0\\) (non-negative emission weights preserve Dale signs)</p>"},{"location":"user-guide/mathematical-framework/#cell-type-organization","title":"Cell-Type Organization","text":"<p>Neurons are organized into cell types with dimensions: - \\(D_E\\): number of excitatory neurons in latent space - \\(D_I\\): number of inhibitory neurons in latent space - Total: \\(D = D_E + D_I\\)</p>"},{"location":"user-guide/mathematical-framework/#parameter-estimation","title":"Parameter Estimation","text":""},{"location":"user-guide/mathematical-framework/#em-algorithm","title":"EM Algorithm","text":"<p>CTDS uses the Expectation-Maximization (EM) algorithm for parameter estimation:</p> <p>E-step: Compute posterior statistics using the Kalman smoother \\(\\(p(x_{1:T} | y_{1:T}, \\theta^{(k)})\\)\\)</p> <p>M-step: Update parameters using constrained optimization \\(\\(\\theta^{(k+1)} = \\arg\\max_\\theta \\mathbb{E}[\\log p(x_{1:T}, y_{1:T} | \\theta)]\\)\\)</p>"},{"location":"user-guide/mathematical-framework/#sufficient-statistics","title":"Sufficient Statistics","text":"<p>The E-step computes sufficient statistics: - \\(\\hat{x}_t = \\mathbb{E}[x_t | y_{1:T}]\\): posterior means - \\(\\hat{P}_t = \\text{Cov}[x_t | y_{1:T}]\\): posterior covariances - \\(\\hat{P}_{t,t-1} = \\text{Cov}[x_t, x_{t-1} | y_{1:T}]\\): lag-1 cross-covariances</p>"},{"location":"user-guide/mathematical-framework/#constrained-m-step-updates","title":"Constrained M-step Updates","text":"<p>Dynamics Matrix \\(A\\): Solved via constrained quadratic programming: \\(\\(\\min_A \\|A\\|_F^2 \\text{ s.t. Dale's law constraints}\\)\\)</p> <p>Emission Matrix \\(C\\): Block-wise non-negative least squares (NNLS): \\(\\(\\min_C \\|Y - CX\\|_F^2 \\text{ s.t. } C \\geq 0\\)\\)</p> <p>Covariances \\(Q, R\\): Residual covariance estimation: \\(\\(Q = \\frac{1}{T-1} \\sum_{t=2}^T \\mathbb{E}[(x_t - Ax_{t-1})(x_t - Ax_{t-1})^T]\\)\\) \\(\\(R = \\frac{1}{T} \\sum_{t=1}^T \\mathbb{E}[(y_t - Cx_t)(y_t - Cx_t)^T]\\)\\)</p>"},{"location":"user-guide/mathematical-framework/#inference","title":"Inference","text":""},{"location":"user-guide/mathematical-framework/#kalman-filteringsmoothing","title":"Kalman Filtering/Smoothing","text":"<p>CTDS uses exact Bayesian inference via Kalman filtering:</p> <p>Forward Pass (Filtering): \\(\\(p(x_t | y_{1:t}) = \\mathcal{N}(x_t | \\mu_{t|t}, \\Sigma_{t|t})\\)\\)</p> <p>Backward Pass (Smoothing): \\(\\(p(x_t | y_{1:T}) = \\mathcal{N}(x_t | \\mu_{t|T}, \\Sigma_{t|T})\\)\\)</p>"},{"location":"user-guide/mathematical-framework/#likelihood-computation","title":"Likelihood Computation","text":"<p>The marginal likelihood is computed as: \\(\\(\\log p(y_{1:T}) = \\sum_{t=1}^T \\log p(y_t | y_{1:t-1})\\)\\)</p> <p>This provides a principled way to compare models and assess fit quality.</p>"},{"location":"user-guide/mathematical-framework/#biological-interpretation","title":"Biological Interpretation","text":""},{"location":"user-guide/mathematical-framework/#network-connectivity","title":"Network Connectivity","text":"<p>The learned dynamics matrix \\(A\\) represents effective connectivity between neural populations. The factorization \\(A = UV^T\\) allows interpretation of:</p> <ul> <li>\\(U\\) matrix: Cell-type mixing coefficients</li> <li>\\(V\\) matrix: Dale-constrained connectivity patterns</li> </ul>"},{"location":"user-guide/mathematical-framework/#neural-subspaces","title":"Neural Subspaces","text":"<p>The latent states \\(x_t\\) represent activity in a low-dimensional neural subspace that captures the essential dynamics while respecting biological constraints.</p>"},{"location":"user-guide/mathematical-framework/#population-dynamics","title":"Population Dynamics","text":"<p>The model captures both: - Fast timescales: Observation noise and rapid fluctuations - Slow timescales: Latent dynamics and connectivity structure</p>"},{"location":"user-guide/mathematical-framework/#extensions-and-limitations","title":"Extensions and Limitations","text":""},{"location":"user-guide/mathematical-framework/#current-limitations","title":"Current Limitations","text":"<ul> <li>Linear dynamics: No support for nonlinear dynamics</li> <li>Gaussian noise: Assumes Gaussian process and observation noise</li> <li>Single region: No multi-region connectivity</li> <li>Static parameters: Time-invariant dynamics and emission matrices</li> </ul>"},{"location":"user-guide/mathematical-framework/#future-extensions","title":"Future Extensions","text":"<ul> <li>Nonlinear dynamics: Neural network components for \\(f(x_t)\\)</li> <li>Multi-region models: Cross-region connectivity patterns</li> <li>Time-varying parameters: Adaptive dynamics and emissions</li> <li>Non-Gaussian noise: Robust noise models</li> </ul>"}]}